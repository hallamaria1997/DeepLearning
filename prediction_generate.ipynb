{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8CO1camtnDy",
        "outputId": "30c79bc0-1c74-4f19-bf9c-0ab7cf16c206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "apache-beam 2.42.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.13 requires dill>=0.3.5.1, but you have dill 0.3.1.1 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets==2.5.1\n",
        "\n",
        "!pip install -q apache_beam==2.42.0\n",
        "#mwparserfromhell\n",
        "\n",
        "!pip install -q farm-haystack -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!pip install -q wikipedia==1.4.0"
      ],
      "id": "V8CO1camtnDy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "156432dc"
      },
      "source": [
        "### Imports"
      ],
      "id": "156432dc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c61ce6d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "import wikipedia as wiki\n",
        "import re"
      ],
      "id": "6c61ce6d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfbc345a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_dataset "
      ],
      "id": "cfbc345a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvGD-jLqtrBk",
        "outputId": "49296954-cf25-4ee3-8935-9e9767645e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "TvGD-jLqtrBk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb5674a4"
      },
      "source": [
        "### Evaluation dataset"
      ],
      "id": "eb5674a4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "d0490958a53649f79c7a722f5ee64fa6",
            "641a965f328e486ebd891cffc2651f96",
            "093d93fe15f341eab1fc3242edf6f706",
            "fd105d36dd32429493faa4adbc5cb2db",
            "2e4a46739bd34b90904952defda723bd",
            "6e7c17b9517f4234a4295cc834c8bac3",
            "b775861d7a164ecca1f3a13b9e62f3dc",
            "5d7e75433aea4a8a921d0e3ac0afc53b",
            "0989b2985b2446e7b381b32b48abea84",
            "eb05db4f82384aed919a1afef81f92f1",
            "25050867c1b94216a28c20696f6ecbb9",
            "8c6980069f4b42aca4f4a96db780a87e",
            "099f1963d9994587b9287547ee1e2f2a",
            "73bf105963c0488faa1f5d380c274c18",
            "66e487d31a00443ba1a3e1d25611f414",
            "2287d3815d7247a2acfaf87dbdea28c0",
            "036e122c3d454cb5a751b5625d2cefec",
            "605e01e7a6d0470bbf78f9973db610d7",
            "07652631fda94743abc34ce411375d58",
            "937aded167874d28a49f244e9ca00138",
            "ffb215bf4f824804bf61fdc865175652",
            "e074cec37e114bc5ada2e5223a07bdce",
            "9c70a44d728b4e5792f1e5dabb48aa58",
            "47738716ef4c4200876acc8586896d4b",
            "f4b77305816e43c8b71d7a3906ddc664",
            "cadda54e30e049018436f43cb72bea9e",
            "d0ce2f77553c416ea03b71a902d066d6",
            "b182549b8c4c41c28004e940e898f400",
            "0504834615214693a0f93d6e04bfa435",
            "6097852a87ba4b4f9c056347b5b8909f",
            "eff345c071df4a33b6c34ce64934de34",
            "990e436d54174baf9fdc648a6013d41e",
            "41c4f9539cf441469f1d5a6fea48d546",
            "5a99c9e555bc41319d503bcd96b410a2",
            "95428fc3db1e4c13b8771ffd911add3b",
            "5d81818a3a404c3285f4549446827bb1",
            "3f55f424e82a431c9b61713359b7914a",
            "43f96edf4c9c4cb6bc6b4b3e12e5a970",
            "20a7bd472cc34986a60a423bab8d2ae7",
            "7a7fdf7cadc74d378b140a1bb790c3d3",
            "47f9e017e33945b2ac413d5d7c3a6568",
            "a9567bd4b2004af19995c300db38ceeb",
            "ece90426b93d45dda2d9346395b5b1e1",
            "efec0ea3f4cf4b49b6ea480c0f5704f7",
            "31a63be9ffa04fc5880f6da061c738cb",
            "80c85d4983ef4998b4397b53d2e6189f",
            "abb99ffe34c345c08054d4d4e3f7a91b",
            "568e2dc6d714462b98a6148fe2d27474",
            "7b94b973b7fc473faad1b76c124c49bc",
            "ab963247b708418a94e35215ff80efcb",
            "27f602e56b08443b8b2e059146fd0b08",
            "f3f531fbf39d41c4961316843a4fab60",
            "7273d9a1701549a5b874b635c4b38213",
            "c5a481b8908347009a50ce243bd15d3b",
            "e7c73b8b539247c5b319ea055f71b809"
          ]
        },
        "id": "7d662492",
        "outputId": "bae545ec-15db-4d67-9987-4a4f2168eec6",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0490958a53649f79c7a722f5ee64fa6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.80k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c6980069f4b42aca4f4a96db780a87e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/1.95k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset covid_qa_deepset/covid_qa_deepset (download: 4.21 MiB, generated: 62.13 MiB, post-processed: Unknown size, total: 66.35 MiB) to /root/.cache/huggingface/datasets/covid_qa_deepset/covid_qa_deepset/1.0.0/fb886523842e312176f92ec8e01e77a08fa15a694f5741af6fc42796ee9c8c46...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c70a44d728b4e5792f1e5dabb48aa58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a99c9e555bc41319d503bcd96b410a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/2019 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset covid_qa_deepset downloaded and prepared to /root/.cache/huggingface/datasets/covid_qa_deepset/covid_qa_deepset/1.0.0/fb886523842e312176f92ec8e01e77a08fa15a694f5741af6fc42796ee9c8c46. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31a63be9ffa04fc5880f6da061c738cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# evaluation data set 1: covid qa deepset, scientific annotated data used to train deepset/roberta-base-squad2-covid\n",
        "d_covid = load_dataset('covid_qa_deepset')\n",
        "QA_covid = d_covid.data['train'].to_pandas()\n",
        "dataQA = {'question': QA_covid['question'][:100],\n",
        "          'answer':[d.get('text')[0] for d in QA_covid[:100].answers],\n",
        "          'wrong_answer': [''] * 100} \n",
        "qa_dataset = pd.DataFrame(dataQA)\n",
        "\n",
        "# evaluation data set 2: qovid qa dataset created using news platforms around the world\n",
        "#qa_dataset = pd.read_csv('/content/drive/MyDrive/DeepLearning/news.csv')\n",
        "#qa_dataset = qa_dataset[['question', 'answer', 'wrong_answer']]\n",
        "#qa_dataset['question'] = 'Covid 19, '+ qa_dataset['question'].astype(str)\n",
        "#qa_dataset.head(3)"
      ],
      "id": "7d662492"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "umOB2SzUgFS6",
        "outputId": "61e32c02-c705-42fa-d208-0690835ed04a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-95cdc6e2-6011-42c6-afa8-d68873c1c75a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>wrong_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the main cause of HIV-1 infection in c...</td>\n",
              "      <td>Mother-to-child transmission (MTCT) is the mai...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What plays the crucial role in the Mother to C...</td>\n",
              "      <td>DC-SIGNR plays a crucial role in MTCT of HIV-1...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How many children were infected by HIV-1 in 20...</td>\n",
              "      <td>more than 400,000 children were infected world...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the role of C-C Motif Chemokine Ligand...</td>\n",
              "      <td>High copy numbers of CCL3L1, a potent HIV-1 su...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is DC-GENR and where is  it expressed?</td>\n",
              "      <td>Dendritic cell-specific ICAM-grabbing non-inte...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>How is exhaled breath condensate used in viral...</td>\n",
              "      <td>the isolation of respiratory viruses</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>How many patients were i this study?</td>\n",
              "      <td>102</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>What was the conclusion of this study?</td>\n",
              "      <td>EBC collection using the RTube™ is not reliabl...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>How long did the patient breath into the RTube?</td>\n",
              "      <td>10 minutes</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>What followed the reverse transcription step i...</td>\n",
              "      <td>denaturation step</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95cdc6e2-6011-42c6-afa8-d68873c1c75a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95cdc6e2-6011-42c6-afa8-d68873c1c75a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95cdc6e2-6011-42c6-afa8-d68873c1c75a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             question  \\\n",
              "0   What is the main cause of HIV-1 infection in c...   \n",
              "1   What plays the crucial role in the Mother to C...   \n",
              "2   How many children were infected by HIV-1 in 20...   \n",
              "3   What is the role of C-C Motif Chemokine Ligand...   \n",
              "4         What is DC-GENR and where is  it expressed?   \n",
              "..                                                ...   \n",
              "95  How is exhaled breath condensate used in viral...   \n",
              "96               How many patients were i this study?   \n",
              "97             What was the conclusion of this study?   \n",
              "98    How long did the patient breath into the RTube?   \n",
              "99  What followed the reverse transcription step i...   \n",
              "\n",
              "                                               answer wrong_answer  \n",
              "0   Mother-to-child transmission (MTCT) is the mai...               \n",
              "1   DC-SIGNR plays a crucial role in MTCT of HIV-1...               \n",
              "2   more than 400,000 children were infected world...               \n",
              "3   High copy numbers of CCL3L1, a potent HIV-1 su...               \n",
              "4   Dendritic cell-specific ICAM-grabbing non-inte...               \n",
              "..                                                ...          ...  \n",
              "95               the isolation of respiratory viruses               \n",
              "96                                                102               \n",
              "97  EBC collection using the RTube™ is not reliabl...               \n",
              "98                                         10 minutes               \n",
              "99                                  denaturation step               \n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_dataset"
      ],
      "id": "umOB2SzUgFS6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ead6bbe1"
      },
      "source": [
        "### Elasticsearch Server"
      ],
      "id": "ead6bbe1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45bf410b"
      },
      "source": [
        "```bash\n",
        "docker pull docker.elastic.co/elasticsearch/elasticsearch:7.9.2\n",
        "\n",
        "docker run -d -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.9.2\n",
        "\n",
        "```"
      ],
      "id": "45bf410b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmdEDw2VuAtq"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "id": "QmdEDw2VuAtq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp0AYIkPuB7x"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "id": "Vp0AYIkPuB7x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMhjvfmpuDRg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(30)"
      ],
      "id": "LMhjvfmpuDRg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aebc075"
      },
      "source": [
        "### Reader imports"
      ],
      "id": "3aebc075"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "add6e734"
      },
      "outputs": [],
      "source": [
        "# just to display the different retrievers available\n",
        "from haystack.nodes import (\n",
        "    BM25Retriever,\n",
        "    TfidfRetriever,\n",
        "    DensePassageRetriever,\n",
        "    FARMReader,\n",
        "    RAGenerator,\n",
        "    BaseComponent,\n",
        "    JoinDocuments,\n",
        ")\n",
        "\n",
        "from haystack.pipelines import (\n",
        "    ExtractiveQAPipeline, \n",
        "    DocumentSearchPipeline, \n",
        "    GenerativeQAPipeline\n",
        ")\n",
        "\n",
        "from haystack.nodes import FARMReader\n",
        "\n",
        "from haystack.utils import print_answers"
      ],
      "id": "add6e734"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8816fdd3"
      },
      "source": [
        "# Definition: MODEL, GPU, TOP_K, READER"
      ],
      "id": "8816fdd3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eebd14cc"
      },
      "outputs": [],
      "source": [
        "# CHANGE MODEL NAME FOR DIFFERENT MODELs\n",
        "# squad 2 model\n",
        "model = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "# squad 2 model trained on covid 19 data\n",
        "# model = \"deepset/roberta-base-squad2-covid\"\n",
        "\n",
        "\n",
        "use_gpu = True\n",
        "\n",
        "top_k_retriever = 10\n",
        "top_k_reader = 5\n",
        "\n",
        "\n",
        "reader = FARMReader(\n",
        "    model_name_or_path=model, #\"deepset/roberta-base-squad2\"\n",
        "    use_gpu=use_gpu\n",
        ")"
      ],
      "id": "eebd14cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e63cfedb"
      },
      "source": [
        "# Covid "
      ],
      "id": "e63cfedb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0f6b7ef"
      },
      "source": [
        "### Context: COVID-19 Dataset [Link here](https://github.com/deepset-ai/COVID-QA)"
      ],
      "id": "f0f6b7ef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "98a049d2",
        "outputId": "1c8d47ef-49f5-4f5d-eb12-fb47f5f87a23"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3dfe3f9a-5dc7-477d-b139-801cf817bffa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answers</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>context</th>\n",
              "      <th>document_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the main cause of HIV-1 infection in children?</td>\n",
              "      <td>262</td>\n",
              "      <td>[{'text': 'Mother-to-child transmission (MTCT) is the main cause of HIV-1 in...</td>\n",
              "      <td>False</td>\n",
              "      <td>Functional Genetic Variants in DC-SIGNR Are Associated with Mother-to-Child ...</td>\n",
              "      <td>630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What plays the crucial role in the Mother to Child Transmission of HIV-1 and...</td>\n",
              "      <td>276</td>\n",
              "      <td>[{'text': 'DC-SIGNR plays a crucial role in MTCT of HIV-1 and that impaired ...</td>\n",
              "      <td>False</td>\n",
              "      <td>Functional Genetic Variants in DC-SIGNR Are Associated with Mother-to-Child ...</td>\n",
              "      <td>630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How many children were infected by HIV-1 in 2008-2009, worldwide?</td>\n",
              "      <td>278</td>\n",
              "      <td>[{'text': 'more than 400,000 children were infected worldwide, mostly throug...</td>\n",
              "      <td>False</td>\n",
              "      <td>Functional Genetic Variants in DC-SIGNR Are Associated with Mother-to-Child ...</td>\n",
              "      <td>630</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dfe3f9a-5dc7-477d-b139-801cf817bffa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3dfe3f9a-5dc7-477d-b139-801cf817bffa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3dfe3f9a-5dc7-477d-b139-801cf817bffa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                          question  \\\n",
              "0                           What is the main cause of HIV-1 infection in children?   \n",
              "1  What plays the crucial role in the Mother to Child Transmission of HIV-1 and...   \n",
              "2                How many children were infected by HIV-1 in 2008-2009, worldwide?   \n",
              "\n",
              "    id  \\\n",
              "0  262   \n",
              "1  276   \n",
              "2  278   \n",
              "\n",
              "                                                                           answers  \\\n",
              "0  [{'text': 'Mother-to-child transmission (MTCT) is the main cause of HIV-1 in...   \n",
              "1  [{'text': 'DC-SIGNR plays a crucial role in MTCT of HIV-1 and that impaired ...   \n",
              "2  [{'text': 'more than 400,000 children were infected worldwide, mostly throug...   \n",
              "\n",
              "   is_impossible  \\\n",
              "0          False   \n",
              "1          False   \n",
              "2          False   \n",
              "\n",
              "                                                                           context  \\\n",
              "0  Functional Genetic Variants in DC-SIGNR Are Associated with Mother-to-Child ...   \n",
              "1  Functional Genetic Variants in DC-SIGNR Are Associated with Mother-to-Child ...   \n",
              "2  Functional Genetic Variants in DC-SIGNR Are Associated with Mother-to-Child ...   \n",
              "\n",
              "   document_id  \n",
              "0          630  \n",
              "1          630  \n",
              "2          630  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/DeepLearning/COVID-QA.json') as jsonfile:\n",
        "    data = json.load(jsonfile)\n",
        "\n",
        "# create the dataframe that holds our data\n",
        "df_covid = pd.DataFrame()\n",
        "for i in range(len(data['data'])):\n",
        "    q = pd.json_normalize(data['data'][i]['paragraphs'][0]['qas'])\n",
        "    contxt = data['data'][i]['paragraphs'][0]['context']\n",
        "    d_id = data['data'][i]['paragraphs'][0]['document_id']\n",
        "    q['context'] = contxt\n",
        "    q['document_id'] = d_id\n",
        "    df_covid = pd.concat([df_covid, q])\n",
        "    \n",
        "df_covid.reset_index(drop=True, inplace=True)\n",
        "df_covid.head(3)"
      ],
      "id": "98a049d2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb3b47b5"
      },
      "outputs": [],
      "source": [
        "df_covid['context_cleaned'] = df_covid.context.apply(\n",
        "    lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]|\\n|[^a-zA-z0-9.]', ' ', x)\n",
        ")"
      ],
      "id": "fb3b47b5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aa0680a"
      },
      "source": [
        "### Document Store"
      ],
      "id": "8aa0680a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sH3PpJX4Rgc"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "id": "5sH3PpJX4Rgc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khHIreAw4Rge"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "id": "khHIreAw4Rge"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXCoafAj4Rge"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(30)"
      ],
      "id": "LXCoafAj4Rge"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26905fb5"
      },
      "outputs": [],
      "source": [
        "documents_lst_covid = df_covid.to_dict(orient='records')"
      ],
      "id": "26905fb5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "520b893d",
        "outputId": "91d40897-fbb3-4925-e593-2f85a6b5edcf",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2019/2019 [00:00<00:00, 441563.24it/s]\n"
          ]
        }
      ],
      "source": [
        "# the way document_store wants data formatted\n",
        "dicts_covid = [\n",
        "    {\n",
        "        'content' : str(elm['context']),\n",
        "        'meta' : {\n",
        "            'name' : str(elm['question'])\n",
        "        }\n",
        "    } \n",
        "    for elm in tqdm(documents_lst_covid)\n",
        "]"
      ],
      "id": "520b893d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef236394"
      },
      "source": [
        "Write to document store"
      ],
      "id": "ef236394"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94ca0967",
        "outputId": "3da7d3fe-6a39-4449-a6f6-a662e8c25ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 832 ms, sys: 103 ms, total: 935 ms\n",
            "Wall time: 12.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "document_store_covid = ElasticsearchDocumentStore(\n",
        "    port=9200\n",
        ")\n",
        "document_store_covid.delete_documents()\n",
        "document_store_covid.write_documents(\n",
        "    documents=dicts_covid\n",
        ")"
      ],
      "id": "94ca0967"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b54e9620"
      },
      "source": [
        "### Retrieving answers"
      ],
      "id": "b54e9620"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "186523bd"
      },
      "source": [
        "#### BM25"
      ],
      "id": "186523bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dabea22",
        "outputId": "2773ede9-216e-4d37-b2f6-3ed201b3e102"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the main cause of HIV-1 infection in children?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:01<00:12,  1.28s/ Batches]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:08,  1.09 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:02,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:05<00:02,  1.46 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.48 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:07<00:00,  1.38 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26973, -26956) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-19766, -19714) with a span answer. \n",
            "  1%|          | 1/100 [00:08<13:47,  8.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What plays the crucial role in the Mother to Child Transmission of HIV-1 and what increases the risk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:06,  1.50 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.49 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:05,  1.49 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:02<00:04,  1.49 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.49 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.49 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:04<00:02,  1.49 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:05<00:02,  1.49 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.49 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:06<00:00,  1.48 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:07<00:00,  1.50 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31976, -31966) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21838, -21822) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32255, -32245) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31706, -31676) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31274, -31241) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30113, -30094) with a span answer. \n",
            "  2%|▏         | 2/100 [00:16<13:33,  8.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many children were infected by HIV-1 in 2008-2009, worldwide?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.50 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:07,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:06,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:02<00:06,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:05,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:04,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:04<00:04,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:05<00:03,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:02,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:06<00:02,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:07<00:01,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:08<00:00,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:08<00:00,  1.57 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17891, -17828) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32478, -32445) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28365, -28356) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29317, -29316) with a span answer. \n",
            "  3%|▎         | 3/100 [00:25<13:41,  8.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the role of C-C Motif Chemokine Ligand 3 Like 1 (CCL3L1) in mother to child transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.49 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:06,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:02<00:05,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:04,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:04<00:03,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:05<00:02,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:06<00:01,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:07<00:00,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.49 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21838, -21772) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21432, -21356) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21838, -21832) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30319, -30303) with a span answer. \n",
            "  4%|▍         | 4/100 [00:34<13:54,  8.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is DC-GENR and where is  it expressed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:08,  1.49 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:08,  1.46 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:07,  1.47 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:06,  1.46 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:07,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:05<00:04,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:02,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:07<00:02,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:08<00:01,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:09<00:00,  1.49 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30635, -30629) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25084, -25074) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29733, -29716) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-16389, -16365) with a span answer. \n",
            "  5%|▌         | 5/100 [00:44<14:47,  9.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does the presence of DC-SIGNR affect the MTCT of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:09,  1.48 Batches/s]\u001b[A\n",
            "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:08,  1.45 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 3/15 [00:02<00:08,  1.45 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:07,  1.45 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 5/15 [00:03<00:06,  1.45 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 6/15 [00:04<00:06,  1.45 Batches/s]\u001b[A\n",
            "Inferencing Samples:  47%|████▋     | 7/15 [00:04<00:05,  1.45 Batches/s]\u001b[A\n",
            "Inferencing Samples:  53%|█████▎    | 8/15 [00:05<00:04,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 9/15 [00:06<00:04,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 10/15 [00:06<00:03,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 11/15 [00:07<00:02,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 12/15 [00:08<00:02,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  87%|████████▋ | 13/15 [00:08<00:01,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 14/15 [00:09<00:00,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 15/15 [00:10<00:00,  1.48 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-12662, -12588) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23913, -23876) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-12720, -12588) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27947, -27912) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17836, -17828) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24526, -24511) with a span answer. \n",
            "  6%|▌         | 6/100 [00:55<15:15,  9.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why do low levels of DC-SIGNR enhance Mother to Child Transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:08,  1.46 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:08,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:07,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:06,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:05,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:04<00:04,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:05<00:04,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:06<00:02,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:07<00:02,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:08<00:01,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:09<00:00,  1.53 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27958, -27883) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-6768, -6755) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26287, -26252) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31747, -31711) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20331, -19967) with a span answer. \n",
            "  7%|▋         | 7/100 [01:04<14:58,  9.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the percentage of Mother to Child Transmission of HIV-1, when there is no intervention?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.46 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:07,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:06,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:02<00:06,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:05,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:04,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:04<00:04,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:05<00:03,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:02,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:06<00:02,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:07<00:01,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:08<00:00,  1.54 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29961, -29864) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24871, -24870) with a span answer. \n",
            "  8%|▊         | 8/100 [01:13<14:23,  9.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Does C-C chemokine receptor type 5 (CCR5) affect the transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:08,  1.45 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:08,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:07,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:07,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:05,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:04<00:04,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:05<00:04,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:02,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:07<00:02,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:08<00:01,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:09<00:00,  1.45 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27947, -27912) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32134, -32075) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31410, -31312) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22134, -22041) with a span answer. \n",
            "  9%|▉         | 9/100 [01:23<14:32,  9.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does Mannanose Binding Lectin (MBL) affect elimination of HIV-1 pathogen?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:08,  1.45 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:08,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:07,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:07,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:05,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:04<00:04,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:05<00:04,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:02,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:07<00:02,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:08<00:01,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:09<00:00,  1.44 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30252, -30204) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24526, -24511) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29072, -28945) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27947, -27912) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23049, -23036) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29917, -29886) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24495, -24437) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20369, -20328) with a span answer. \n",
            " 10%|█         | 10/100 [01:34<14:56,  9.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How can CCR5's effect in HIV-1 transmission be reduced?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:08,  1.45 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:08,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:07,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:07,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:05,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:04<00:04,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:05<00:04,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:02,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:07<00:02,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:08<00:01,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:09<00:00,  1.47 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28003, -27958) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27947, -27912) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-12652, -12585) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24526, -24511) with a span answer. \n",
            " 11%|█         | 11/100 [01:44<14:44,  9.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is IFITM?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.45 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:04,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:02<00:03,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:02,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:04<00:01,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:05<00:00,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.41 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28626, -28601) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32142, -32067) with a span answer. \n",
            " 12%|█▏        | 12/100 [01:51<13:18,  9.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cysteine residues are contained in the first transmembrane domain of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:02<00:06,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:05,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:04,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:04<00:04,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:05<00:03,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:02,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:07<00:01,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:08<00:00,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:08<00:00,  1.49 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30511, -30505) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29640, -29638) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27276, -27225) with a span answer. \n",
            " 13%|█▎        | 13/100 [02:01<13:26,  9.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What inhibits S-palmitoylation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:08,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:08,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:05,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:05<00:04,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:02,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:07<00:02,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:08<00:01,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:09<00:00,  1.46 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31151, -31130) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17870, -17865) with a span answer. \n",
            " 14%|█▍        | 14/100 [02:11<13:52,  9.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What interaction is inhibited by the presence of 2-bromopalmitic acid (2BP)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:09,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:09,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 3/15 [00:02<00:08,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 5/15 [00:03<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 6/15 [00:04<00:06,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  47%|████▋     | 7/15 [00:05<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  53%|█████▎    | 8/15 [00:05<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 9/15 [00:06<00:04,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 10/15 [00:07<00:03,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 11/15 [00:07<00:02,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 12/15 [00:08<00:02,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  87%|████████▋ | 13/15 [00:09<00:01,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 15/15 [00:10<00:00,  1.49 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23189, -23171) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28373, -28362) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28029, -28006) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29080, -29066) with a span answer. \n",
            " 15%|█▌        | 15/100 [02:22<14:21, 10.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a function associated with IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:06,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:05,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:02<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:02,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:05<00:02,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:07<00:00,  1.48 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20057, -20043) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27993, -27968) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31737, -31696) with a span answer. \n",
            " 16%|█▌        | 16/100 [02:30<13:10,  9.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the antiviral activity of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:09,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:09,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 3/15 [00:02<00:08,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:07,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 5/15 [00:03<00:07,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 6/15 [00:04<00:06,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  47%|████▋     | 7/15 [00:05<00:05,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  53%|█████▎    | 8/15 [00:05<00:05,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 9/15 [00:06<00:04,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 10/15 [00:07<00:03,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 11/15 [00:07<00:02,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 12/15 [00:08<00:02,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  87%|████████▋ | 13/15 [00:09<00:01,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 14/15 [00:10<00:00,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 15/15 [00:10<00:00,  1.44 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27213, -27182) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20329, -20312) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20879, -20825) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27612, -27606) with a span answer. \n",
            " 17%|█▋        | 17/100 [02:42<13:54, 10.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is another name for IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:05,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:02<00:05,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:02,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:05<00:02,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:07<00:00,  1.39 Batches/s]\n",
            " 18%|█▊        | 18/100 [02:51<13:17,  9.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the expression of IFITM5 not promoted by interferons?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:08,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:07,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:05,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:05<00:04,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:02,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:08<00:01,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.38 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26078, -25985) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26916, -26850) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32023, -31922) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28003, -27958) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23410, -23251) with a span answer. \n",
            " 19%|█▉        | 19/100 [03:01<13:26,  9.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM5 and the other IFITM proteins?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:09,  1.44 Batches/s]\u001b[A\n",
            "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:09,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 3/15 [00:02<00:08,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:08,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 5/15 [00:03<00:07,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 6/15 [00:04<00:06,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  47%|████▋     | 7/15 [00:05<00:05,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  53%|█████▎    | 8/15 [00:05<00:05,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 9/15 [00:06<00:04,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 10/15 [00:07<00:03,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 11/15 [00:08<00:02,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 12/15 [00:08<00:02,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  87%|████████▋ | 13/15 [00:09<00:01,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 15/15 [00:10<00:00,  1.45 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29471, -29442) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-15797, -15782) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25015, -25001) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24413, -24389) with a span answer. \n",
            " 20%|██        | 20/100 [03:13<13:51, 10.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM 1, IFITM 2, and IFITM 3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:09,  1.43 Batches/s]\u001b[A\n",
            "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:09,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 3/15 [00:02<00:08,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:08,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 5/15 [00:03<00:07,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 6/15 [00:04<00:06,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  47%|████▋     | 7/15 [00:05<00:05,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  53%|█████▎    | 8/15 [00:05<00:05,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 9/15 [00:06<00:04,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 10/15 [00:07<00:03,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 11/15 [00:08<00:02,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 12/15 [00:08<00:02,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  87%|████████▋ | 13/15 [00:09<00:01,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 15/15 [00:10<00:00,  1.45 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29471, -29442) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25015, -24959) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24413, -24389) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25015, -25001) with a span answer. \n",
            " 21%|██        | 21/100 [03:23<13:49, 10.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What amino acid might be involved in calcium binding in the C-terminal region of a protein?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:09,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:09,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 3/15 [00:02<00:08,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:08,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 5/15 [00:03<00:07,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 6/15 [00:04<00:06,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  47%|████▋     | 7/15 [00:05<00:05,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  53%|█████▎    | 8/15 [00:05<00:05,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 9/15 [00:06<00:04,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 10/15 [00:07<00:03,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 11/15 [00:08<00:02,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 12/15 [00:08<00:02,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  87%|████████▋ | 13/15 [00:09<00:01,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 14/15 [00:10<00:00,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 15/15 [00:10<00:00,  1.41 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21950, -21945) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27956, -27950) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17351, -17345) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25119, -25106) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27673, -27336) with a span answer. \n",
            " 22%|██▏       | 22/100 [03:34<13:50, 10.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.42 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:02<00:05,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:05<00:02,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.35 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31008, -31004) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-4960, -4952) with a span answer. \n",
            " 23%|██▎       | 23/100 [03:44<13:05, 10.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the molecular structure of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:09,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:09,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 3/15 [00:02<00:08,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:08,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 5/15 [00:03<00:07,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 6/15 [00:04<00:06,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  47%|████▋     | 7/15 [00:05<00:05,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  53%|█████▎    | 8/15 [00:05<00:05,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 9/15 [00:06<00:04,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 10/15 [00:07<00:03,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 11/15 [00:08<00:02,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 12/15 [00:08<00:02,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  87%|████████▋ | 13/15 [00:09<00:01,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 14/15 [00:10<00:00,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 15/15 [00:10<00:00,  1.37 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-2586, -2583) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-18192, -18170) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-15613, -15599) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23740, -23734) with a span answer. \n",
            " 24%|██▍       | 24/100 [03:55<13:21, 10.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many nucleotides does bovine coronavirus contain?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:08,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:07,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:05,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:05<00:04,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:03,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:08<00:01,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.35 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-15797, -15782) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-18718, -18715) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-2586, -2584) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29522, -29519) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23891, -23885) with a span answer. \n",
            " 25%|██▌       | 25/100 [04:06<13:16, 10.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of the orf1ab gene in bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:08,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:02<00:06,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:05,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:05,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:05<00:04,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:05<00:03,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:02,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:08<00:01,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:09<00:00,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:09<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-18192, -18170) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25106, -25102) with a span answer. \n",
            " 26%|██▌       | 26/100 [04:16<13:01, 10.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is the orf1ab gene at the 3' or 5' end of the bovine coronavirus genome?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:08,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:03<00:07,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:06,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:06<00:04,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:03,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:09<00:01,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.37 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25856, -25853) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-19612, -19611) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-18874, -18872) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-14240, -14234) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29143, -29134) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29966, -29957) with a span answer. \n",
            " 27%|██▋       | 27/100 [04:27<13:06, 10.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a significant cause of Influenze like illness among healthy adolescents and adults presenting for medical evaluation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:02<00:05,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:08<00:00,  1.33 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30624, -30596) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26699, -26685) with a span answer. \n",
            " 28%|██▊       | 28/100 [04:36<12:20, 10.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the most common species of Human Coronavirus among adults?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26118, -26114) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-12836, -12834) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23367, -23354) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-13402, -13396) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-18071, -18060) with a span answer. \n",
            " 29%|██▉       | 29/100 [04:45<11:41,  9.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which Human Coronavirus showed species specific clinical characteristics of its infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  10%|█         | 1/10 [00:00<00:06,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 2/10 [00:01<00:05,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  30%|███       | 3/10 [00:02<00:05,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 4/10 [00:03<00:04,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 5/10 [00:03<00:03,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 6/10 [00:04<00:03,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  70%|███████   | 7/10 [00:05<00:02,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 8/10 [00:06<00:01,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 10/10 [00:06<00:00,  1.47 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31407, -31386) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26049, -26032) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21977, -21962) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25281, -25273) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-19355, -19350) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25540, -25536) with a span answer. \n",
            " 30%|███       | 30/100 [04:53<10:43,  9.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What causes the outbreak of SARS and MERS.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  10%|█         | 1/10 [00:00<00:06,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 2/10 [00:01<00:05,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  30%|███       | 3/10 [00:02<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 4/10 [00:03<00:05,  1.18 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 5/10 [00:04<00:04,  1.22 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 6/10 [00:04<00:03,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples:  70%|███████   | 7/10 [00:05<00:02,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 8/10 [00:06<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  90%|█████████ | 9/10 [00:07<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 10/10 [00:07<00:00,  1.37 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-2176, -2142) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30624, -30596) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22796, -22766) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20419, -20405) with a span answer. \n",
            " 31%|███       | 31/100 [05:01<10:10,  8.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the case fatality rate of SARS and MERS?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  10%|█         | 1/10 [00:00<00:06,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 2/10 [00:01<00:06,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  30%|███       | 3/10 [00:02<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 4/10 [00:03<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 5/10 [00:03<00:03,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 6/10 [00:04<00:03,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  70%|███████   | 7/10 [00:05<00:02,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 8/10 [00:06<00:01,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  90%|█████████ | 9/10 [00:06<00:00,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 10/10 [00:07<00:00,  1.36 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30217, -30212) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29847, -29845) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-2581, -2578) with a span answer. \n",
            " 32%|███▏      | 32/100 [05:09<09:46,  8.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What were the common HCOV strains in the 5 year USA study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.43 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22587, -22557) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29558, -29520) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27572, -27554) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27078, -27046) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-18401, -18369) with a span answer. \n",
            " 33%|███▎      | 33/100 [05:18<09:50,  8.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which species are more prevalent but less severe?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.43 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29912, -29900) with a span answer. \n",
            " 34%|███▍      | 34/100 [05:25<09:04,  8.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is required for a Hepatitis B infection in cells?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32251, -32239) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31574, -31549) with a span answer. \n",
            " 35%|███▌      | 35/100 [05:35<09:22,  8.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the broad, but less specific, virus-cell interaction in a hepatitis B infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:09,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:03<00:07,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:06<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:09<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24526, -24511) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27213, -27182) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-18860, -18819) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-15520, -15501) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20329, -20312) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20530, -20508) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29763, -29750) with a span answer. \n",
            " 36%|███▌      | 36/100 [05:46<10:00,  9.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which protein domain of the Hepatitis B envelope is necessary for infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.35 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28993, -28988) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-19726, -19714) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31302, -31293) with a span answer. \n",
            " 37%|███▋      | 37/100 [05:56<09:58,  9.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Where is NTCP located in the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.33 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23744, -23730) with a span answer. \n",
            " 38%|███▊      | 38/100 [06:06<09:56,  9.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What does the NTCP protein mediate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:08,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:03<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:05<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:06<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:08<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:09<00:00,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:09<00:00,  1.31 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31864, -31790) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29187, -29134) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22124, -22118) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30213, -30180) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-9454, -9439) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-9454, -9060) with a span answer. \n",
            " 39%|███▉      | 39/100 [06:17<10:10, 10.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is NTCP sufficient to allow HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.42 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29023, -28974) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27799, -27771) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29432, -29362) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23607, -23593) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30228, -30187) with a span answer. \n",
            " 40%|████      | 40/100 [06:26<09:47,  9.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is NTCP thought to not be sufficient for HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:08,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:03<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:05<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:06<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:08<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:09<00:00,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:09<00:00,  1.31 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31858, -31833) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28388, -28362) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32463, -31922) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-9177, -9110) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27958, -27883) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-10942, -10925) with a span answer. \n",
            " 41%|████      | 41/100 [06:37<09:57, 10.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kinds of viruses are Japanese encephalitis virus(JEV), tick-borne encephalitis virus(TBEV), eastern equine encephalitis virus (EEEV), sindbis virus(SV), and dengue virus(DV)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:08<00:00,  1.32 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31977, -31958) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28616, -28603) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24526, -24511) with a span answer. \n",
            " 42%|████▏     | 42/100 [06:46<09:30,  9.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the current clinically-available methods to detect encephalitis viral antigens?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:08<00:00,  1.31 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27776, -27764) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31682, -31669) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28607, -28571) with a span answer. \n",
            " 43%|████▎     | 43/100 [06:55<09:10,  9.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What methods exist for detecting multiple antigens simultaneously in a one-sample, laboratory test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:08,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:05<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:08<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:09<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:09<00:00,  1.35 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22943, -22875) with a span answer. \n",
            " 44%|████▍     | 44/100 [07:06<09:16,  9.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many antigens could be detected by Liew's multiplex ELISA test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.33 Batches/s]\n",
            " 45%|████▌     | 45/100 [07:13<08:26,  9.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kind of antibodies were used in the ELISA-array assay?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:09,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:03<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:06<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:09<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.37 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27430, -27425) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32467, -32444) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21670, -21651) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23863, -23847) with a span answer. \n",
            " 46%|████▌     | 46/100 [07:25<08:54,  9.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:08,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:05<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:08<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:09<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:09<00:00,  1.38 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24206, -24184) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28367, -28336) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27419, -27403) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29300, -29284) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31852, -31836) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27372, -27324) with a span answer. \n",
            " 47%|████▋     | 47/100 [07:35<08:52, 10.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What capture antibodies were used in the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.35 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23863, -23847) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31675, -31671) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29775, -29767) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32179, -32156) with a span answer. \n",
            " 48%|████▊     | 48/100 [07:45<08:38,  9.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the spotting concentration range for the capture antibodies?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:08,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:05<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:08<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:09<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:09<00:00,  1.37 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17340, -17334) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28303, -28297) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-16698, -16693) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25378, -25374) with a span answer. \n",
            " 49%|████▉     | 49/100 [07:55<08:35, 10.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the proper spotting concentration determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.39 Batches/s]\n",
            " 50%|█████     | 50/100 [08:03<07:41,  9.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was cross reaction detection determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  10%|█         | 1/10 [00:00<00:06,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 2/10 [00:01<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  30%|███       | 3/10 [00:02<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 4/10 [00:03<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 5/10 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 6/10 [00:04<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  70%|███████   | 7/10 [00:05<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 8/10 [00:06<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  90%|█████████ | 9/10 [00:06<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 10/10 [00:07<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32417, -32400) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26082, -26067) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32414, -32400) with a span answer. \n",
            " 51%|█████     | 51/100 [08:10<07:05,  8.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA-array assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:08,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:05<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:08<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:09<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:09<00:00,  1.33 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24206, -24184) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32253, -32175) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27419, -27403) with a span answer. \n",
            " 52%|█████▏    | 52/100 [08:21<07:26,  9.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  In 2010, how many cases of tuberculosis were estimated in China?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  10%|█         | 1/10 [00:00<00:06,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 2/10 [00:01<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  30%|███       | 3/10 [00:02<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 4/10 [00:03<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 5/10 [00:03<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 6/10 [00:04<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  70%|███████   | 7/10 [00:05<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 8/10 [00:06<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  90%|█████████ | 9/10 [00:06<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 10/10 [00:07<00:00,  1.34 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25866, -25847) with a span answer. \n",
            " 53%|█████▎    | 53/100 [08:29<07:02,  8.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the population of Shandong province?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/8 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  12%|█▎        | 1/8 [00:00<00:05,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 2/8 [00:01<00:04,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 3/8 [00:02<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 4/8 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▎   | 5/8 [00:03<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 6/8 [00:04<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  88%|████████▊ | 7/8 [00:05<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 8/8 [00:05<00:00,  1.43 Batches/s]\n",
            " 54%|█████▍    | 54/100 [08:35<06:14,  8.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the purpose of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:07<00:00,  1.41 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29087, -28897) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30435, -30385) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28500, -28447) with a span answer. \n",
            " 55%|█████▌    | 55/100 [08:44<06:11,  8.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the age range for the people surveyed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:10,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:09,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 3/15 [00:02<00:09,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 4/15 [00:03<00:08,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 5/15 [00:03<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 6/15 [00:04<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  47%|████▋     | 7/15 [00:05<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  53%|█████▎    | 8/15 [00:06<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 9/15 [00:06<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 10/15 [00:07<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 11/15 [00:08<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 12/15 [00:09<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  87%|████████▋ | 13/15 [00:09<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 14/15 [00:10<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 15/15 [00:11<00:00,  1.30 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29219, -29208) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-1720, -1694) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31174, -31162) with a span answer. \n",
            " 56%|█████▌    | 56/100 [08:56<06:51,  9.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the survey designed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.30 Batches/s]\n",
            " 57%|█████▋    | 57/100 [09:03<06:13,  8.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Was was the sample size?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:08<00:00,  1.36 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24426, -24409) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17577, -17572) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-19691, -19652) with a span answer. \n",
            " 58%|█████▊    | 58/100 [09:11<06:01,  8.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How were the clusters selected?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:08,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:05<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:08<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:09<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:09<00:00,  1.32 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22118, -22086) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24526, -24399) with a span answer. \n",
            " 59%|█████▉    | 59/100 [09:21<06:12,  9.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many people were in a community cluster?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.40 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-308, -300) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-13570, -13566) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-302, -300) with a span answer. \n",
            " 60%|██████    | 60/100 [09:28<05:35,  8.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who was excluded from the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/8 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  12%|█▎        | 1/8 [00:00<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 2/8 [00:01<00:04,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 3/8 [00:02<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 4/8 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▎   | 5/8 [00:03<00:02,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 6/8 [00:04<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  88%|████████▊ | 7/8 [00:05<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 8/8 [00:05<00:00,  1.39 Batches/s]\n",
            " 61%|██████    | 61/100 [09:34<05:01,  7.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When was the study conducted?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.41 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22290, -22257) with a span answer. \n",
            " 62%|██████▏   | 62/100 [09:41<04:40,  7.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who conducted the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/8 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  12%|█▎        | 1/8 [00:00<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 2/8 [00:01<00:04,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 3/8 [00:02<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 4/8 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▎   | 5/8 [00:03<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 6/8 [00:04<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  88%|████████▊ | 7/8 [00:05<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 8/8 [00:06<00:00,  1.33 Batches/s]\n",
            " 63%|██████▎   | 63/100 [09:47<04:20,  7.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medium was used to collect the sputum samples?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:08,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:05<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:08<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:09<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:09<00:00,  1.36 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21642, -21630) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24305, -24289) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22633, -22605) with a span answer. \n",
            " 64%|██████▍   | 64/100 [09:57<04:44,  7.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the response rate for the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:08<00:00,  1.34 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23707, -23693) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22082, -22070) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25095, -25092) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25378, -25374) with a span answer. \n",
            " 65%|██████▌   | 65/100 [10:06<04:42,  8.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the average age of a study participant?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:09,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:03<00:07,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:06<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:09<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:10<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.35 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29219, -29208) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24414, -24401) with a span answer. \n",
            " 66%|██████▌   | 66/100 [10:16<05:01,  8.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the prevalence rate in Shandong in 2010 for sputum positive cases of tuberculosis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.43 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25378, -25374) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-437, -428) with a span answer. \n",
            " 67%|██████▋   | 67/100 [10:23<04:29,  8.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the most striking finding of the study regarding tuberculosis patients?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  10%|█         | 1/10 [00:00<00:06,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 2/10 [00:01<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  30%|███       | 3/10 [00:02<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 4/10 [00:03<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 5/10 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 6/10 [00:04<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  70%|███████   | 7/10 [00:05<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 8/10 [00:06<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  90%|█████████ | 9/10 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 10/10 [00:07<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26443, -26412) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17799, -17717) with a span answer. \n",
            " 68%|██████▊   | 68/100 [10:30<04:14,  7.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cases of sputum positive tuberculosis patients had no persistent cough?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-10801, -10782) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-5733, -5727) with a span answer. \n",
            " 69%|██████▉   | 69/100 [10:39<04:15,  8.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many tuberculosis patients in Shandong were over 65 years old?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.33 Batches/s]\n",
            " 70%|███████   | 70/100 [10:46<03:55,  7.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What enzymes have been reported to be linked with severity of infection and various pathological conditions caused by microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:09,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:03<00:07,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:06<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:09<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:10<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.30 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-14862, -14855) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28729, -28723) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-18860, -18819) with a span answer. \n",
            " 71%|███████   | 71/100 [10:57<04:16,  8.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  At what temperatures was the assay completed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:09<00:00,  1.33 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31706, -31664) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27690, -27686) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22372, -22342) with a span answer. \n",
            " 72%|███████▏  | 72/100 [11:07<04:11,  8.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What criteria sets the guideline for drug-like properties?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:08,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:03<00:07,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:05<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:08<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 12/13 [00:09<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:09<00:00,  1.34 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26293, -26264) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25516, -25476) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23762, -23730) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30602, -30576) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17340, -17319) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-16789, -16753) with a span answer. \n",
            " 73%|███████▎  | 73/100 [11:17<04:11,  9.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What could be novel candidates as potent inhibitors of papain like cysteine proteases in resistant microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:09,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:03<00:07,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:06<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:09<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:10<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.30 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29336, -29323) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21424, -21415) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30656, -30633) with a span answer. \n",
            " 74%|███████▍  | 74/100 [11:28<04:16,  9.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What method is useful in administering small molecules for systemic delivery to the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:10,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:09,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 3/15 [00:02<00:09,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 4/15 [00:03<00:08,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 5/15 [00:03<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 6/15 [00:04<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  47%|████▋     | 7/15 [00:05<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  53%|█████▎    | 8/15 [00:06<00:05,  1.19 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 9/15 [00:07<00:04,  1.21 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 10/15 [00:07<00:04,  1.23 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 11/15 [00:08<00:03,  1.24 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 12/15 [00:09<00:02,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  87%|████████▋ | 13/15 [00:10<00:01,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 14/15 [00:11<00:00,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 15/15 [00:11<00:00,  1.34 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22539, -22527) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25895, -25881) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27225, -27208) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32564, -32554) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28965, -28943) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-13242, -13229) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27080, -27062) with a span answer. \n",
            " 75%|███████▌  | 75/100 [11:39<04:19, 10.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the nasal mucosa useful in the delivery of small molecules into the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:09,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:03<00:07,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:06<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:09<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:10<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.30 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29674, -29644) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32619, -32517) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20879, -20825) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25861, -25783) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22960, -22929) with a span answer. \n",
            " 76%|███████▌  | 76/100 [11:51<04:14, 10.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the most common methods of inhaled delivery of medications?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:07<00:00,  1.42 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30822, -30786) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22539, -22481) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22539, -22527) with a span answer. \n",
            " 77%|███████▋  | 77/100 [11:59<03:46,  9.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medications have shown good promise to in vivo delivery via dry powder inhalers?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:09,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:03<00:07,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:06<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:09<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31706, -31676) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30847, -30841) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31052, -31032) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28607, -28571) with a span answer. \n",
            " 78%|███████▊  | 78/100 [12:09<03:40, 10.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How are siRNAs typically delivered for systemic effect?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/16 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   6%|▋         | 1/16 [00:00<00:10,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  12%|█▎        | 2/16 [00:01<00:10,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  19%|█▉        | 3/16 [00:02<00:10,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 4/16 [00:03<00:09,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███▏      | 5/16 [00:03<00:08,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 6/16 [00:04<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 7/16 [00:05<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 8/16 [00:06<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▋    | 9/16 [00:06<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▎   | 10/16 [00:07<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 11/16 [00:08<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 12/16 [00:09<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  81%|████████▏ | 13/16 [00:10<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  88%|████████▊ | 14/16 [00:10<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  94%|█████████▍| 15/16 [00:11<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 16/16 [00:12<00:00,  1.32 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20010, -20008) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21481, -21438) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-18674, -18581) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26810, -26798) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23707, -23693) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27119, -27096) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32564, -32554) with a span answer. \n",
            " 79%|███████▉  | 79/100 [12:22<03:46, 10.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What structures form the human airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.37 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22960, -22940) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20329, -20312) with a span answer. \n",
            " 80%|████████  | 80/100 [12:31<03:25, 10.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What size of particle has been shown to be most effective in the delivery to the lower airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:09,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:03<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:06<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:09<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 13/14 [00:09<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.32 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25894, -25872) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20530, -20508) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22429, -22415) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28332, -28325) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29778, -29766) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24727, -24715) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21577, -21556) with a span answer. \n",
            " 81%|████████  | 81/100 [12:42<03:18, 10.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the essential conditions in siRNA delivery to effectively produce gene silencing in the lungs?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/16 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   6%|▋         | 1/16 [00:00<00:10,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  12%|█▎        | 2/16 [00:01<00:10,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  19%|█▉        | 3/16 [00:02<00:10,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 4/16 [00:03<00:09,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███▏      | 5/16 [00:03<00:08,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 6/16 [00:04<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 7/16 [00:05<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 8/16 [00:06<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▋    | 9/16 [00:06<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▎   | 10/16 [00:07<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 11/16 [00:08<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 12/16 [00:09<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  81%|████████▏ | 13/16 [00:10<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  88%|████████▊ | 14/16 [00:10<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  94%|█████████▍| 15/16 [00:11<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 16/16 [00:12<00:00,  1.33 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24526, -24511) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29170, -28981) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22803, -22776) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28467, -28428) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24736, -24715) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25634, -25535) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22960, -22929) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20329, -20312) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26873, -26831) with a span answer. \n",
            " 82%|████████▏ | 82/100 [12:54<03:19, 11.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long is the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:10,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:09,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 3/15 [00:02<00:09,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 4/15 [00:03<00:08,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 5/15 [00:03<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 6/15 [00:04<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  47%|████▋     | 7/15 [00:05<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  53%|█████▎    | 8/15 [00:06<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 9/15 [00:06<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 10/15 [00:07<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 11/15 [00:08<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 12/15 [00:09<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  87%|████████▋ | 13/15 [00:10<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  93%|█████████▎| 14/15 [00:10<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 15/15 [00:11<00:00,  1.36 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-13795, -13787) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29822, -29808) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-1720, -1694) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-2586, -2583) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31915, -31890) with a span answer. \n",
            " 83%|████████▎ | 83/100 [13:05<03:09, 11.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many open reading frames are in the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.36 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-2586, -2584) with a span answer. \n",
            " 84%|████████▍ | 84/100 [13:15<02:48, 10.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What virus has the closest genetic identity with the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  10%|█         | 1/10 [00:00<00:06,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 2/10 [00:01<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  30%|███       | 3/10 [00:02<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 4/10 [00:03<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 5/10 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 6/10 [00:04<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  70%|███████   | 7/10 [00:05<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 8/10 [00:06<00:01,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  90%|█████████ | 9/10 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 10/10 [00:07<00:00,  1.36 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24533, -24524) with a span answer. \n",
            " 85%|████████▌ | 85/100 [13:22<02:24,  9.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many surgical masks or respirators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.36 Batches/s]\n",
            " 86%|████████▌ | 86/100 [13:29<02:03,  8.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the acronym MERS-CoV?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  10%|█         | 1/10 [00:00<00:06,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 2/10 [00:01<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  30%|███       | 3/10 [00:02<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 4/10 [00:03<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 5/10 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 6/10 [00:04<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  70%|███████   | 7/10 [00:05<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 8/10 [00:06<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  90%|█████████ | 9/10 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 10/10 [00:07<00:00,  1.38 Batches/s]\n",
            " 87%|████████▋ | 87/100 [13:37<01:49,  8.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the critical factors that determine the effect of an epidemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.40 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17810, -17717) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29559, -29503) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20608, -20454) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25924, -25879) with a span answer. \n",
            " 88%|████████▊ | 88/100 [13:45<01:42,  8.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When did the World Health Organization (WHO) officially declare the 2019-nCoV epidemic as a Public Health Emergency of International Concern?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.37 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23897, -23888) with a span answer. \n",
            " 89%|████████▉ | 89/100 [13:52<01:29,  8.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What influenza virus was identified in China in 2013?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.38 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24818, -24805) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30572, -30564) with a span answer. \n",
            " 90%|█████████ | 90/100 [13:59<01:16,  7.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What past research has been done on severe, single-wave pandemics?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.31 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32536, -32475) with a span answer. \n",
            " 91%|█████████ | 91/100 [14:06<01:07,  7.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a clinical attack rate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:09<00:00,  1.31 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32273, -32268) with a span answer. \n",
            " 92%|█████████▏| 92/100 [14:16<01:04,  8.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the clinical attack rate in the 2009 H1N1 pandemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:09,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:09,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:08,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 4/14 [00:03<00:07,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 6/14 [00:04<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 7/14 [00:05<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 8/14 [00:06<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▍   | 9/14 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 10/14 [00:07<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  79%|███████▊  | 11/14 [00:08<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 12/14 [00:09<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 14/14 [00:10<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27695, -27686) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27684, -27683) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27366, -27338) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-16000, -15993) with a span answer. \n",
            " 93%|█████████▎| 93/100 [14:26<01:01,  8.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the estimated R0 of COVID-19?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/8 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  12%|█▎        | 1/8 [00:00<00:05,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 2/8 [00:01<00:04,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 3/8 [00:02<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 4/8 [00:03<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▎   | 5/8 [00:03<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 6/8 [00:04<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  88%|████████▊ | 7/8 [00:05<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 8/8 [00:06<00:00,  1.30 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20167, -20119) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-15247, -15243) with a span answer. \n",
            " 94%|█████████▍| 94/100 [14:33<00:48,  8.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many ventilators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:08,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:08,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  23%|██▎       | 3/13 [00:02<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  31%|███       | 4/13 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 5/13 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  46%|████▌     | 6/13 [00:04<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  54%|█████▍    | 7/13 [00:05<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▏   | 8/13 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  69%|██████▉   | 9/13 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  77%|███████▋  | 10/13 [00:07<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  85%|████████▍ | 11/13 [00:08<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 13/13 [00:09<00:00,  1.40 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-2924, -2919) with a span answer. \n",
            " 95%|█████████▌| 95/100 [14:42<00:42,  8.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How is exhaled breath condensate used in viral research?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:07<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20576, -20546) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31244, -31224) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26293, -26264) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22542, -22527) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23720, -23693) with a span answer. \n",
            " 96%|█████████▌| 96/100 [14:50<00:33,  8.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many patients were i this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:07<00:00,  1.40 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24526, -24511) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31443, -31399) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23702, -23680) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32136, -32122) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31581, -31578) with a span answer. \n",
            " 97%|█████████▋| 97/100 [14:58<00:25,  8.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the conclusion of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:09<00:00,  1.32 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21412, -21360) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21175, -21111) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17016, -16973) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26287, -26252) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-6170, -6115) with a span answer. \n",
            " 98%|█████████▊| 98/100 [15:08<00:17,  8.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long did the patient breath into the RTube?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:08<00:00,  1.37 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30511, -30483) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29592, -29585) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23707, -23693) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25044, -25035) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-9540, -9536) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-430, -428) with a span answer. \n",
            " 99%|█████████▉| 99/100 [15:17<00:08,  8.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What followed the reverse transcription step in the analysis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:07,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:07,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 3/12 [00:02<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 4/12 [00:03<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  42%|████▏     | 5/12 [00:03<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 6/12 [00:04<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  58%|█████▊    | 7/12 [00:05<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 8/12 [00:06<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 9/12 [00:06<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 10/12 [00:07<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  92%|█████████▏| 11/12 [00:08<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 12/12 [00:09<00:00,  1.31 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17683, -17657) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21286, -21254) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25040, -25024) with a span answer. \n",
            "100%|██████████| 100/100 [15:26<00:00,  9.27s/it]\n"
          ]
        }
      ],
      "source": [
        "retriever = BM25Retriever(document_store=document_store_covid)\n",
        "\n",
        "reader = FARMReader(\n",
        "    model_name_or_path=model, #\"deepset/roberta-base-squad2\"\n",
        "    use_gpu=use_gpu\n",
        ")\n",
        "\n",
        "pipe_covid = ExtractiveQAPipeline(reader, retriever)\n",
        "\n",
        "df_res_covid_bm25  = qa_dataset.copy()\n",
        "\n",
        "# prepare columns for answers\n",
        "df_res_covid_bm25['predictions_covid_context_bm25'] = [list() for x in range(len(df_res_covid_bm25.index))]\n",
        "\n",
        "for q_i in tqdm(range(len(qa_dataset.question.tolist()))):\n",
        "\n",
        "    print('question : ', qa_dataset.question[q_i])\n",
        "\n",
        "    try:\n",
        "        # covid dataset context prediction\n",
        "        prediction_covid = pipe_covid.run(\n",
        "            query=qa_dataset.question[q_i],\n",
        "            params={\n",
        "                \"Retriever\" : {\"top_k\": top_k_retriever},\n",
        "                \"Reader\": {\"top_k\": top_k_reader}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        df_res_covid_bm25.loc[q_i, 'predictions_covid_context_bm25'].append(\n",
        "            [prediction_covid['answers'][k].answer for k in range(len(prediction_covid['answers']))]\n",
        "        )\n",
        "    except:\n",
        "        df_res_covid_bm25.loc[q_i, 'predictions_covid_context_bm25'].append([])\n",
        "\n",
        "    \n",
        "df_res_covid_bm25.to_csv('/content/drive/MyDrive/DeepLearning/df_res_covid_bm25_org_data.csv', index=False)"
      ],
      "id": "6dabea22"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPFfpo9T5gvJ"
      },
      "source": [
        "#### RE-INITIALISE ELASTICSEARCH"
      ],
      "id": "FPFfpo9T5gvJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFkv0KnX7ey-"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "id": "FFkv0KnX7ey-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ffcvat6f7ey-"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "id": "Ffcvat6f7ey-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRyc8fyb7ey_"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(30)"
      ],
      "id": "dRyc8fyb7ey_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K65RwKWB7bmt"
      },
      "source": [
        "Write to document store"
      ],
      "id": "K65RwKWB7bmt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sOnvxjg7bmu",
        "outputId": "9369b229-c584-4617-c5f8-249a2c37fd28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 822 ms, sys: 87.4 ms, total: 909 ms\n",
            "Wall time: 11.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "document_store_covid = ElasticsearchDocumentStore(\n",
        "    port=9200\n",
        ")\n",
        "document_store_covid.delete_documents()\n",
        "document_store_covid.write_documents(\n",
        "    documents=dicts_covid\n",
        ")"
      ],
      "id": "7sOnvxjg7bmu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc835355"
      },
      "source": [
        "#### TF IDF"
      ],
      "id": "bc835355"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6acd905e",
        "outputId": "6c4ab073-55b6-4b83-dc4e-62f477d6c1ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the main cause of HIV-1 infection in children?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.35 Batches/s]\n",
            "  1%|          | 1/100 [00:00<00:35,  2.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What plays the crucial role in the Mother to Child Transmission of HIV-1 and what increases the risk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.60 Batches/s]\n",
            "  2%|▏         | 2/100 [00:00<00:29,  3.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many children were infected by HIV-1 in 2008-2009, worldwide?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.61 Batches/s]\n",
            "  3%|▎         | 3/100 [00:00<00:31,  3.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the role of C-C Motif Chemokine Ligand 3 Like 1 (CCL3L1) in mother to child transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.73 Batches/s]\n",
            "  4%|▍         | 4/100 [00:01<00:30,  3.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is DC-GENR and where is  it expressed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.23 Batches/s]\n",
            "  5%|▌         | 5/100 [00:01<00:32,  2.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does the presence of DC-SIGNR affect the MTCT of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.65 Batches/s]\n",
            "  6%|▌         | 6/100 [00:02<00:34,  2.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why do low levels of DC-SIGNR enhance Mother to Child Transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.02 Batches/s]\n",
            "  7%|▋         | 7/100 [00:02<00:35,  2.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the percentage of Mother to Child Transmission of HIV-1, when there is no intervention?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.67 Batches/s]\n",
            "  8%|▊         | 8/100 [00:02<00:31,  2.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Does C-C chemokine receptor type 5 (CCR5) affect the transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.10 Batches/s]\n",
            "  9%|▉         | 9/100 [00:03<00:29,  3.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does Mannanose Binding Lectin (MBL) affect elimination of HIV-1 pathogen?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\n",
            " 10%|█         | 10/100 [00:03<00:31,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How can CCR5's effect in HIV-1 transmission be reduced?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.35 Batches/s]\n",
            " 11%|█         | 11/100 [00:03<00:29,  3.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is IFITM?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.79 Batches/s]\n",
            " 12%|█▏        | 12/100 [00:03<00:27,  3.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cysteine residues are contained in the first transmembrane domain of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.32 Batches/s]\n",
            " 13%|█▎        | 13/100 [00:04<00:31,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What inhibits S-palmitoylation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.38 Batches/s]\n",
            " 14%|█▍        | 14/100 [00:04<00:29,  2.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What interaction is inhibited by the presence of 2-bromopalmitic acid (2BP)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.47 Batches/s]\n",
            " 15%|█▌        | 15/100 [00:05<00:31,  2.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a function associated with IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.90 Batches/s]\n",
            " 16%|█▌        | 16/100 [00:05<00:36,  2.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the antiviral activity of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\n",
            " 17%|█▋        | 17/100 [00:06<00:35,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is another name for IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.93 Batches/s]\n",
            " 18%|█▊        | 18/100 [00:06<00:38,  2.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the expression of IFITM5 not promoted by interferons?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.78 Batches/s]\n",
            " 19%|█▉        | 19/100 [00:07<00:41,  1.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM5 and the other IFITM proteins?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.73 Batches/s]\n",
            " 20%|██        | 20/100 [00:07<00:43,  1.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM 1, IFITM 2, and IFITM 3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.23 Batches/s]\n",
            " 21%|██        | 21/100 [00:08<00:42,  1.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What amino acid might be involved in calcium binding in the C-terminal region of a protein?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.40 Batches/s]\n",
            " 22%|██▏       | 22/100 [00:08<00:37,  2.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.80 Batches/s]\n",
            " 23%|██▎       | 23/100 [00:09<00:29,  2.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the molecular structure of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.26 Batches/s]\n",
            " 24%|██▍       | 24/100 [00:09<00:24,  3.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many nucleotides does bovine coronavirus contain?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.09 Batches/s]\n",
            " 25%|██▌       | 25/100 [00:09<00:21,  3.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of the orf1ab gene in bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.07 Batches/s]\n",
            " 26%|██▌       | 26/100 [00:09<00:19,  3.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is the orf1ab gene at the 3' or 5' end of the bovine coronavirus genome?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.96 Batches/s]\n",
            " 27%|██▋       | 27/100 [00:10<00:22,  3.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a significant cause of Influenze like illness among healthy adolescents and adults presenting for medical evaluation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.98 Batches/s]\n",
            " 28%|██▊       | 28/100 [00:10<00:20,  3.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the most common species of Human Coronavirus among adults?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.77 Batches/s]\n",
            " 29%|██▉       | 29/100 [00:10<00:18,  3.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which Human Coronavirus showed species specific clinical characteristics of its infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.20 Batches/s]\n",
            " 30%|███       | 30/100 [00:10<00:17,  3.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What causes the outbreak of SARS and MERS.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.93 Batches/s]\n",
            " 31%|███       | 31/100 [00:11<00:18,  3.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the case fatality rate of SARS and MERS?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.48 Batches/s]\n",
            " 32%|███▏      | 32/100 [00:11<00:18,  3.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What were the common HCOV strains in the 5 year USA study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.88 Batches/s]\n",
            " 33%|███▎      | 33/100 [00:11<00:18,  3.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which species are more prevalent but less severe?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.12 Batches/s]\n",
            " 34%|███▍      | 34/100 [00:11<00:18,  3.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is required for a Hepatitis B infection in cells?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.79 Batches/s]\n",
            " 35%|███▌      | 35/100 [00:12<00:17,  3.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the broad, but less specific, virus-cell interaction in a hepatitis B infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.00 Batches/s]\n",
            " 36%|███▌      | 36/100 [00:12<00:16,  3.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which protein domain of the Hepatitis B envelope is necessary for infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.07 Batches/s]\n",
            " 37%|███▋      | 37/100 [00:12<00:16,  3.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Where is NTCP located in the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.12 Batches/s]\n",
            " 38%|███▊      | 38/100 [00:12<00:15,  4.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What does the NTCP protein mediate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.18 Batches/s]\n",
            " 39%|███▉      | 39/100 [00:12<00:13,  4.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is NTCP sufficient to allow HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.80 Batches/s]\n",
            " 40%|████      | 40/100 [00:13<00:12,  4.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is NTCP thought to not be sufficient for HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.84 Batches/s]\n",
            " 41%|████      | 41/100 [00:13<00:12,  4.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kinds of viruses are Japanese encephalitis virus(JEV), tick-borne encephalitis virus(TBEV), eastern equine encephalitis virus (EEEV), sindbis virus(SV), and dengue virus(DV)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 42%|████▏     | 42/100 [00:13<00:12,  4.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the current clinically-available methods to detect encephalitis viral antigens?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.64 Batches/s]\n",
            " 43%|████▎     | 43/100 [00:13<00:12,  4.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What methods exist for detecting multiple antigens simultaneously in a one-sample, laboratory test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.71 Batches/s]\n",
            " 44%|████▍     | 44/100 [00:14<00:11,  4.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many antigens could be detected by Liew's multiplex ELISA test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.96 Batches/s]\n",
            " 45%|████▌     | 45/100 [00:14<00:12,  4.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kind of antibodies were used in the ELISA-array assay?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.66 Batches/s]\n",
            " 46%|████▌     | 46/100 [00:14<00:12,  4.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.98 Batches/s]\n",
            " 47%|████▋     | 47/100 [00:14<00:12,  4.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What capture antibodies were used in the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.24 Batches/s]\n",
            " 48%|████▊     | 48/100 [00:15<00:13,  4.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the spotting concentration range for the capture antibodies?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.35 Batches/s]\n",
            " 49%|████▉     | 49/100 [00:15<00:13,  3.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the proper spotting concentration determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.54 Batches/s]\n",
            " 50%|█████     | 50/100 [00:15<00:11,  4.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was cross reaction detection determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.97 Batches/s]\n",
            " 51%|█████     | 51/100 [00:15<00:12,  3.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA-array assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.65 Batches/s]\n",
            " 52%|█████▏    | 52/100 [00:16<00:12,  3.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  In 2010, how many cases of tuberculosis were estimated in China?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.92 Batches/s]\n",
            " 53%|█████▎    | 53/100 [00:16<00:11,  4.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the population of Shandong province?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.35 Batches/s]\n",
            " 54%|█████▍    | 54/100 [00:16<00:10,  4.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the purpose of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.01 Batches/s]\n",
            " 55%|█████▌    | 55/100 [00:16<00:09,  4.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the age range for the people surveyed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.99 Batches/s]\n",
            " 56%|█████▌    | 56/100 [00:16<00:10,  4.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the survey designed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.14 Batches/s]\n",
            " 57%|█████▋    | 57/100 [00:17<00:09,  4.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Was was the sample size?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\n",
            " 58%|█████▊    | 58/100 [00:17<00:11,  3.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How were the clusters selected?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.65 Batches/s]\n",
            " 59%|█████▉    | 59/100 [00:17<00:10,  4.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many people were in a community cluster?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.00 Batches/s]\n",
            " 60%|██████    | 60/100 [00:17<00:09,  4.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who was excluded from the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.84 Batches/s]\n",
            " 61%|██████    | 61/100 [00:18<00:08,  4.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When was the study conducted?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.38 Batches/s]\n",
            " 62%|██████▏   | 62/100 [00:18<00:09,  4.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who conducted the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.82 Batches/s]\n",
            " 63%|██████▎   | 63/100 [00:18<00:09,  4.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medium was used to collect the sputum samples?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.42 Batches/s]\n",
            " 64%|██████▍   | 64/100 [00:18<00:09,  3.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the response rate for the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.74 Batches/s]\n",
            " 65%|██████▌   | 65/100 [00:19<00:10,  3.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the average age of a study participant?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.10 Batches/s]\n",
            " 66%|██████▌   | 66/100 [00:19<00:09,  3.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the prevalence rate in Shandong in 2010 for sputum positive cases of tuberculosis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.03 Batches/s]\n",
            " 67%|██████▋   | 67/100 [00:19<00:08,  3.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the most striking finding of the study regarding tuberculosis patients?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.16 Batches/s]\n",
            " 68%|██████▊   | 68/100 [00:20<00:08,  3.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cases of sputum positive tuberculosis patients had no persistent cough?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.75 Batches/s]\n",
            " 69%|██████▉   | 69/100 [00:20<00:08,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many tuberculosis patients in Shandong were over 65 years old?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.32 Batches/s]\n",
            " 70%|███████   | 70/100 [00:20<00:07,  3.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What enzymes have been reported to be linked with severity of infection and various pathological conditions caused by microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.46 Batches/s]\n",
            " 71%|███████   | 71/100 [00:20<00:07,  3.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  At what temperatures was the assay completed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.60 Batches/s]\n",
            " 72%|███████▏  | 72/100 [00:21<00:06,  4.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What criteria sets the guideline for drug-like properties?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.77 Batches/s]\n",
            " 73%|███████▎  | 73/100 [00:21<00:06,  4.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What could be novel candidates as potent inhibitors of papain like cysteine proteases in resistant microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.32 Batches/s]\n",
            " 74%|███████▍  | 74/100 [00:21<00:08,  2.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What method is useful in administering small molecules for systemic delivery to the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.66 Batches/s]\n",
            " 75%|███████▌  | 75/100 [00:22<00:08,  3.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the nasal mucosa useful in the delivery of small molecules into the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.54 Batches/s]\n",
            " 76%|███████▌  | 76/100 [00:22<00:08,  2.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the most common methods of inhaled delivery of medications?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.57 Batches/s]\n",
            " 77%|███████▋  | 77/100 [00:22<00:08,  2.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medications have shown good promise to in vivo delivery via dry powder inhalers?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.42 Batches/s]\n",
            " 78%|███████▊  | 78/100 [00:23<00:07,  3.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How are siRNAs typically delivered for systemic effect?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\n",
            " 79%|███████▉  | 79/100 [00:23<00:07,  2.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What structures form the human airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.35 Batches/s]\n",
            " 80%|████████  | 80/100 [00:23<00:06,  3.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What size of particle has been shown to be most effective in the delivery to the lower airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.12 Batches/s]\n",
            " 81%|████████  | 81/100 [00:24<00:06,  3.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the essential conditions in siRNA delivery to effectively produce gene silencing in the lungs?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.68 Batches/s]\n",
            " 82%|████████▏ | 82/100 [00:24<00:06,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long is the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.60 Batches/s]\n",
            " 83%|████████▎ | 83/100 [00:24<00:05,  3.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many open reading frames are in the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.52 Batches/s]\n",
            " 84%|████████▍ | 84/100 [00:25<00:04,  3.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What virus has the closest genetic identity with the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.31 Batches/s]\n",
            " 85%|████████▌ | 85/100 [00:25<00:04,  3.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many surgical masks or respirators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.11 Batches/s]\n",
            " 86%|████████▌ | 86/100 [00:25<00:03,  3.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the acronym MERS-CoV?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\n",
            " 87%|████████▋ | 87/100 [00:25<00:03,  3.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the critical factors that determine the effect of an epidemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.75 Batches/s]\n",
            " 88%|████████▊ | 88/100 [00:26<00:03,  3.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When did the World Health Organization (WHO) officially declare the 2019-nCoV epidemic as a Public Health Emergency of International Concern?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.15 Batches/s]\n",
            " 89%|████████▉ | 89/100 [00:26<00:03,  3.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What influenza virus was identified in China in 2013?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.11 Batches/s]\n",
            " 90%|█████████ | 90/100 [00:26<00:02,  3.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What past research has been done on severe, single-wave pandemics?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.29 Batches/s]\n",
            " 91%|█████████ | 91/100 [00:26<00:02,  3.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a clinical attack rate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.11 Batches/s]\n",
            " 92%|█████████▏| 92/100 [00:27<00:02,  3.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the clinical attack rate in the 2009 H1N1 pandemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.57 Batches/s]\n",
            " 93%|█████████▎| 93/100 [00:27<00:01,  3.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the estimated R0 of COVID-19?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.82 Batches/s]\n",
            " 94%|█████████▍| 94/100 [00:27<00:01,  3.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many ventilators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            " 95%|█████████▌| 95/100 [00:28<00:01,  3.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How is exhaled breath condensate used in viral research?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.02 Batches/s]\n",
            " 96%|█████████▌| 96/100 [00:28<00:01,  3.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many patients were i this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.85 Batches/s]\n",
            " 97%|█████████▋| 97/100 [00:28<00:00,  3.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the conclusion of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.18 Batches/s]\n",
            " 98%|█████████▊| 98/100 [00:28<00:00,  4.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long did the patient breath into the RTube?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.29 Batches/s]\n",
            " 99%|█████████▉| 99/100 [00:28<00:00,  4.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What followed the reverse transcription step in the analysis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.05 Batches/s]\n",
            "100%|██████████| 100/100 [00:29<00:00,  3.42it/s]\n"
          ]
        }
      ],
      "source": [
        "retriever_tfidf = TfidfRetriever(document_store=document_store_covid)\n",
        "\n",
        "reader = FARMReader(\n",
        "    model_name_or_path=model, #\"deepset/roberta-base-squad2\"\n",
        "    use_gpu=use_gpu\n",
        ")\n",
        "\n",
        "pipe_covid_tfidf = ExtractiveQAPipeline(reader, retriever_tfidf)\n",
        "\n",
        "df_res_covid_tfidf = qa_dataset.copy()\n",
        "\n",
        "# prepare columns for answers\n",
        "df_res_covid_tfidf['predictions_covid_context_tfidf'] = [list() for x in range(len(df_res_covid_tfidf.index))]\n",
        "\n",
        "for q_i in tqdm(range(len(qa_dataset.question.tolist()))):\n",
        "\n",
        "    print('question : ', qa_dataset.question[q_i])\n",
        "\n",
        "    try:\n",
        "        # covid dataset context prediction\n",
        "        prediction_covid = pipe_covid_tfidf.run(\n",
        "            query=qa_dataset.question[q_i],\n",
        "            params={\n",
        "                \"Retriever\" : {\"top_k\": top_k_retriever},\n",
        "                \"Reader\": {\"top_k\": top_k_reader}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        df_res_covid_tfidf.loc[q_i, 'predictions_covid_context_tfidf'].append(\n",
        "            [prediction_covid['answers'][k].answer for k in range(len(prediction_covid['answers']))]\n",
        "        )\n",
        "    except:\n",
        "        df_res_covid_tfidf.loc[q_i, 'predictions_covid_context_tfidf'].append([])\n",
        "\n",
        "\n",
        "df_res_covid_tfidf.to_csv('/content/drive/MyDrive/DeepLearning/df_res_covid_tfidf_org_data.csv', index=False)"
      ],
      "id": "6acd905e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRpWr-SioNLP"
      },
      "source": [
        "#### RE-INITIALISE ELASTICSEARCH"
      ],
      "id": "BRpWr-SioNLP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9UwUlObomh4"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "id": "V9UwUlObomh4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX3Gpmr4onz9"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "id": "AX3Gpmr4onz9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD82KmIKowL9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(30)"
      ],
      "id": "PD82KmIKowL9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o53jkorpow7F",
        "outputId": "99ec359d-b8e1-4d32-832b-bb66dff1e5dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 825 ms, sys: 117 ms, total: 942 ms\n",
            "Wall time: 11.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "document_store_covid = ElasticsearchDocumentStore(\n",
        "    port=9200\n",
        ")\n",
        "document_store_covid.delete_documents()\n",
        "document_store_covid.write_documents(\n",
        "    documents=dicts_covid\n",
        ")"
      ],
      "id": "o53jkorpow7F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXonIb2Tv24S"
      },
      "source": [
        "#### DensePassageRetriever"
      ],
      "id": "NXonIb2Tv24S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d0165a49d0e3425b8588321b419b51eb",
            "bcfa6320bcd34264b4396a76384c348e"
          ]
        },
        "id": "65pXQGjio99N",
        "outputId": "d8b1b3e8-9cff-4dda-d847-a8bb977241e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0165a49d0e3425b8588321b419b51eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Updating embeddings:   0%|          | 0/147 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcfa6320bcd34264b4396a76384c348e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/160 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the main cause of HIV-1 infection in children?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.45 Batches/s]\n",
            "  1%|          | 1/100 [00:00<00:33,  2.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What plays the crucial role in the Mother to Child Transmission of HIV-1 and what increases the risk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.46 Batches/s]\n",
            "  2%|▏         | 2/100 [00:00<00:29,  3.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many children were infected by HIV-1 in 2008-2009, worldwide?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.65 Batches/s]\n",
            "  3%|▎         | 3/100 [00:00<00:29,  3.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the role of C-C Motif Chemokine Ligand 3 Like 1 (CCL3L1) in mother to child transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.68 Batches/s]\n",
            "  4%|▍         | 4/100 [00:01<00:30,  3.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is DC-GENR and where is  it expressed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.06 Batches/s]\n",
            "  5%|▌         | 5/100 [00:01<00:31,  2.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does the presence of DC-SIGNR affect the MTCT of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.79 Batches/s]\n",
            "  6%|▌         | 6/100 [00:02<00:33,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why do low levels of DC-SIGNR enhance Mother to Child Transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.01 Batches/s]\n",
            "  7%|▋         | 7/100 [00:02<00:34,  2.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the percentage of Mother to Child Transmission of HIV-1, when there is no intervention?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.31 Batches/s]\n",
            "  8%|▊         | 8/100 [00:02<00:30,  2.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Does C-C chemokine receptor type 5 (CCR5) affect the transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.95 Batches/s]\n",
            "  9%|▉         | 9/100 [00:02<00:29,  3.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does Mannanose Binding Lectin (MBL) affect elimination of HIV-1 pathogen?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\n",
            " 10%|█         | 10/100 [00:03<00:31,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How can CCR5's effect in HIV-1 transmission be reduced?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.20 Batches/s]\n",
            " 11%|█         | 11/100 [00:03<00:29,  3.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is IFITM?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.62 Batches/s]\n",
            " 12%|█▏        | 12/100 [00:03<00:27,  3.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cysteine residues are contained in the first transmembrane domain of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.21 Batches/s]\n",
            " 13%|█▎        | 13/100 [00:04<00:32,  2.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What inhibits S-palmitoylation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.32 Batches/s]\n",
            " 14%|█▍        | 14/100 [00:04<00:29,  2.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What interaction is inhibited by the presence of 2-bromopalmitic acid (2BP)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.40 Batches/s]\n",
            " 15%|█▌        | 15/100 [00:05<00:32,  2.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a function associated with IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.81 Batches/s]\n",
            " 16%|█▌        | 16/100 [00:05<00:37,  2.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the antiviral activity of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.79 Batches/s]\n",
            " 17%|█▋        | 17/100 [00:06<00:36,  2.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is another name for IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.94 Batches/s]\n",
            " 18%|█▊        | 18/100 [00:06<00:39,  2.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the expression of IFITM5 not promoted by interferons?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.65 Batches/s]\n",
            " 19%|█▉        | 19/100 [00:07<00:43,  1.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM5 and the other IFITM proteins?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.72 Batches/s]\n",
            " 20%|██        | 20/100 [00:08<00:45,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM 1, IFITM 2, and IFITM 3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.11 Batches/s]\n",
            " 21%|██        | 21/100 [00:08<00:43,  1.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What amino acid might be involved in calcium binding in the C-terminal region of a protein?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\n",
            " 22%|██▏       | 22/100 [00:08<00:38,  2.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.04 Batches/s]\n",
            " 23%|██▎       | 23/100 [00:09<00:30,  2.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the molecular structure of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.76 Batches/s]\n",
            " 24%|██▍       | 24/100 [00:09<00:25,  2.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many nucleotides does bovine coronavirus contain?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.93 Batches/s]\n",
            " 25%|██▌       | 25/100 [00:09<00:21,  3.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of the orf1ab gene in bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
            " 26%|██▌       | 26/100 [00:09<00:20,  3.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is the orf1ab gene at the 3' or 5' end of the bovine coronavirus genome?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\n",
            " 27%|██▋       | 27/100 [00:10<00:23,  3.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a significant cause of Influenze like illness among healthy adolescents and adults presenting for medical evaluation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.88 Batches/s]\n",
            " 28%|██▊       | 28/100 [00:10<00:21,  3.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the most common species of Human Coronavirus among adults?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.68 Batches/s]\n",
            " 29%|██▉       | 29/100 [00:10<00:19,  3.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which Human Coronavirus showed species specific clinical characteristics of its infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.06 Batches/s]\n",
            " 30%|███       | 30/100 [00:10<00:18,  3.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What causes the outbreak of SARS and MERS.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.79 Batches/s]\n",
            " 31%|███       | 31/100 [00:11<00:19,  3.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the case fatality rate of SARS and MERS?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.41 Batches/s]\n",
            " 32%|███▏      | 32/100 [00:11<00:18,  3.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What were the common HCOV strains in the 5 year USA study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.02 Batches/s]\n",
            " 33%|███▎      | 33/100 [00:11<00:18,  3.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which species are more prevalent but less severe?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.97 Batches/s]\n",
            " 34%|███▍      | 34/100 [00:12<00:18,  3.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is required for a Hepatitis B infection in cells?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.69 Batches/s]\n",
            " 35%|███▌      | 35/100 [00:12<00:17,  3.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the broad, but less specific, virus-cell interaction in a hepatitis B infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.78 Batches/s]\n",
            " 36%|███▌      | 36/100 [00:12<00:16,  3.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which protein domain of the Hepatitis B envelope is necessary for infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.90 Batches/s]\n",
            " 37%|███▋      | 37/100 [00:12<00:17,  3.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Where is NTCP located in the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.77 Batches/s]\n",
            " 38%|███▊      | 38/100 [00:13<00:15,  4.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What does the NTCP protein mediate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.87 Batches/s]\n",
            " 39%|███▉      | 39/100 [00:13<00:14,  4.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is NTCP sufficient to allow HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.68 Batches/s]\n",
            " 40%|████      | 40/100 [00:13<00:13,  4.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is NTCP thought to not be sufficient for HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.75 Batches/s]\n",
            " 41%|████      | 41/100 [00:13<00:12,  4.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kinds of viruses are Japanese encephalitis virus(JEV), tick-borne encephalitis virus(TBEV), eastern equine encephalitis virus (EEEV), sindbis virus(SV), and dengue virus(DV)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.03 Batches/s]\n",
            " 42%|████▏     | 42/100 [00:13<00:12,  4.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the current clinically-available methods to detect encephalitis viral antigens?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.55 Batches/s]\n",
            " 43%|████▎     | 43/100 [00:14<00:12,  4.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What methods exist for detecting multiple antigens simultaneously in a one-sample, laboratory test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.87 Batches/s]\n",
            " 44%|████▍     | 44/100 [00:14<00:11,  4.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many antigens could be detected by Liew's multiplex ELISA test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.83 Batches/s]\n",
            " 45%|████▌     | 45/100 [00:14<00:12,  4.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kind of antibodies were used in the ELISA-array assay?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.48 Batches/s]\n",
            " 46%|████▌     | 46/100 [00:14<00:12,  4.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.68 Batches/s]\n",
            " 47%|████▋     | 47/100 [00:15<00:12,  4.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What capture antibodies were used in the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.25 Batches/s]\n",
            " 48%|████▊     | 48/100 [00:15<00:13,  3.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the spotting concentration range for the capture antibodies?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.28 Batches/s]\n",
            " 49%|████▉     | 49/100 [00:15<00:13,  3.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the proper spotting concentration determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.72 Batches/s]\n",
            " 50%|█████     | 50/100 [00:15<00:12,  3.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was cross reaction detection determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.21 Batches/s]\n",
            " 51%|█████     | 51/100 [00:16<00:13,  3.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA-array assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.71 Batches/s]\n",
            " 52%|█████▏    | 52/100 [00:16<00:12,  3.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  In 2010, how many cases of tuberculosis were estimated in China?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.56 Batches/s]\n",
            " 53%|█████▎    | 53/100 [00:16<00:11,  4.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the population of Shandong province?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.17 Batches/s]\n",
            " 54%|█████▍    | 54/100 [00:16<00:11,  4.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the purpose of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.93 Batches/s]\n",
            " 55%|█████▌    | 55/100 [00:16<00:10,  4.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the age range for the people surveyed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.95 Batches/s]\n",
            " 56%|█████▌    | 56/100 [00:17<00:10,  4.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the survey designed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.05 Batches/s]\n",
            " 57%|█████▋    | 57/100 [00:17<00:10,  4.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Was was the sample size?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\n",
            " 58%|█████▊    | 58/100 [00:17<00:11,  3.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How were the clusters selected?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.92 Batches/s]\n",
            " 59%|█████▉    | 59/100 [00:18<00:10,  4.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many people were in a community cluster?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.99 Batches/s]\n",
            " 60%|██████    | 60/100 [00:18<00:09,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who was excluded from the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.81 Batches/s]\n",
            " 61%|██████    | 61/100 [00:18<00:09,  4.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When was the study conducted?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.44 Batches/s]\n",
            " 62%|██████▏   | 62/100 [00:18<00:09,  4.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who conducted the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.72 Batches/s]\n",
            " 63%|██████▎   | 63/100 [00:18<00:09,  4.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medium was used to collect the sputum samples?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.49 Batches/s]\n",
            " 64%|██████▍   | 64/100 [00:19<00:09,  3.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the response rate for the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.76 Batches/s]\n",
            " 65%|██████▌   | 65/100 [00:19<00:10,  3.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the average age of a study participant?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.24 Batches/s]\n",
            " 66%|██████▌   | 66/100 [00:19<00:09,  3.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the prevalence rate in Shandong in 2010 for sputum positive cases of tuberculosis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.17 Batches/s]\n",
            " 67%|██████▋   | 67/100 [00:20<00:08,  3.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the most striking finding of the study regarding tuberculosis patients?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.28 Batches/s]\n",
            " 68%|██████▊   | 68/100 [00:20<00:08,  3.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cases of sputum positive tuberculosis patients had no persistent cough?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.99 Batches/s]\n",
            " 69%|██████▉   | 69/100 [00:20<00:08,  3.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many tuberculosis patients in Shandong were over 65 years old?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.26 Batches/s]\n",
            " 70%|███████   | 70/100 [00:20<00:07,  3.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What enzymes have been reported to be linked with severity of infection and various pathological conditions caused by microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.44 Batches/s]\n",
            " 71%|███████   | 71/100 [00:21<00:07,  3.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  At what temperatures was the assay completed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.00 Batches/s]\n",
            " 72%|███████▏  | 72/100 [00:21<00:06,  4.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What criteria sets the guideline for drug-like properties?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.65 Batches/s]\n",
            " 73%|███████▎  | 73/100 [00:21<00:06,  4.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What could be novel candidates as potent inhibitors of papain like cysteine proteases in resistant microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.21 Batches/s]\n",
            " 74%|███████▍  | 74/100 [00:21<00:06,  4.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What method is useful in administering small molecules for systemic delivery to the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.52 Batches/s]\n",
            " 75%|███████▌  | 75/100 [00:22<00:06,  3.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the nasal mucosa useful in the delivery of small molecules into the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.81 Batches/s]\n",
            " 76%|███████▌  | 76/100 [00:22<00:07,  3.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the most common methods of inhaled delivery of medications?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.59 Batches/s]\n",
            " 77%|███████▋  | 77/100 [00:22<00:07,  3.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medications have shown good promise to in vivo delivery via dry powder inhalers?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.47 Batches/s]\n",
            " 78%|███████▊  | 78/100 [00:23<00:06,  3.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How are siRNAs typically delivered for systemic effect?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.53 Batches/s]\n",
            " 79%|███████▉  | 79/100 [00:23<00:06,  3.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What structures form the human airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.44 Batches/s]\n",
            " 80%|████████  | 80/100 [00:23<00:06,  3.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What size of particle has been shown to be most effective in the delivery to the lower airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.22 Batches/s]\n",
            " 81%|████████  | 81/100 [00:24<00:06,  3.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the essential conditions in siRNA delivery to effectively produce gene silencing in the lungs?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.81 Batches/s]\n",
            " 82%|████████▏ | 82/100 [00:24<00:06,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long is the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.80 Batches/s]\n",
            " 83%|████████▎ | 83/100 [00:24<00:05,  3.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many open reading frames are in the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.65 Batches/s]\n",
            " 84%|████████▍ | 84/100 [00:25<00:04,  3.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What virus has the closest genetic identity with the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.31 Batches/s]\n",
            " 85%|████████▌ | 85/100 [00:25<00:04,  3.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many surgical masks or respirators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.22 Batches/s]\n",
            " 86%|████████▌ | 86/100 [00:25<00:03,  3.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the acronym MERS-CoV?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.34 Batches/s]\n",
            " 87%|████████▋ | 87/100 [00:25<00:03,  3.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the critical factors that determine the effect of an epidemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.71 Batches/s]\n",
            " 88%|████████▊ | 88/100 [00:26<00:03,  3.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When did the World Health Organization (WHO) officially declare the 2019-nCoV epidemic as a Public Health Emergency of International Concern?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.13 Batches/s]\n",
            " 89%|████████▉ | 89/100 [00:26<00:03,  3.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What influenza virus was identified in China in 2013?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.13 Batches/s]\n",
            " 90%|█████████ | 90/100 [00:26<00:02,  3.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What past research has been done on severe, single-wave pandemics?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.58 Batches/s]\n",
            " 91%|█████████ | 91/100 [00:26<00:02,  3.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a clinical attack rate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.34 Batches/s]\n",
            " 92%|█████████▏| 92/100 [00:27<00:02,  3.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the clinical attack rate in the 2009 H1N1 pandemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.56 Batches/s]\n",
            " 93%|█████████▎| 93/100 [00:27<00:01,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the estimated R0 of COVID-19?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 94%|█████████▍| 94/100 [00:27<00:01,  3.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many ventilators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.12 Batches/s]\n",
            " 95%|█████████▌| 95/100 [00:27<00:01,  3.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How is exhaled breath condensate used in viral research?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.14 Batches/s]\n",
            " 96%|█████████▌| 96/100 [00:28<00:01,  3.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many patients were i this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.00 Batches/s]\n",
            " 97%|█████████▋| 97/100 [00:28<00:00,  3.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the conclusion of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.35 Batches/s]\n",
            " 98%|█████████▊| 98/100 [00:28<00:00,  4.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long did the patient breath into the RTube?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.28 Batches/s]\n",
            " 99%|█████████▉| 99/100 [00:28<00:00,  4.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What followed the reverse transcription step in the analysis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.14 Batches/s]\n",
            "100%|██████████| 100/100 [00:29<00:00,  3.43it/s]\n"
          ]
        }
      ],
      "source": [
        "from haystack.nodes import DensePassageRetriever  \n",
        "retriever_DPR = DensePassageRetriever(document_store=document_store_covid,\n",
        "    query_embedding_model=\"bert-base-uncased\",\n",
        "    passage_embedding_model=\"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "document_store_covid.update_embeddings(retriever_DPR)\n",
        "\n",
        "reader = FARMReader(\n",
        "    model_name_or_path=model, #\"deepset/roberta-base-squad2\"\n",
        "    use_gpu=use_gpu\n",
        ")\n",
        "\n",
        "pipe_covid_DPR = ExtractiveQAPipeline(reader, retriever_tfidf)\n",
        "\n",
        "df_res_covid_DPR = qa_dataset.copy()\n",
        "\n",
        "# prepare columns for answers\n",
        "df_res_covid_DPR['predictions_covid_context_DPR'] = [list() for x in range(len(df_res_covid_DPR.index))]\n",
        "\n",
        "for q_i in tqdm(range(len(qa_dataset.question.tolist()))):\n",
        "\n",
        "    print('question : ', qa_dataset.question[q_i])\n",
        "\n",
        "    try:\n",
        "        # covid dataset context prediction\n",
        "        prediction_covid = pipe_covid_DPR.run(\n",
        "            query=qa_dataset.question[q_i],\n",
        "            params={\n",
        "                \"Retriever\" : {\"top_k\": top_k_retriever},\n",
        "                \"Reader\": {\"top_k\": top_k_reader}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        df_res_covid_DPR.loc[q_i, 'predictions_covid_context_DPR'].append(\n",
        "            [prediction_covid['answers'][k].answer for k in range(len(prediction_covid['answers']))]\n",
        "        )\n",
        "    except:\n",
        "        df_res_covid_DPR.loc[q_i, 'predictions_covid_context_DPR'].append([])\n",
        "    \n",
        "\n",
        "df_res_covid_DPR.to_csv('/content/drive/MyDrive/DeepLearning/df_res_covid_DPR_org_data.csv', index=False)"
      ],
      "id": "65pXQGjio99N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0764ab08"
      },
      "source": [
        "# Wikipedia"
      ],
      "id": "0764ab08"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9422de54"
      },
      "outputs": [],
      "source": [
        "## if not already downloaded\n",
        "# ds = load_dataset('wikipedia', \"20220301.simple\")\n",
        "## We take the training data and convert it to a Pandas DataFrame\n",
        "# df = ds.data['train'].to_pandas()"
      ],
      "id": "9422de54"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "022ecb73078c4289a183c74d3e0b50ad",
            "b677f50de6eb4e68b2d6b13691e04fd9",
            "098c9f6938394fa982c5f3f6f3cefed1",
            "528ca76f405f41739fdda062a0cb30ee",
            "2eb689cc78c640cea5bac92d924646cb",
            "d5a21e4d7fad45b7870578f796197a16",
            "4d3d2fab04964a08acaae50e9ef637f1",
            "5779fe1603a84378886969626b04a725",
            "f1a4a0db250647df82dd6773c17dde5e",
            "bdb0b8a4ba68497d85cc57b0c2e760bc",
            "4c3b4e2758da491f83b4a13b649ea3ba"
          ]
        },
        "id": "45cbece6",
        "outputId": "54610288-c40d-49c0-8908-76f724baf2e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset wikipedia (/root/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "022ecb73078c4289a183c74d3e0b50ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3c043174-705e-46f8-9855-60eb8f43fac5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>https://simple.wikipedia.org/wiki/April</td>\n",
              "      <td>April</td>\n",
              "      <td>April is the fourth month of the year in the Julian and Gregorian calendars,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>https://simple.wikipedia.org/wiki/August</td>\n",
              "      <td>August</td>\n",
              "      <td>August (Aug.) is the eighth month of the year in the Gregorian calendar, com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>https://simple.wikipedia.org/wiki/Art</td>\n",
              "      <td>Art</td>\n",
              "      <td>Art is a creative activity that expresses imaginative or technical skill. It...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c043174-705e-46f8-9855-60eb8f43fac5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c043174-705e-46f8-9855-60eb8f43fac5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c043174-705e-46f8-9855-60eb8f43fac5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "  id                                       url   title  \\\n",
              "0  1   https://simple.wikipedia.org/wiki/April   April   \n",
              "1  2  https://simple.wikipedia.org/wiki/August  August   \n",
              "2  6     https://simple.wikipedia.org/wiki/Art     Art   \n",
              "\n",
              "                                                                              text  \n",
              "0  April is the fourth month of the year in the Julian and Gregorian calendars,...  \n",
              "1  August (Aug.) is the eighth month of the year in the Gregorian calendar, com...  \n",
              "2  Art is a creative activity that expresses imaginative or technical skill. It...  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## if not already downloaded\n",
        "ds = load_dataset('wikipedia', \"20220301.simple\")\n",
        "## We take the training data and convert it to a Pandas DataFrame\n",
        "df_wiki = ds.data['train'].to_pandas()\n",
        "df_wiki.head(3)"
      ],
      "id": "45cbece6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85460bf7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "df_wiki['context_cleaned'] = df_wiki.text.apply(\n",
        "    lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]|\\n|[^a-zA-z0-9.]', ' ', x)\n",
        ")"
      ],
      "id": "85460bf7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95c728ce"
      },
      "source": [
        "### Document Store"
      ],
      "id": "95c728ce"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr4CB8b44UUn"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "id": "Kr4CB8b44UUn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4jVprJ64UUq"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "id": "V4jVprJ64UUq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-70flpq4UUr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(30)"
      ],
      "id": "L-70flpq4UUr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0140550"
      },
      "outputs": [],
      "source": [
        "documents_lst_wiki = df_wiki.to_dict(orient='records')"
      ],
      "id": "c0140550"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c277d2f7",
        "outputId": "9f6380d0-e670-43c1-9d09-09d021c5f3e1",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 205328/205328 [00:00<00:00, 363988.95it/s]\n"
          ]
        }
      ],
      "source": [
        "# the way document_store wants data formatted\n",
        "dicts_wiki = [\n",
        "    {\n",
        "        'content' : elm['text'],\n",
        "        'meta' : {\n",
        "            'name' : elm['title']\n",
        "        }\n",
        "    } \n",
        "    for elm in tqdm(documents_lst_wiki)\n",
        "]"
      ],
      "id": "c277d2f7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b101685a",
        "outputId": "fc736ca0-3481-4079-d652-32d79fad74c6",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 16.4 s, sys: 3.33 s, total: 19.7 s\n",
            "Wall time: 7min 12s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "document_store_wiki = ElasticsearchDocumentStore(\n",
        "    port=9200\n",
        ")\n",
        "document_store_wiki.delete_documents()\n",
        "document_store_wiki.write_documents(\n",
        "    documents=dicts_wiki\n",
        ")"
      ],
      "id": "b101685a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a4da1dc"
      },
      "source": [
        "### Retrieving answers"
      ],
      "id": "8a4da1dc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3cee012"
      },
      "source": [
        "#### BM25"
      ],
      "id": "f3cee012"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8bdaced",
        "outputId": "15cd118b-d042-49c6-88de-3e7e158537fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the main cause of HIV-1 infection in children?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.36 Batches/s]\n",
            "  1%|          | 1/100 [00:00<01:23,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What plays the crucial role in the Mother to Child Transmission of HIV-1 and what increases the risk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.78 Batches/s]\n",
            "  2%|▏         | 2/100 [00:02<01:44,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many children were infected by HIV-1 in 2008-2009, worldwide?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:00<00:00,  2.24 Batches/s]\n",
            "  3%|▎         | 3/100 [00:03<01:39,  1.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the role of C-C Motif Chemokine Ligand 3 Like 1 (CCL3L1) in mother to child transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.38 Batches/s]\n",
            "  4%|▍         | 4/100 [00:04<01:58,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is DC-GENR and where is  it expressed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.67 Batches/s]\n",
            "  5%|▌         | 5/100 [00:05<01:37,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does the presence of DC-SIGNR affect the MTCT of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.34 Batches/s]\n",
            "  6%|▌         | 6/100 [00:06<01:29,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why do low levels of DC-SIGNR enhance Mother to Child Transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.46 Batches/s]\n",
            "  7%|▋         | 7/100 [00:07<01:43,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the percentage of Mother to Child Transmission of HIV-1, when there is no intervention?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.77 Batches/s]\n",
            "  8%|▊         | 8/100 [00:08<01:45,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Does C-C chemokine receptor type 5 (CCR5) affect the transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 1/3 [00:00<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 2/3 [00:01<00:00,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 3/3 [00:02<00:00,  1.37 Batches/s]\n",
            "  9%|▉         | 9/100 [00:10<02:16,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does Mannanose Binding Lectin (MBL) affect elimination of HIV-1 pathogen?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:00<00:00,  2.07 Batches/s]\n",
            " 10%|█         | 10/100 [00:12<02:01,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How can CCR5's effect in HIV-1 transmission be reduced?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.62 Batches/s]\n",
            " 11%|█         | 11/100 [00:13<01:59,  1.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is IFITM?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.26 Batches/s]\n",
            " 12%|█▏        | 12/100 [00:13<01:34,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cysteine residues are contained in the first transmembrane domain of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\n",
            " 13%|█▎        | 13/100 [00:14<01:16,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What inhibits S-palmitoylation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.49 Batches/s]\n",
            " 14%|█▍        | 14/100 [00:14<01:00,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What interaction is inhibited by the presence of 2-bromopalmitic acid (2BP)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.49 Batches/s]\n",
            " 15%|█▌        | 15/100 [00:14<00:53,  1.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a function associated with IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.80 Batches/s]\n",
            " 16%|█▌        | 16/100 [00:15<00:47,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the antiviral activity of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.14 Batches/s]\n",
            " 17%|█▋        | 17/100 [00:15<00:45,  1.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is another name for IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.65 Batches/s]\n",
            " 18%|█▊        | 18/100 [00:16<00:41,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the expression of IFITM5 not promoted by interferons?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.99 Batches/s]\n",
            " 19%|█▉        | 19/100 [00:16<00:35,  2.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM5 and the other IFITM proteins?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.29 Batches/s]\n",
            " 20%|██        | 20/100 [00:17<00:36,  2.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM 1, IFITM 2, and IFITM 3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.84 Batches/s]\n",
            " 21%|██        | 21/100 [00:17<00:34,  2.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What amino acid might be involved in calcium binding in the C-terminal region of a protein?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.19 Batches/s]\n",
            " 22%|██▏       | 22/100 [00:17<00:35,  2.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\n",
            " 23%|██▎       | 23/100 [00:18<00:32,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the molecular structure of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.39 Batches/s]\n",
            " 24%|██▍       | 24/100 [00:18<00:33,  2.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many nucleotides does bovine coronavirus contain?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.82 Batches/s]\n",
            " 25%|██▌       | 25/100 [00:19<00:32,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of the orf1ab gene in bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.89 Batches/s]\n",
            " 26%|██▌       | 26/100 [00:19<00:30,  2.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is the orf1ab gene at the 3' or 5' end of the bovine coronavirus genome?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.67 Batches/s]\n",
            " 27%|██▋       | 27/100 [00:20<00:35,  2.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a significant cause of Influenze like illness among healthy adolescents and adults presenting for medical evaluation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.51 Batches/s]\n",
            " 28%|██▊       | 28/100 [00:20<00:40,  1.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the most common species of Human Coronavirus among adults?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.84 Batches/s]\n",
            " 29%|██▉       | 29/100 [00:21<00:40,  1.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which Human Coronavirus showed species specific clinical characteristics of its infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:00<00:00,  2.43 Batches/s]\n",
            " 30%|███       | 30/100 [00:22<00:46,  1.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What causes the outbreak of SARS and MERS.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.49 Batches/s]\n",
            " 31%|███       | 31/100 [00:22<00:41,  1.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the case fatality rate of SARS and MERS?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\n",
            " 32%|███▏      | 32/100 [00:23<00:36,  1.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What were the common HCOV strains in the 5 year USA study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.31 Batches/s]\n",
            " 33%|███▎      | 33/100 [00:23<00:35,  1.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which species are more prevalent but less severe?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.78 Batches/s]\n",
            " 34%|███▍      | 34/100 [00:24<00:36,  1.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is required for a Hepatitis B infection in cells?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.30 Batches/s]\n",
            " 35%|███▌      | 35/100 [00:26<00:56,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the broad, but less specific, virus-cell interaction in a hepatitis B infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 1/3 [00:00<00:01,  1.36 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 2/3 [00:01<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 3/3 [00:01<00:00,  1.72 Batches/s]\n",
            " 36%|███▌      | 36/100 [00:27<01:14,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which protein domain of the Hepatitis B envelope is necessary for infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.34 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.44 Batches/s]\n",
            " 37%|███▋      | 37/100 [00:29<01:19,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Where is NTCP located in the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.58 Batches/s]\n",
            " 38%|███▊      | 38/100 [00:29<00:58,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What does the NTCP protein mediate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.19 Batches/s]\n",
            " 39%|███▉      | 39/100 [00:30<00:49,  1.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is NTCP sufficient to allow HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.83 Batches/s]\n",
            " 40%|████      | 40/100 [00:30<00:44,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is NTCP thought to not be sufficient for HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.90 Batches/s]\n",
            " 41%|████      | 41/100 [00:31<00:41,  1.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kinds of viruses are Japanese encephalitis virus(JEV), tick-borne encephalitis virus(TBEV), eastern equine encephalitis virus (EEEV), sindbis virus(SV), and dengue virus(DV)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.35 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.79 Batches/s]\n",
            " 42%|████▏     | 42/100 [00:32<00:49,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the current clinically-available methods to detect encephalitis viral antigens?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.89 Batches/s]\n",
            " 43%|████▎     | 43/100 [00:33<00:43,  1.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What methods exist for detecting multiple antigens simultaneously in a one-sample, laboratory test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.33 Batches/s]\n",
            " 44%|████▍     | 44/100 [00:33<00:35,  1.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many antigens could be detected by Liew's multiplex ELISA test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.67 Batches/s]\n",
            " 45%|████▌     | 45/100 [00:33<00:31,  1.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kind of antibodies were used in the ELISA-array assay?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.71 Batches/s]\n",
            " 46%|████▌     | 46/100 [00:34<00:32,  1.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.51 Batches/s]\n",
            " 47%|████▋     | 47/100 [00:34<00:26,  2.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What capture antibodies were used in the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.89 Batches/s]\n",
            " 48%|████▊     | 48/100 [00:35<00:27,  1.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the spotting concentration range for the capture antibodies?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.86 Batches/s]\n",
            " 49%|████▉     | 49/100 [00:35<00:27,  1.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the proper spotting concentration determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.41 Batches/s]\n",
            " 50%|█████     | 50/100 [00:36<00:24,  2.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was cross reaction detection determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.07 Batches/s]\n",
            " 51%|█████     | 51/100 [00:36<00:24,  2.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA-array assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.84 Batches/s]\n",
            " 52%|█████▏    | 52/100 [00:37<00:22,  2.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  In 2010, how many cases of tuberculosis were estimated in China?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.74 Batches/s]\n",
            " 53%|█████▎    | 53/100 [00:37<00:24,  1.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the population of Shandong province?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.28 Batches/s]\n",
            " 54%|█████▍    | 54/100 [00:37<00:19,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the purpose of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.67 Batches/s]\n",
            " 55%|█████▌    | 55/100 [00:38<00:19,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the age range for the people surveyed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.27 Batches/s]\n",
            " 56%|█████▌    | 56/100 [00:38<00:19,  2.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the survey designed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.08 Batches/s]\n",
            " 57%|█████▋    | 57/100 [00:39<00:16,  2.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Was was the sample size?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.91 Batches/s]\n",
            " 58%|█████▊    | 58/100 [00:39<00:18,  2.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How were the clusters selected?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.44 Batches/s]\n",
            " 59%|█████▉    | 59/100 [00:40<00:17,  2.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many people were in a community cluster?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.92 Batches/s]\n",
            " 60%|██████    | 60/100 [00:40<00:16,  2.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who was excluded from the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.80 Batches/s]\n",
            " 61%|██████    | 61/100 [00:40<00:15,  2.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When was the study conducted?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.86 Batches/s]\n",
            " 62%|██████▏   | 62/100 [00:41<00:17,  2.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who conducted the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.75 Batches/s]\n",
            " 63%|██████▎   | 63/100 [00:42<00:18,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medium was used to collect the sputum samples?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.30 Batches/s]\n",
            " 64%|██████▍   | 64/100 [00:42<00:18,  2.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the response rate for the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.92 Batches/s]\n",
            " 65%|██████▌   | 65/100 [00:43<00:18,  1.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the average age of a study participant?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.05 Batches/s]\n",
            " 66%|██████▌   | 66/100 [00:43<00:17,  1.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the prevalence rate in Shandong in 2010 for sputum positive cases of tuberculosis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.57 Batches/s]\n",
            " 67%|██████▋   | 67/100 [00:44<00:19,  1.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the most striking finding of the study regarding tuberculosis patients?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.88 Batches/s]\n",
            " 68%|██████▊   | 68/100 [00:45<00:23,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cases of sputum positive tuberculosis patients had no persistent cough?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.92 Batches/s]\n",
            " 69%|██████▉   | 69/100 [00:46<00:26,  1.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many tuberculosis patients in Shandong were over 65 years old?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.80 Batches/s]\n",
            " 70%|███████   | 70/100 [00:47<00:23,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What enzymes have been reported to be linked with severity of infection and various pathological conditions caused by microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.41 Batches/s]\n",
            " 71%|███████   | 71/100 [00:48<00:22,  1.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  At what temperatures was the assay completed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.51 Batches/s]\n",
            " 72%|███████▏  | 72/100 [00:48<00:18,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What criteria sets the guideline for drug-like properties?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:00<00:00,  2.24 Batches/s]\n",
            " 73%|███████▎  | 73/100 [00:49<00:20,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What could be novel candidates as potent inhibitors of papain like cysteine proteases in resistant microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.76 Batches/s]\n",
            " 74%|███████▍  | 74/100 [00:49<00:18,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What method is useful in administering small molecules for systemic delivery to the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.87 Batches/s]\n",
            " 75%|███████▌  | 75/100 [00:50<00:16,  1.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the nasal mucosa useful in the delivery of small molecules into the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.81 Batches/s]\n",
            " 76%|███████▌  | 76/100 [00:50<00:14,  1.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the most common methods of inhaled delivery of medications?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.76 Batches/s]\n",
            " 77%|███████▋  | 77/100 [00:51<00:13,  1.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medications have shown good promise to in vivo delivery via dry powder inhalers?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.59 Batches/s]\n",
            " 78%|███████▊  | 78/100 [00:51<00:12,  1.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How are siRNAs typically delivered for systemic effect?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.62 Batches/s]\n",
            " 79%|███████▉  | 79/100 [00:52<00:12,  1.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What structures form the human airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.41 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.89 Batches/s]\n",
            " 80%|████████  | 80/100 [00:53<00:14,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What size of particle has been shown to be most effective in the delivery to the lower airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.96 Batches/s]\n",
            " 81%|████████  | 81/100 [00:54<00:13,  1.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the essential conditions in siRNA delivery to effectively produce gene silencing in the lungs?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.47 Batches/s]\n",
            " 82%|████████▏ | 82/100 [00:54<00:11,  1.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long is the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.03 Batches/s]\n",
            " 83%|████████▎ | 83/100 [00:55<00:10,  1.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many open reading frames are in the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.48 Batches/s]\n",
            " 84%|████████▍ | 84/100 [00:55<00:08,  1.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What virus has the closest genetic identity with the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.74 Batches/s]\n",
            " 85%|████████▌ | 85/100 [00:56<00:07,  1.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many surgical masks or respirators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.29 Batches/s]\n",
            " 86%|████████▌ | 86/100 [00:56<00:07,  1.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the acronym MERS-CoV?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.54 Batches/s]\n",
            " 87%|████████▋ | 87/100 [00:57<00:06,  2.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the critical factors that determine the effect of an epidemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.37 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.78 Batches/s]\n",
            " 88%|████████▊ | 88/100 [00:58<00:08,  1.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When did the World Health Organization (WHO) officially declare the 2019-nCoV epidemic as a Public Health Emergency of International Concern?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.72 Batches/s]\n",
            " 89%|████████▉ | 89/100 [00:58<00:06,  1.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What influenza virus was identified in China in 2013?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.63 Batches/s]\n",
            " 90%|█████████ | 90/100 [00:59<00:06,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What past research has been done on severe, single-wave pandemics?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.80 Batches/s]\n",
            " 91%|█████████ | 91/100 [01:00<00:05,  1.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a clinical attack rate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.38 Batches/s]\n",
            " 92%|█████████▏| 92/100 [01:00<00:04,  1.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the clinical attack rate in the 2009 H1N1 pandemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.64 Batches/s]\n",
            " 93%|█████████▎| 93/100 [01:01<00:04,  1.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the estimated R0 of COVID-19?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.69 Batches/s]\n",
            " 94%|█████████▍| 94/100 [01:01<00:03,  1.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many ventilators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.94 Batches/s]\n",
            " 95%|█████████▌| 95/100 [01:02<00:02,  1.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How is exhaled breath condensate used in viral research?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.50 Batches/s]\n",
            " 96%|█████████▌| 96/100 [01:02<00:02,  1.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many patients were i this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 1/3 [00:00<00:01,  1.40 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 2/3 [00:01<00:00,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 3/3 [00:01<00:00,  1.86 Batches/s]\n",
            " 97%|█████████▋| 97/100 [01:04<00:02,  1.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the conclusion of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.96 Batches/s]\n",
            " 98%|█████████▊| 98/100 [01:05<00:01,  1.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long did the patient breath into the RTube?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.39 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:00<00:00,  2.10 Batches/s]\n",
            " 99%|█████████▉| 99/100 [01:06<00:00,  1.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What followed the reverse transcription step in the analysis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.09 Batches/s]\n",
            "100%|██████████| 100/100 [01:06<00:00,  1.50it/s]\n"
          ]
        }
      ],
      "source": [
        "retriever = BM25Retriever(document_store=document_store_wiki)\n",
        "\n",
        "reader = FARMReader(\n",
        "    model_name_or_path=model, #\"deepset/roberta-base-squad2\"\n",
        "    use_gpu=use_gpu\n",
        ")\n",
        "\n",
        "pipe_wiki = ExtractiveQAPipeline(reader, retriever)\n",
        "\n",
        "df_res_wiki_bm25 = qa_dataset.copy()\n",
        "\n",
        "# prepare columns for answers\n",
        "df_res_wiki_bm25['predictions_wiki_context_bm25'] = [list() for x in range(len(df_res_wiki_bm25.index))]\n",
        "\n",
        "for q_i in tqdm(range(len(qa_dataset.question.tolist()))):\n",
        "    \n",
        "    print('question : ', qa_dataset.question[q_i])\n",
        "    \n",
        "    try:\n",
        "        # covid dataset context prediction\n",
        "        prediction_wiki = pipe_wiki.run(\n",
        "            query=qa_dataset.question[q_i],\n",
        "            params={\n",
        "                \"Retriever\" : {\"top_k\": top_k_retriever},\n",
        "                \"Reader\": {\"top_k\": top_k_reader}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        df_res_wiki_bm25.loc[q_i, 'predictions_wiki_context_bm25'].append(\n",
        "            [prediction_wiki['answers'][k].answer for k in range(len(prediction_wiki['answers']))]\n",
        "        )\n",
        "    except:\n",
        "        df_res_wiki_bm25.loc[q_i, 'predictions_wiki_context_bm25'].append([])\n",
        "\n",
        "df_res_wiki_bm25.to_csv('/content/drive/MyDrive/DeepLearning/df_res_wiki_bm25_org_data.csv', index=False)"
      ],
      "id": "a8bdaced"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r51SFx_5Rsp"
      },
      "source": [
        "#### RE-INITIALISE ELASTICSEARCH"
      ],
      "id": "6r51SFx_5Rsp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzmiEhVt76LR"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "id": "nzmiEhVt76LR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq2iZEjt76LT"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "id": "Cq2iZEjt76LT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_L-B1UR76LT"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(30)"
      ],
      "id": "y_L-B1UR76LT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ueb8ETi76LU"
      },
      "outputs": [],
      "source": [
        "documents_lst_wiki = df_wiki.to_dict(orient='records')"
      ],
      "id": "5ueb8ETi76LU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDLOfN3B76LV",
        "outputId": "a2ab38db-6c58-4798-ae05-57eb7b4a7cf0",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 205328/205328 [00:00<00:00, 310810.17it/s]\n"
          ]
        }
      ],
      "source": [
        "# the way document_store wants data formatted\n",
        "dicts_wiki = [\n",
        "    {\n",
        "        'content' : elm['text'],\n",
        "        'meta' : {\n",
        "            'name' : elm['title']\n",
        "        }\n",
        "    } \n",
        "    for elm in tqdm(documents_lst_wiki)\n",
        "]"
      ],
      "id": "eDLOfN3B76LV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8hU1JY776LX",
        "outputId": "cffbf529-9df6-4158-e783-6027a6c8b546",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 16.6 s, sys: 899 ms, total: 17.5 s\n",
            "Wall time: 7min 29s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "document_store_wiki = ElasticsearchDocumentStore(\n",
        "    port=9200\n",
        ")\n",
        "document_store_wiki.delete_documents()\n",
        "document_store_wiki.write_documents(\n",
        "    documents=dicts_wiki\n",
        ")"
      ],
      "id": "g8hU1JY776LX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b58968a3"
      },
      "source": [
        "#### TF IDF"
      ],
      "id": "b58968a3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "371de3e6",
        "outputId": "8be796de-c6c2-45d3-b234-2abfeefbfa0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the main cause of HIV-1 infection in children?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.85 Batches/s]\n",
            "  1%|          | 1/100 [00:03<04:58,  3.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What plays the crucial role in the Mother to Child Transmission of HIV-1 and what increases the risk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.15 Batches/s]\n",
            "  2%|▏         | 2/100 [00:05<04:46,  2.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many children were infected by HIV-1 in 2008-2009, worldwide?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.87 Batches/s]\n",
            "  3%|▎         | 3/100 [00:08<04:26,  2.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the role of C-C Motif Chemokine Ligand 3 Like 1 (CCL3L1) in mother to child transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            "  4%|▍         | 4/100 [00:11<04:28,  2.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is DC-GENR and where is  it expressed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.43 Batches/s]\n",
            "  5%|▌         | 5/100 [00:13<04:14,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does the presence of DC-SIGNR affect the MTCT of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            "  6%|▌         | 6/100 [00:16<04:12,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why do low levels of DC-SIGNR enhance Mother to Child Transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.99 Batches/s]\n",
            "  7%|▋         | 7/100 [00:18<04:03,  2.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the percentage of Mother to Child Transmission of HIV-1, when there is no intervention?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.99 Batches/s]\n",
            "  8%|▊         | 8/100 [00:21<04:06,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Does C-C chemokine receptor type 5 (CCR5) affect the transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.30 Batches/s]\n",
            "  9%|▉         | 9/100 [00:24<04:03,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does Mannanose Binding Lectin (MBL) affect elimination of HIV-1 pathogen?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.98 Batches/s]\n",
            " 10%|█         | 10/100 [00:26<03:52,  2.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How can CCR5's effect in HIV-1 transmission be reduced?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.36 Batches/s]\n",
            " 11%|█         | 11/100 [00:29<03:46,  2.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is IFITM?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.35 Batches/s]\n",
            " 12%|█▏        | 12/100 [00:31<03:30,  2.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cysteine residues are contained in the first transmembrane domain of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.21 Batches/s]\n",
            " 13%|█▎        | 13/100 [00:34<03:39,  2.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What inhibits S-palmitoylation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.99 Batches/s]\n",
            " 14%|█▍        | 14/100 [00:35<03:06,  2.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What interaction is inhibited by the presence of 2-bromopalmitic acid (2BP)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.97 Batches/s]\n",
            " 15%|█▌        | 15/100 [00:38<03:19,  2.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a function associated with IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
            " 16%|█▌        | 16/100 [00:40<03:12,  2.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the antiviral activity of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.93 Batches/s]\n",
            " 17%|█▋        | 17/100 [00:43<03:20,  2.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is another name for IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.19 Batches/s]\n",
            " 18%|█▊        | 18/100 [00:45<03:13,  2.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the expression of IFITM5 not promoted by interferons?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
            " 19%|█▉        | 19/100 [00:48<03:21,  2.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM5 and the other IFITM proteins?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.84 Batches/s]\n",
            " 20%|██        | 20/100 [00:50<03:25,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM 1, IFITM 2, and IFITM 3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.89 Batches/s]\n",
            " 21%|██        | 21/100 [00:53<03:25,  2.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What amino acid might be involved in calcium binding in the C-terminal region of a protein?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.02 Batches/s]\n",
            " 22%|██▏       | 22/100 [00:56<03:28,  2.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.85 Batches/s]\n",
            " 23%|██▎       | 23/100 [00:59<03:27,  2.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the molecular structure of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.45 Batches/s]\n",
            " 24%|██▍       | 24/100 [01:01<03:25,  2.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many nucleotides does bovine coronavirus contain?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.06 Batches/s]\n",
            " 25%|██▌       | 25/100 [01:03<02:56,  2.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of the orf1ab gene in bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.26 Batches/s]\n",
            " 26%|██▌       | 26/100 [01:06<03:09,  2.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is the orf1ab gene at the 3' or 5' end of the bovine coronavirus genome?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.95 Batches/s]\n",
            " 27%|██▋       | 27/100 [01:09<03:11,  2.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a significant cause of Influenze like illness among healthy adolescents and adults presenting for medical evaluation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.28 Batches/s]\n",
            " 28%|██▊       | 28/100 [01:11<03:10,  2.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the most common species of Human Coronavirus among adults?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.17 Batches/s]\n",
            " 29%|██▉       | 29/100 [01:14<03:11,  2.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which Human Coronavirus showed species specific clinical characteristics of its infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 30%|███       | 30/100 [01:17<03:05,  2.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What causes the outbreak of SARS and MERS.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.87 Batches/s]\n",
            " 31%|███       | 31/100 [01:20<03:05,  2.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the case fatality rate of SARS and MERS?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 32%|███▏      | 32/100 [01:22<03:05,  2.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What were the common HCOV strains in the 5 year USA study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.15 Batches/s]\n",
            " 33%|███▎      | 33/100 [01:25<03:02,  2.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which species are more prevalent but less severe?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.14 Batches/s]\n",
            " 34%|███▍      | 34/100 [01:27<02:40,  2.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is required for a Hepatitis B infection in cells?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.94 Batches/s]\n",
            " 35%|███▌      | 35/100 [01:29<02:41,  2.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the broad, but less specific, virus-cell interaction in a hepatitis B infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.27 Batches/s]\n",
            " 36%|███▌      | 36/100 [01:32<02:43,  2.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which protein domain of the Hepatitis B envelope is necessary for infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 37%|███▋      | 37/100 [01:35<02:44,  2.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Where is NTCP located in the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 38%|███▊      | 38/100 [01:38<02:46,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What does the NTCP protein mediate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.85 Batches/s]\n",
            " 39%|███▉      | 39/100 [01:41<02:48,  2.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is NTCP sufficient to allow HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 40%|████      | 40/100 [01:43<02:38,  2.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is NTCP thought to not be sufficient for HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
            " 41%|████      | 41/100 [01:45<02:32,  2.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kinds of viruses are Japanese encephalitis virus(JEV), tick-borne encephalitis virus(TBEV), eastern equine encephalitis virus (EEEV), sindbis virus(SV), and dengue virus(DV)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 42%|████▏     | 42/100 [01:48<02:30,  2.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the current clinically-available methods to detect encephalitis viral antigens?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.94 Batches/s]\n",
            " 43%|████▎     | 43/100 [01:51<02:28,  2.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What methods exist for detecting multiple antigens simultaneously in a one-sample, laboratory test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.27 Batches/s]\n",
            " 44%|████▍     | 44/100 [01:53<02:24,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many antigens could be detected by Liew's multiplex ELISA test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.89 Batches/s]\n",
            " 45%|████▌     | 45/100 [01:55<02:08,  2.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kind of antibodies were used in the ELISA-array assay?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.22 Batches/s]\n",
            " 46%|████▌     | 46/100 [01:58<02:14,  2.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.97 Batches/s]\n",
            " 47%|████▋     | 47/100 [02:00<02:13,  2.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What capture antibodies were used in the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.24 Batches/s]\n",
            " 48%|████▊     | 48/100 [02:03<02:13,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the spotting concentration range for the capture antibodies?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.24 Batches/s]\n",
            " 49%|████▉     | 49/100 [02:06<02:11,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the proper spotting concentration determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.84 Batches/s]\n",
            " 50%|█████     | 50/100 [02:08<02:08,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was cross reaction detection determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.28 Batches/s]\n",
            " 51%|█████     | 51/100 [02:10<01:55,  2.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA-array assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.87 Batches/s]\n",
            " 52%|█████▏    | 52/100 [02:13<01:55,  2.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  In 2010, how many cases of tuberculosis were estimated in China?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.24 Batches/s]\n",
            " 53%|█████▎    | 53/100 [02:15<01:57,  2.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the population of Shandong province?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.93 Batches/s]\n",
            " 54%|█████▍    | 54/100 [02:18<01:58,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the purpose of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.29 Batches/s]\n",
            " 55%|█████▌    | 55/100 [02:21<01:57,  2.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the age range for the people surveyed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 56%|█████▌    | 56/100 [02:23<01:55,  2.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the survey designed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.88 Batches/s]\n",
            " 57%|█████▋    | 57/100 [02:26<01:52,  2.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Was was the sample size?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.87 Batches/s]\n",
            " 58%|█████▊    | 58/100 [02:29<01:49,  2.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How were the clusters selected?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.25 Batches/s]\n",
            " 59%|█████▉    | 59/100 [02:31<01:46,  2.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many people were in a community cluster?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.84 Batches/s]\n",
            " 60%|██████    | 60/100 [02:34<01:42,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who was excluded from the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.42 Batches/s]\n",
            " 61%|██████    | 61/100 [02:36<01:41,  2.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When was the study conducted?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
            " 62%|██████▏   | 62/100 [02:39<01:38,  2.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who conducted the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            " 63%|██████▎   | 63/100 [02:42<01:35,  2.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medium was used to collect the sputum samples?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.77 Batches/s]\n",
            " 64%|██████▍   | 64/100 [02:44<01:33,  2.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the response rate for the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.01 Batches/s]\n",
            " 65%|██████▌   | 65/100 [02:47<01:31,  2.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the average age of a study participant?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.02 Batches/s]\n",
            " 66%|██████▌   | 66/100 [02:50<01:31,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the prevalence rate in Shandong in 2010 for sputum positive cases of tuberculosis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 67%|██████▋   | 67/100 [02:53<01:31,  2.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the most striking finding of the study regarding tuberculosis patients?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.42 Batches/s]\n",
            " 68%|██████▊   | 68/100 [02:55<01:27,  2.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cases of sputum positive tuberculosis patients had no persistent cough?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.49 Batches/s]\n",
            " 69%|██████▉   | 69/100 [02:58<01:22,  2.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many tuberculosis patients in Shandong were over 65 years old?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.33 Batches/s]\n",
            " 70%|███████   | 70/100 [03:00<01:17,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What enzymes have been reported to be linked with severity of infection and various pathological conditions caused by microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.84 Batches/s]\n",
            " 71%|███████   | 71/100 [03:03<01:15,  2.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  At what temperatures was the assay completed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.89 Batches/s]\n",
            " 72%|███████▏  | 72/100 [03:05<01:12,  2.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What criteria sets the guideline for drug-like properties?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.26 Batches/s]\n",
            " 73%|███████▎  | 73/100 [03:08<01:09,  2.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What could be novel candidates as potent inhibitors of papain like cysteine proteases in resistant microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
            " 74%|███████▍  | 74/100 [03:11<01:08,  2.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What method is useful in administering small molecules for systemic delivery to the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.88 Batches/s]\n",
            " 75%|███████▌  | 75/100 [03:14<01:07,  2.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the nasal mucosa useful in the delivery of small molecules into the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.89 Batches/s]\n",
            " 76%|███████▌  | 76/100 [03:16<01:05,  2.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the most common methods of inhaled delivery of medications?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.20 Batches/s]\n",
            " 77%|███████▋  | 77/100 [03:19<01:02,  2.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medications have shown good promise to in vivo delivery via dry powder inhalers?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.85 Batches/s]\n",
            " 78%|███████▊  | 78/100 [03:22<00:58,  2.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How are siRNAs typically delivered for systemic effect?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.95 Batches/s]\n",
            " 79%|███████▉  | 79/100 [03:23<00:50,  2.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What structures form the human airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 80%|████████  | 80/100 [03:26<00:48,  2.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What size of particle has been shown to be most effective in the delivery to the lower airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.87 Batches/s]\n",
            " 81%|████████  | 81/100 [03:29<00:48,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the essential conditions in siRNA delivery to effectively produce gene silencing in the lungs?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.33 Batches/s]\n",
            " 82%|████████▏ | 82/100 [03:32<00:47,  2.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long is the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.35 Batches/s]\n",
            " 83%|████████▎ | 83/100 [03:34<00:44,  2.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many open reading frames are in the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.97 Batches/s]\n",
            " 84%|████████▍ | 84/100 [03:37<00:42,  2.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What virus has the closest genetic identity with the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.19 Batches/s]\n",
            " 85%|████████▌ | 85/100 [03:39<00:39,  2.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many surgical masks or respirators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.11 Batches/s]\n",
            " 86%|████████▌ | 86/100 [03:42<00:37,  2.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the acronym MERS-CoV?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.31 Batches/s]\n",
            " 87%|████████▋ | 87/100 [03:45<00:34,  2.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the critical factors that determine the effect of an epidemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
            " 88%|████████▊ | 88/100 [03:47<00:31,  2.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When did the World Health Organization (WHO) officially declare the 2019-nCoV epidemic as a Public Health Emergency of International Concern?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.95 Batches/s]\n",
            " 89%|████████▉ | 89/100 [03:50<00:29,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What influenza virus was identified in China in 2013?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.35 Batches/s]\n",
            " 90%|█████████ | 90/100 [03:53<00:26,  2.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What past research has been done on severe, single-wave pandemics?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.89 Batches/s]\n",
            " 91%|█████████ | 91/100 [03:55<00:21,  2.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a clinical attack rate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.99 Batches/s]\n",
            " 92%|█████████▏| 92/100 [03:57<00:18,  2.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the clinical attack rate in the 2009 H1N1 pandemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 93%|█████████▎| 93/100 [03:59<00:17,  2.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the estimated R0 of COVID-19?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.34 Batches/s]\n",
            " 94%|█████████▍| 94/100 [04:02<00:15,  2.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many ventilators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.84 Batches/s]\n",
            " 95%|█████████▌| 95/100 [04:05<00:12,  2.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How is exhaled breath condensate used in viral research?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.81 Batches/s]\n",
            " 96%|█████████▌| 96/100 [04:07<00:10,  2.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many patients were i this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.94 Batches/s]\n",
            " 97%|█████████▋| 97/100 [04:09<00:06,  2.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the conclusion of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.84 Batches/s]\n",
            " 98%|█████████▊| 98/100 [04:12<00:04,  2.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long did the patient breath into the RTube?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.19 Batches/s]\n",
            " 99%|█████████▉| 99/100 [04:14<00:02,  2.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What followed the reverse transcription step in the analysis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.16 Batches/s]\n",
            "100%|██████████| 100/100 [04:17<00:00,  2.58s/it]\n"
          ]
        }
      ],
      "source": [
        "retriever = TfidfRetriever(document_store=document_store_wiki)\n",
        "\n",
        "reader = FARMReader(\n",
        "    model_name_or_path=model, #\"deepset/roberta-base-squad2\"\n",
        "    use_gpu=use_gpu\n",
        ")\n",
        "\n",
        "pipe_wiki = ExtractiveQAPipeline(reader, retriever)\n",
        "\n",
        "df_res_wiki_tfidf = qa_dataset.copy()\n",
        "\n",
        "# prepare columns for answers\n",
        "df_res_wiki_tfidf['predictions_wiki_context_tfidf'] = [list() for x in range(len(df_res_wiki_tfidf.index))]\n",
        "\n",
        "for q_i in tqdm(range(len(qa_dataset.question.tolist()))):\n",
        "    \n",
        "    print('question : ', qa_dataset.question[q_i])\n",
        "    \n",
        "    try:\n",
        "        # covid dataset context prediction\n",
        "        prediction_wiki = pipe_wiki.run(\n",
        "            query=qa_dataset.question[q_i],\n",
        "            params={\n",
        "                \"Retriever\" : {\"top_k\": top_k_retriever},\n",
        "                \"Reader\": {\"top_k\": top_k_reader}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        df_res_wiki_tfidf.loc[q_i, 'predictions_wiki_context_tfidf'].append(\n",
        "            [prediction_wiki['answers'][k].answer for k in range(len(prediction_wiki['answers']))]\n",
        "        )\n",
        "    except:\n",
        "        df_res_wiki_tfidf.loc[q_i, 'predictions_wiki_context_tfidf'].append([])\n",
        "\n",
        "\n",
        "df_res_wiki_tfidf.to_csv('/content/drive/MyDrive/DeepLearning/df_res_wiki_tfidf_org_data.csv', index=False)"
      ],
      "id": "371de3e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS24YsUXwQ-N"
      },
      "source": [
        "#### RE-INITIALISE ELASTICSEARCH"
      ],
      "id": "VS24YsUXwQ-N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4ace601"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "id": "b4ace601"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrCc18uowquf"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "id": "hrCc18uowquf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruOCBh9Zwqi9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(30)"
      ],
      "id": "ruOCBh9Zwqi9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYKDs9vJwqVE"
      },
      "outputs": [],
      "source": [
        "documents_lst_wiki = df_wiki.to_dict(orient='records')"
      ],
      "id": "GYKDs9vJwqVE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk6T4wsixCQO",
        "outputId": "1ac30c83-c2ca-49ad-982a-654c7ba93bd3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 205328/205328 [00:00<00:00, 274181.01it/s]\n"
          ]
        }
      ],
      "source": [
        "# the way document_store wants data formatted\n",
        "dicts_wiki = [\n",
        "    {\n",
        "        'content' : elm['text'],\n",
        "        'meta' : {\n",
        "            'name' : elm['title']\n",
        "        }\n",
        "    } \n",
        "    for elm in tqdm(documents_lst_wiki)\n",
        "]"
      ],
      "id": "yk6T4wsixCQO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JinrRnaGxB5k",
        "outputId": "66b819e1-dc4e-484c-90fb-fa0dec780d41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 16.6 s, sys: 530 ms, total: 17.2 s\n",
            "Wall time: 7min 10s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "document_store_wiki = ElasticsearchDocumentStore(\n",
        "    port=9200\n",
        ")\n",
        "document_store_wiki.delete_documents()\n",
        "document_store_wiki.write_documents(\n",
        "    documents=dicts_wiki\n",
        ")"
      ],
      "id": "JinrRnaGxB5k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-DMeKUNxRzs"
      },
      "source": [
        "#### DensePassageRetriever"
      ],
      "id": "M-DMeKUNxRzs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "7ef0500ec43c4bd0b1e53b7d61fa5368",
            "71ddcab38f3041a8aad236526a51b378",
            "4771cb3fe87a4a14875c11593e451cb7",
            "0eb4cecad09346f68e4f091037cae185",
            "9e346bb7a4e5406db064db077754326b",
            "ad09ba7aaaa746af8c04814048bd13a3",
            "1af92a07b1f24d08aaa44c32024308e1",
            "a430912b4790440185645f9e6aad2bff",
            "48613288d7264d59834f995efe5fe2cd",
            "8ac5f13aed224b7caf728f6a5684691d",
            "23e27a03940c4a2ba29f24f760aec3f8",
            "aa8919339ca748618b9172134d9d57ce",
            "c2432ab627e142bd8e684a41bcba0bbb",
            "623fa9bf9a1146bfab054bc7fcc2aaae",
            "84c97522b5bf48a1ace8f8b3bbade4ca",
            "08c345379ea747d4adf56029178678bf",
            "63a4b302243045c7b7b86b34caa16c0b",
            "34efb61579044eb0a86364052f4ed5d7",
            "f301da8245f04120851511653dd57bc9",
            "de90500a68834dc3a0c6402282f7841c",
            "9454e85ecbb643c29c213255e0aaf807",
            "aa1b908676c94e86bec5d4dc01b91995"
          ]
        },
        "id": "a-4vDxcIwpEL",
        "outputId": "7536d7a9-97be-4cd1-b6d8-de04e3155adf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ef0500ec43c4bd0b1e53b7d61fa5368",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Updating embeddings:   0%|          | 0/205308 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71ddcab38f3041a8aad236526a51b378",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4771cb3fe87a4a14875c11593e451cb7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0eb4cecad09346f68e4f091037cae185",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e346bb7a4e5406db064db077754326b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad09ba7aaaa746af8c04814048bd13a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1af92a07b1f24d08aaa44c32024308e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a430912b4790440185645f9e6aad2bff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48613288d7264d59834f995efe5fe2cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ac5f13aed224b7caf728f6a5684691d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23e27a03940c4a2ba29f24f760aec3f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa8919339ca748618b9172134d9d57ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2432ab627e142bd8e684a41bcba0bbb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "623fa9bf9a1146bfab054bc7fcc2aaae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84c97522b5bf48a1ace8f8b3bbade4ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08c345379ea747d4adf56029178678bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63a4b302243045c7b7b86b34caa16c0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34efb61579044eb0a86364052f4ed5d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f301da8245f04120851511653dd57bc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de90500a68834dc3a0c6402282f7841c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9454e85ecbb643c29c213255e0aaf807",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/10000 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa1b908676c94e86bec5d4dc01b91995",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Create embeddings:   0%|          | 0/5312 [00:00<?, ? Docs/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]WARNING:haystack.nodes.retriever.sparse:Indexed documents have been updated and fit() method needs to be run before retrieval. Running it now.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the main cause of HIV-1 infection in children?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.04 Batches/s]\n",
            "  1%|          | 1/100 [02:23<3:57:34, 143.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What plays the crucial role in the Mother to Child Transmission of HIV-1 and what increases the risk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.26 Batches/s]\n",
            "  2%|▏         | 2/100 [02:27<1:39:56, 61.19s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many children were infected by HIV-1 in 2008-2009, worldwide?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            "  3%|▎         | 3/100 [02:29<55:46, 34.50s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the role of C-C Motif Chemokine Ligand 3 Like 1 (CCL3L1) in mother to child transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.31 Batches/s]\n",
            "  4%|▍         | 4/100 [02:33<35:22, 22.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is DC-GENR and where is  it expressed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.94 Batches/s]\n",
            "  5%|▌         | 5/100 [02:35<23:56, 15.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does the presence of DC-SIGNR affect the MTCT of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.24 Batches/s]\n",
            "  6%|▌         | 6/100 [02:38<17:09, 10.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why do low levels of DC-SIGNR enhance Mother to Child Transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.96 Batches/s]\n",
            "  7%|▋         | 7/100 [02:41<12:48,  8.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the percentage of Mother to Child Transmission of HIV-1, when there is no intervention?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            "  8%|▊         | 8/100 [02:44<10:06,  6.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Does C-C chemokine receptor type 5 (CCR5) affect the transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            "  9%|▉         | 9/100 [02:47<08:14,  5.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does Mannanose Binding Lectin (MBL) affect elimination of HIV-1 pathogen?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.10 Batches/s]\n",
            " 10%|█         | 10/100 [02:49<06:47,  4.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How can CCR5's effect in HIV-1 transmission be reduced?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.09 Batches/s]\n",
            " 11%|█         | 11/100 [02:52<05:50,  3.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is IFITM?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.41 Batches/s]\n",
            " 12%|█▏        | 12/100 [02:54<04:59,  3.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cysteine residues are contained in the first transmembrane domain of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.42 Batches/s]\n",
            " 13%|█▎        | 13/100 [02:57<04:46,  3.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What inhibits S-palmitoylation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.36 Batches/s]\n",
            " 14%|█▍        | 14/100 [02:58<03:53,  2.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What interaction is inhibited by the presence of 2-bromopalmitic acid (2BP)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            " 15%|█▌        | 15/100 [03:01<03:55,  2.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a function associated with IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.22 Batches/s]\n",
            " 16%|█▌        | 16/100 [03:04<03:41,  2.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the antiviral activity of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.93 Batches/s]\n",
            " 17%|█▋        | 17/100 [03:07<03:44,  2.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is another name for IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 18%|█▊        | 18/100 [03:09<03:35,  2.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the expression of IFITM5 not promoted by interferons?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.85 Batches/s]\n",
            " 19%|█▉        | 19/100 [03:12<03:43,  2.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM5 and the other IFITM proteins?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.93 Batches/s]\n",
            " 20%|██        | 20/100 [03:15<03:46,  2.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM 1, IFITM 2, and IFITM 3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.14 Batches/s]\n",
            " 21%|██        | 21/100 [03:18<03:44,  2.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What amino acid might be involved in calcium binding in the C-terminal region of a protein?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.27 Batches/s]\n",
            " 22%|██▏       | 22/100 [03:21<03:46,  2.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 23%|██▎       | 23/100 [03:24<03:43,  2.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the molecular structure of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.26 Batches/s]\n",
            " 24%|██▍       | 24/100 [03:27<03:40,  2.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many nucleotides does bovine coronavirus contain?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.15 Batches/s]\n",
            " 25%|██▌       | 25/100 [03:28<03:07,  2.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of the orf1ab gene in bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.06 Batches/s]\n",
            " 26%|██▌       | 26/100 [03:31<03:17,  2.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is the orf1ab gene at the 3' or 5' end of the bovine coronavirus genome?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.93 Batches/s]\n",
            " 27%|██▋       | 27/100 [03:34<03:22,  2.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a significant cause of Influenze like illness among healthy adolescents and adults presenting for medical evaluation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.31 Batches/s]\n",
            " 28%|██▊       | 28/100 [03:37<03:21,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the most common species of Human Coronavirus among adults?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 29%|██▉       | 29/100 [03:40<03:21,  2.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which Human Coronavirus showed species specific clinical characteristics of its infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            " 30%|███       | 30/100 [03:43<03:14,  2.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What causes the outbreak of SARS and MERS.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.93 Batches/s]\n",
            " 31%|███       | 31/100 [03:46<03:14,  2.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the case fatality rate of SARS and MERS?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.10 Batches/s]\n",
            " 32%|███▏      | 32/100 [03:49<03:15,  2.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What were the common HCOV strains in the 5 year USA study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.37 Batches/s]\n",
            " 33%|███▎      | 33/100 [03:52<03:14,  2.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which species are more prevalent but less severe?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  7.02 Batches/s]\n",
            " 34%|███▍      | 34/100 [03:54<02:49,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is required for a Hepatitis B infection in cells?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.25 Batches/s]\n",
            " 35%|███▌      | 35/100 [03:56<02:51,  2.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the broad, but less specific, virus-cell interaction in a hepatitis B infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.96 Batches/s]\n",
            " 36%|███▌      | 36/100 [03:59<02:55,  2.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which protein domain of the Hepatitis B envelope is necessary for infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.88 Batches/s]\n",
            " 37%|███▋      | 37/100 [04:02<02:57,  2.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Where is NTCP located in the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.95 Batches/s]\n",
            " 38%|███▊      | 38/100 [04:05<02:58,  2.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What does the NTCP protein mediate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.36 Batches/s]\n",
            " 39%|███▉      | 39/100 [04:08<02:52,  2.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is NTCP sufficient to allow HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.84 Batches/s]\n",
            " 40%|████      | 40/100 [04:11<02:42,  2.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is NTCP thought to not be sufficient for HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.31 Batches/s]\n",
            " 41%|████      | 41/100 [04:13<02:37,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kinds of viruses are Japanese encephalitis virus(JEV), tick-borne encephalitis virus(TBEV), eastern equine encephalitis virus (EEEV), sindbis virus(SV), and dengue virus(DV)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.35 Batches/s]\n",
            " 42%|████▏     | 42/100 [04:16<02:37,  2.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the current clinically-available methods to detect encephalitis viral antigens?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.00 Batches/s]\n",
            " 43%|████▎     | 43/100 [04:19<02:35,  2.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What methods exist for detecting multiple antigens simultaneously in a one-sample, laboratory test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.81 Batches/s]\n",
            " 44%|████▍     | 44/100 [04:21<02:34,  2.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many antigens could be detected by Liew's multiplex ELISA test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
            " 45%|████▌     | 45/100 [04:24<02:21,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kind of antibodies were used in the ELISA-array assay?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.75 Batches/s]\n",
            " 46%|████▌     | 46/100 [04:27<02:27,  2.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            " 47%|████▋     | 47/100 [04:30<02:25,  2.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What capture antibodies were used in the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.87 Batches/s]\n",
            " 48%|████▊     | 48/100 [04:32<02:25,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the spotting concentration range for the capture antibodies?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.85 Batches/s]\n",
            " 49%|████▉     | 49/100 [04:35<02:23,  2.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the proper spotting concentration determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.89 Batches/s]\n",
            " 50%|█████     | 50/100 [04:38<02:20,  2.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was cross reaction detection determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.69 Batches/s]\n",
            " 51%|█████     | 51/100 [04:40<02:05,  2.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA-array assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.23 Batches/s]\n",
            " 52%|█████▏    | 52/100 [04:43<02:08,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  In 2010, how many cases of tuberculosis were estimated in China?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            " 53%|█████▎    | 53/100 [04:46<02:09,  2.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the population of Shandong province?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.43 Batches/s]\n",
            " 54%|█████▍    | 54/100 [04:49<02:09,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the purpose of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 55%|█████▌    | 55/100 [04:52<02:07,  2.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the age range for the people surveyed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.32 Batches/s]\n",
            " 56%|█████▌    | 56/100 [04:55<02:05,  2.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the survey designed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.04 Batches/s]\n",
            " 57%|█████▋    | 57/100 [04:58<02:03,  2.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Was was the sample size?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
            " 58%|█████▊    | 58/100 [05:00<01:58,  2.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How were the clusters selected?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
            " 59%|█████▉    | 59/100 [05:03<01:54,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many people were in a community cluster?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.93 Batches/s]\n",
            " 60%|██████    | 60/100 [05:06<01:52,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who was excluded from the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.43 Batches/s]\n",
            " 61%|██████    | 61/100 [05:09<01:50,  2.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When was the study conducted?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.97 Batches/s]\n",
            " 62%|██████▏   | 62/100 [05:12<01:46,  2.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who conducted the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            " 63%|██████▎   | 63/100 [05:14<01:43,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medium was used to collect the sputum samples?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.87 Batches/s]\n",
            " 64%|██████▍   | 64/100 [05:17<01:41,  2.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the response rate for the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.82 Batches/s]\n",
            " 65%|██████▌   | 65/100 [05:20<01:38,  2.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the average age of a study participant?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.19 Batches/s]\n",
            " 66%|██████▌   | 66/100 [05:23<01:36,  2.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the prevalence rate in Shandong in 2010 for sputum positive cases of tuberculosis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            " 67%|██████▋   | 67/100 [05:26<01:36,  2.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the most striking finding of the study regarding tuberculosis patients?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.93 Batches/s]\n",
            " 68%|██████▊   | 68/100 [05:29<01:34,  2.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cases of sputum positive tuberculosis patients had no persistent cough?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.05 Batches/s]\n",
            " 69%|██████▉   | 69/100 [05:32<01:28,  2.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many tuberculosis patients in Shandong were over 65 years old?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            " 70%|███████   | 70/100 [05:34<01:24,  2.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What enzymes have been reported to be linked with severity of infection and various pathological conditions caused by microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.85 Batches/s]\n",
            " 71%|███████   | 71/100 [05:37<01:23,  2.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  At what temperatures was the assay completed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.80 Batches/s]\n",
            " 72%|███████▏  | 72/100 [05:40<01:21,  2.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What criteria sets the guideline for drug-like properties?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.19 Batches/s]\n",
            " 73%|███████▎  | 73/100 [05:43<01:17,  2.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What could be novel candidates as potent inhibitors of papain like cysteine proteases in resistant microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 74%|███████▍  | 74/100 [05:46<01:14,  2.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What method is useful in administering small molecules for systemic delivery to the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 75%|███████▌  | 75/100 [05:49<01:12,  2.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the nasal mucosa useful in the delivery of small molecules into the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            " 76%|███████▌  | 76/100 [05:52<01:10,  2.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the most common methods of inhaled delivery of medications?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.29 Batches/s]\n",
            " 77%|███████▋  | 77/100 [05:55<01:07,  2.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medications have shown good promise to in vivo delivery via dry powder inhalers?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 78%|███████▊  | 78/100 [05:58<01:03,  2.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How are siRNAs typically delivered for systemic effect?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.82 Batches/s]\n",
            " 79%|███████▉  | 79/100 [06:00<00:55,  2.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What structures form the human airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 80%|████████  | 80/100 [06:03<00:53,  2.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What size of particle has been shown to be most effective in the delivery to the lower airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.00 Batches/s]\n",
            " 81%|████████  | 81/100 [06:06<00:53,  2.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the essential conditions in siRNA delivery to effectively produce gene silencing in the lungs?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 82%|████████▏ | 82/100 [06:09<00:50,  2.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long is the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 83%|████████▎ | 83/100 [06:11<00:48,  2.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many open reading frames are in the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.94 Batches/s]\n",
            " 84%|████████▍ | 84/100 [06:14<00:45,  2.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What virus has the closest genetic identity with the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 85%|████████▌ | 85/100 [06:17<00:42,  2.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many surgical masks or respirators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.85 Batches/s]\n",
            " 86%|████████▌ | 86/100 [06:20<00:40,  2.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the acronym MERS-CoV?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 87%|████████▋ | 87/100 [06:23<00:36,  2.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the critical factors that determine the effect of an epidemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.81 Batches/s]\n",
            " 88%|████████▊ | 88/100 [06:26<00:34,  2.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When did the World Health Organization (WHO) officially declare the 2019-nCoV epidemic as a Public Health Emergency of International Concern?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.94 Batches/s]\n",
            " 89%|████████▉ | 89/100 [06:29<00:31,  2.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What influenza virus was identified in China in 2013?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.93 Batches/s]\n",
            " 90%|█████████ | 90/100 [06:31<00:28,  2.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What past research has been done on severe, single-wave pandemics?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.90 Batches/s]\n",
            " 91%|█████████ | 91/100 [06:33<00:23,  2.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a clinical attack rate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.91 Batches/s]\n",
            " 92%|█████████▏| 92/100 [06:36<00:19,  2.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the clinical attack rate in the 2009 H1N1 pandemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.86 Batches/s]\n",
            " 93%|█████████▎| 93/100 [06:39<00:18,  2.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the estimated R0 of COVID-19?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.88 Batches/s]\n",
            " 94%|█████████▍| 94/100 [06:42<00:16,  2.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many ventilators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.87 Batches/s]\n",
            " 95%|█████████▌| 95/100 [06:45<00:14,  2.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How is exhaled breath condensate used in viral research?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.24 Batches/s]\n",
            " 96%|█████████▌| 96/100 [06:47<00:11,  2.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many patients were i this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  6.67 Batches/s]\n",
            " 97%|█████████▋| 97/100 [06:49<00:07,  2.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the conclusion of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  5.20 Batches/s]\n",
            " 98%|█████████▊| 98/100 [06:52<00:05,  2.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long did the patient breath into the RTube?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.92 Batches/s]\n",
            " 99%|█████████▉| 99/100 [06:55<00:02,  2.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What followed the reverse transcription step in the analysis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.84 Batches/s]\n",
            "100%|██████████| 100/100 [06:58<00:00,  4.18s/it]\n"
          ]
        }
      ],
      "source": [
        "from haystack.nodes import DensePassageRetriever  \n",
        "retriever_DPR = DensePassageRetriever(document_store=document_store_wiki,\n",
        "    query_embedding_model=\"bert-base-uncased\",\n",
        "    passage_embedding_model=\"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "document_store_wiki.update_embeddings(retriever_DPR)\n",
        "\n",
        "reader = FARMReader(\n",
        "    model_name_or_path=model, #\"deepset/roberta-base-squad2\"\n",
        "    use_gpu=use_gpu\n",
        ")\n",
        "\n",
        "pipe_covid_DPR = ExtractiveQAPipeline(reader, retriever_tfidf)\n",
        "\n",
        "df_res_wiki_DPR = qa_dataset.copy()\n",
        "\n",
        "# prepare columns for answers\n",
        "df_res_wiki_DPR['predictions_wiki_context_DPR'] = [list() for x in range(len(df_res_wiki_DPR.index))]\n",
        "\n",
        "for q_i in tqdm(range(len(qa_dataset.question.tolist()))):\n",
        "\n",
        "    print('question : ', qa_dataset.question[q_i])\n",
        "\n",
        "    try:\n",
        "        # covid dataset context prediction\n",
        "        prediction_covid = pipe_covid_DPR.run(\n",
        "            query=qa_dataset.question[q_i],\n",
        "            params={\n",
        "                \"Retriever\" : {\"top_k\": top_k_retriever},\n",
        "                \"Reader\": {\"top_k\": top_k_reader}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        df_res_wiki_DPR.loc[q_i, 'predictions_wiki_context_DPR'].append(\n",
        "            [prediction_covid['answers'][k].answer for k in range(len(prediction_covid['answers']))]\n",
        "        )\n",
        "    except:\n",
        "        df_res_wiki_DPR.loc[q_i, 'predictions_wiki_context_DPR'].append([])\n",
        "\n",
        "df_res_wiki_DPR.to_csv('/content/drive/MyDrive/DeepLearning/df_res_wiki_DPR.csv', index=False)"
      ],
      "id": "a-4vDxcIwpEL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b918c5b"
      },
      "source": [
        "# Wikipedia API"
      ],
      "id": "2b918c5b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b36ea6d6"
      },
      "source": [
        "### Document store"
      ],
      "id": "b36ea6d6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ixsiOIc4SJA"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "id": "2ixsiOIc4SJA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIvdQqfD4SJC"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "id": "BIvdQqfD4SJC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqfg2E7u4SJC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(30)"
      ],
      "id": "bqfg2E7u4SJC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b417fb41",
        "outputId": "3cac6572-9213-4151-c803-2b058aa6610d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 18 ms, sys: 6.98 ms, total: 25 ms\n",
            "Wall time: 2.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "\n",
        "document_store_wikiAPI = ElasticsearchDocumentStore(\n",
        "    port=9200\n",
        ")\n",
        "document_store_wikiAPI.delete_documents()"
      ],
      "id": "b417fb41"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfc9e78d"
      },
      "source": [
        "### Retrieving answers"
      ],
      "id": "dfc9e78d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8a5bd37"
      },
      "source": [
        "#### BM25"
      ],
      "id": "a8a5bd37"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f82f36ec",
        "outputId": "2277d8a3-ad8b-4ce5-e53b-e042a621206a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the main cause of HIV-1 infection in children?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "\r  1%|          | 1/100 [00:01<02:45,  1.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What plays the crucial role in the Mother to Child Transmission of HIV-1 and what increases the risk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 2/100 [00:03<02:51,  1.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many children were infected by HIV-1 in 2008-2009, worldwide?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 3/100 [00:04<01:58,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the role of C-C Motif Chemokine Ligand 3 Like 1 (CCL3L1) in mother to child transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            "  4%|▍         | 4/100 [00:06<02:41,  1.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is DC-GENR and where is  it expressed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            "  5%|▌         | 5/100 [00:08<03:01,  1.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does the presence of DC-SIGNR affect the MTCT of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            "  6%|▌         | 6/100 [00:11<03:11,  2.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why do low levels of DC-SIGNR enhance Mother to Child Transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            "  7%|▋         | 7/100 [00:13<03:16,  2.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the percentage of Mother to Child Transmission of HIV-1, when there is no intervention?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  2.60it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.10it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.59it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.22it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  2.58it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:06,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.40 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23742, -23728) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-10516, -10498) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-19301, -19298) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-2446, -2436) with a span answer. \n",
            "  8%|▊         | 8/100 [00:27<09:10,  5.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Does C-C chemokine receptor type 5 (CCR5) affect the transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 9/100 [00:28<06:38,  4.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does Mannanose Binding Lectin (MBL) affect elimination of HIV-1 pathogen?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 10%|█         | 10/100 [00:30<05:36,  3.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How can CCR5's effect in HIV-1 transmission be reduced?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 11/100 [00:32<04:41,  3.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is IFITM?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:00<00:00,  2.39it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00,  2.66it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 1/3 [00:00<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 2/3 [00:01<00:00,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 3/3 [00:02<00:00,  1.38 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-12690, -12661) with a span answer. \n",
            " 12%|█▏        | 12/100 [00:39<06:10,  4.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cysteine residues are contained in the first transmembrane domain of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 13%|█▎        | 13/100 [00:41<05:15,  3.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What inhibits S-palmitoylation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.50it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.99it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.50it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.30it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.97it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.40 Batches/s]\n",
            " 14%|█▍        | 14/100 [00:50<07:43,  5.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What interaction is inhibited by the presence of 2-bromopalmitic acid (2BP)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 15%|█▌        | 15/100 [00:53<06:18,  4.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a function associated with IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.28 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28878, -28788) with a span answer. \n",
            " 16%|█▌        | 16/100 [00:58<06:39,  4.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the antiviral activity of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 17%|█▋        | 17/100 [01:00<05:32,  4.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is another name for IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:00<00:00,  3.05it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.43it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 1/3 [00:00<00:01,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 2/3 [00:01<00:00,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 3/3 [00:02<00:00,  1.42 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-15228, -15218) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23951, -23927) with a span answer. \n",
            " 18%|█▊        | 18/100 [01:07<06:26,  4.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the expression of IFITM5 not promoted by interferons?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 19%|█▉        | 19/100 [01:09<05:22,  3.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM5 and the other IFITM proteins?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 20%|██        | 20/100 [01:11<04:37,  3.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM 1, IFITM 2, and IFITM 3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 21%|██        | 21/100 [01:14<04:05,  3.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What amino acid might be involved in calcium binding in the C-terminal region of a protein?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 22/100 [01:16<03:33,  2.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 23/100 [01:19<03:38,  2.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the molecular structure of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.07it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.11it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.10it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.21it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.64it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/8 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  12%|█▎        | 1/8 [00:00<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 2/8 [00:01<00:04,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 3/8 [00:02<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 4/8 [00:03<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▎   | 5/8 [00:03<00:02,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 6/8 [00:04<00:01,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  88%|████████▊ | 7/8 [00:05<00:00,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 8/8 [00:05<00:00,  1.34 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-11654, -11646) with a span answer. \n",
            " 24%|██▍       | 24/100 [01:31<07:13,  5.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many nucleotides does bovine coronavirus contain?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:00<00:00,  3.48it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.20it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 1/3 [00:00<00:01,  1.23 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 3/3 [00:01<00:00,  1.77 Batches/s]\n",
            " 25%|██▌       | 25/100 [01:38<07:25,  5.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of the orf1ab gene in bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:00<00:00,  2.19 Batches/s]\n",
            " 26%|██▌       | 26/100 [01:42<06:41,  5.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is the orf1ab gene at the 3' or 5' end of the bovine coronavirus genome?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.09it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:00<00:00,  2.19 Batches/s]\n",
            " 27%|██▋       | 27/100 [01:46<06:05,  5.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a significant cause of Influenze like illness among healthy adolescents and adults presenting for medical evaluation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 28%|██▊       | 28/100 [01:48<05:01,  4.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the most common species of Human Coronavirus among adults?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 29%|██▉       | 29/100 [01:50<04:20,  3.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which Human Coronavirus showed species specific clinical characteristics of its infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 30/100 [01:53<03:46,  3.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What causes the outbreak of SARS and MERS.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 31%|███       | 31/100 [01:55<03:24,  2.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the case fatality rate of SARS and MERS?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 32/100 [01:57<02:55,  2.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What were the common HCOV strains in the 5 year USA study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:00<00:00,  1.52it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.31 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25816, -25765) with a span answer. \n",
            " 33%|███▎      | 33/100 [02:02<03:55,  3.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which species are more prevalent but less severe?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.18it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.09it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.25it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.73it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.20it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/6 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 1/6 [00:00<00:03,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 2/6 [00:01<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 3/6 [00:02<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 4/6 [00:03<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 5/6 [00:03<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 6/6 [00:04<00:00,  1.43 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30572, -30556) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-16681, -16670) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31520, -31515) with a span answer. \n",
            " 34%|███▍      | 34/100 [02:14<06:36,  6.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is required for a Hepatitis B infection in cells?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 35%|███▌      | 35/100 [02:17<05:27,  5.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the broad, but less specific, virus-cell interaction in a hepatitis B infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.21it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.63it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.78it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.15it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  2.83it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/8 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  12%|█▎        | 1/8 [00:00<00:05,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 2/8 [00:01<00:04,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 3/8 [00:02<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 4/8 [00:03<00:03,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▎   | 5/8 [00:03<00:02,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 6/8 [00:04<00:01,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  88%|████████▊ | 7/8 [00:05<00:00,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 8/8 [00:05<00:00,  1.36 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27416, -27412) with a span answer. \n",
            " 36%|███▌      | 36/100 [02:30<07:55,  7.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which protein domain of the Hepatitis B envelope is necessary for infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.18it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.95it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.50it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.31it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.30it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.60 Batches/s]\n",
            " 37%|███▋      | 37/100 [02:39<08:08,  7.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Where is NTCP located in the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.51it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.14 Batches/s]\n",
            " 38%|███▊      | 38/100 [02:42<06:46,  6.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What does the NTCP protein mediate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:00<00:00,  2.39it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.43it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 1/3 [00:00<00:01,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 2/3 [00:01<00:00,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 3/3 [00:01<00:00,  1.58 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-19536, -19515) with a span answer. \n",
            " 39%|███▉      | 39/100 [02:48<06:24,  6.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is NTCP sufficient to allow HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 40%|████      | 40/100 [02:50<05:06,  5.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is NTCP thought to not be sufficient for HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 41%|████      | 41/100 [02:53<04:10,  4.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kinds of viruses are Japanese encephalitis virus(JEV), tick-borne encephalitis virus(TBEV), eastern equine encephalitis virus (EEEV), sindbis virus(SV), and dengue virus(DV)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 42%|████▏     | 42/100 [02:55<03:32,  3.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the current clinically-available methods to detect encephalitis viral antigens?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 43/100 [02:56<02:51,  3.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What methods exist for detecting multiple antigens simultaneously in a one-sample, laboratory test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 44/100 [02:57<02:13,  2.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many antigens could be detected by Liew's multiplex ELISA test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 45%|████▌     | 45/100 [03:00<02:09,  2.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kind of antibodies were used in the ELISA-array assay?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 46/100 [03:01<01:44,  1.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 47/100 [03:02<01:28,  1.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What capture antibodies were used in the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 48/100 [03:03<01:15,  1.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the spotting concentration range for the capture antibodies?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.42it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.60it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  3.00it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.62it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.05it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/7 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 1/7 [00:00<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 2/7 [00:01<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 3/7 [00:02<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 4/7 [00:03<00:02,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 5/7 [00:03<00:01,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 6/7 [00:04<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 7/7 [00:05<00:00,  1.36 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22622, -22617) with a span answer. \n",
            " 49%|████▉     | 49/100 [03:14<03:53,  4.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the proper spotting concentration determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.05it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.26it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.89it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.08it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.35it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/6 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 1/6 [00:00<00:03,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 2/6 [00:01<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 3/6 [00:02<00:02,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 4/6 [00:03<00:01,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 5/6 [00:03<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 6/6 [00:04<00:00,  1.49 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22053, -22045) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25785, -25768) with a span answer. \n",
            " 50%|█████     | 50/100 [03:25<05:23,  6.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was cross reaction detection determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.64it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.58it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.91it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.71it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.10it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:07,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:07<00:01,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:08<00:00,  1.28 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-24636, -24593) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21667, -21594) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-14033, -14018) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-18024, -17919) with a span answer. \n",
            " 51%|█████     | 51/100 [03:41<07:32,  9.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA-array assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 52/100 [03:43<05:37,  7.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  In 2010, how many cases of tuberculosis were estimated in China?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.74it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.68it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.55it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.47it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.30it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/6 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 1/6 [00:00<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 2/6 [00:01<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 3/6 [00:02<00:02,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 4/6 [00:03<00:01,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 5/6 [00:03<00:00,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 6/6 [00:04<00:00,  1.36 Batches/s]\n",
            " 53%|█████▎    | 53/100 [03:55<06:38,  8.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the population of Shandong province?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 54%|█████▍    | 54/100 [03:56<04:50,  6.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the purpose of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 55%|█████▌    | 55/100 [03:57<03:33,  4.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the age range for the people surveyed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 56/100 [03:58<02:39,  3.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the survey designed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.46it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.74it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.47it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.00it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.18it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.41 Batches/s]\n",
            " 57%|█████▋    | 57/100 [04:07<03:46,  5.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Was was the sample size?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 58/100 [04:10<03:09,  4.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How were the clusters selected?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 59%|█████▉    | 59/100 [04:12<02:29,  3.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many people were in a community cluster?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  2.41it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.07it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.73it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.43it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  2.93it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/7 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 1/7 [00:00<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 2/7 [00:01<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 3/7 [00:02<00:03,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 4/7 [00:03<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 5/7 [00:03<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 6/7 [00:04<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 7/7 [00:05<00:00,  1.30 Batches/s]\n",
            " 60%|██████    | 60/100 [04:24<04:09,  6.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who was excluded from the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 61%|██████    | 61/100 [04:26<03:12,  4.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When was the study conducted?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.60it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.77it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.47it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.71it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.64it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/5 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 1/5 [00:00<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 2/5 [00:01<00:02,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 3/5 [00:02<00:01,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 5/5 [00:03<00:00,  1.58 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-32540, -32536) with a span answer. \n",
            " 62%|██████▏   | 62/100 [04:36<04:03,  6.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who conducted the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.96it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.29it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.12it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.71it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.48it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/5 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 1/5 [00:00<00:03,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 2/5 [00:01<00:02,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 3/5 [00:02<00:01,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 4/5 [00:03<00:00,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 5/5 [00:03<00:00,  1.35 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25967, -25961) with a span answer. \n",
            " 63%|██████▎   | 63/100 [04:45<04:32,  7.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medium was used to collect the sputum samples?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25%|██▌       | 1/4 [00:00<00:00,  3.56it/s]\u001b[A\n",
            " 50%|█████     | 2/4 [00:00<00:00,  2.03it/s]\u001b[A\n",
            " 75%|███████▌  | 3/4 [00:01<00:00,  2.37it/s]\u001b[A\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.47it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/8 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  12%|█▎        | 1/8 [00:00<00:05,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 2/8 [00:01<00:04,  1.24 Batches/s]\u001b[A\n",
            "Inferencing Samples:  38%|███▊      | 3/8 [00:02<00:04,  1.24 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 4/8 [00:03<00:03,  1.24 Batches/s]\u001b[A\n",
            "Inferencing Samples:  62%|██████▎   | 5/8 [00:04<00:02,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 6/8 [00:04<00:01,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples:  88%|████████▊ | 7/8 [00:05<00:00,  1.24 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 8/8 [00:05<00:00,  1.39 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21053, -21034) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-19268, -19251) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-13626, -13613) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-10886, -10869) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-10037, -10026) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-6895, -6888) with a span answer. \n",
            " 64%|██████▍   | 64/100 [04:57<05:17,  8.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the response rate for the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.77it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.90it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.65it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.88it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.20it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 1/2 [00:00<00:00,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:01<00:00,  1.58 Batches/s]\n",
            " 65%|██████▌   | 65/100 [05:05<04:52,  8.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the average age of a study participant?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.75it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.60it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.88it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.38it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.29it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.33 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.58 Batches/s]\n",
            " 66%|██████▌   | 66/100 [05:14<04:54,  8.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the prevalence rate in Shandong in 2010 for sputum positive cases of tuberculosis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.33 Batches/s]\n",
            " 67%|██████▋   | 67/100 [05:18<04:01,  7.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the most striking finding of the study regarding tuberculosis patients?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 68/100 [05:20<03:01,  5.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cases of sputum positive tuberculosis patients had no persistent cough?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  2.17it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.20it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.15it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.63it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  10%|█         | 1/10 [00:00<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 2/10 [00:01<00:06,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  30%|███       | 3/10 [00:02<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 4/10 [00:03<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 5/10 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 6/10 [00:04<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  70%|███████   | 7/10 [00:05<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 8/10 [00:06<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  90%|█████████ | 9/10 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 10/10 [00:07<00:00,  1.35 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-14903, -14891) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-9982, -9963) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23571, -23558) with a span answer. \n",
            " 69%|██████▉   | 69/100 [05:35<04:23,  8.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many tuberculosis patients in Shandong were over 65 years old?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  2.52it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.84it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.32it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.76it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  2.69it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/7 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 1/7 [00:00<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 2/7 [00:01<00:03,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 3/7 [00:02<00:03,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 4/7 [00:03<00:02,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 5/7 [00:03<00:01,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 6/7 [00:04<00:00,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 7/7 [00:05<00:00,  1.36 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21505, -21499) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-12621, -12611) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-15186, -15162) with a span answer. \n",
            " 70%|███████   | 70/100 [05:48<04:52,  9.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What enzymes have been reported to be linked with severity of infection and various pathological conditions caused by microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 71%|███████   | 71/100 [05:51<03:41,  7.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  At what temperatures was the assay completed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.15it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.64it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.06it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.40it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.28it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/5 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 1/5 [00:00<00:03,  1.32 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 2/5 [00:01<00:02,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 3/5 [00:02<00:01,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 4/5 [00:03<00:00,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 5/5 [00:03<00:00,  1.38 Batches/s]\n",
            " 72%|███████▏  | 72/100 [06:00<03:51,  8.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What criteria sets the guideline for drug-like properties?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 73/100 [06:02<02:50,  6.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What could be novel candidates as potent inhibitors of papain like cysteine proteases in resistant microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 74%|███████▍  | 74/100 [06:04<02:13,  5.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What method is useful in administering small molecules for systemic delivery to the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.16it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.72it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.21it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.48it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.35it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/6 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  17%|█▋        | 1/6 [00:00<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 2/6 [00:01<00:03,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 3/6 [00:02<00:02,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 4/6 [00:03<00:01,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  83%|████████▎ | 5/6 [00:03<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 6/6 [00:04<00:00,  1.32 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20973, -20965) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21949, -21939) with a span answer. \n",
            " 75%|███████▌  | 75/100 [06:15<02:51,  6.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the nasal mucosa useful in the delivery of small molecules into the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 76/100 [06:17<02:10,  5.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the most common methods of inhaled delivery of medications?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.13it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.77it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.62it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.93it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.05it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/7 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 1/7 [00:00<00:04,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 2/7 [00:01<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 3/7 [00:02<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 4/7 [00:03<00:02,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 5/7 [00:03<00:01,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 6/7 [00:04<00:00,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 7/7 [00:05<00:00,  1.38 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-16873, -16817) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-5863, -5851) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-948, -925) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29283, -29276) with a span answer. \n",
            " 77%|███████▋  | 77/100 [06:29<02:46,  7.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medications have shown good promise to in vivo delivery via dry powder inhalers?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 78%|███████▊  | 78/100 [06:31<02:06,  5.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How are siRNAs typically delivered for systemic effect?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  2.35it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.10it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.25it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.49it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.36it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/5 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 1/5 [00:00<00:03,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 2/5 [00:01<00:02,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 3/5 [00:02<00:01,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 4/5 [00:03<00:00,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 5/5 [00:03<00:00,  1.27 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17847, -17806) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25119, -25082) with a span answer. \n",
            " 79%|███████▉  | 79/100 [06:42<02:31,  7.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What structures form the human airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 80/100 [06:44<01:51,  5.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What size of particle has been shown to be most effective in the delivery to the lower airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 81%|████████  | 81/100 [06:46<01:27,  4.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the essential conditions in siRNA delivery to effectively produce gene silencing in the lungs?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.00it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.47it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.86it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  1.91it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  2.62it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/7 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  14%|█▍        | 1/7 [00:00<00:04,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  29%|██▊       | 2/7 [00:01<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  43%|████▎     | 3/7 [00:02<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  57%|█████▋    | 4/7 [00:03<00:02,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  71%|███████▏  | 5/7 [00:03<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  86%|████████▌ | 6/7 [00:04<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 7/7 [00:05<00:00,  1.38 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-30243, -30212) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-26334, -26291) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-13461, -13454) with a span answer. \n",
            " 82%|████████▏ | 82/100 [06:58<02:03,  6.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long is the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 83%|████████▎ | 83/100 [07:00<01:33,  5.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many open reading frames are in the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 84%|████████▍ | 84/100 [07:02<01:12,  4.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What virus has the closest genetic identity with the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 85%|████████▌ | 85/100 [07:05<00:57,  3.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many surgical masks or respirators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.05it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.86it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.91it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.61it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:02<00:00,  2.13it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:07,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:07<00:01,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  91%|█████████ | 10/11 [00:07<00:00,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:08<00:00,  1.37 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22719, -22716) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-15827, -15797) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-28334, -28314) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-27582, -27576) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-15340, -15198) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22475, -22467) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-19981, -19944) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25445, -25435) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23751, -23744) with a span answer. \n",
            " 86%|████████▌ | 86/100 [07:21<01:46,  7.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the acronym MERS-CoV?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.53it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.18it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.07it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.20it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.03it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:06,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.24 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:04,  1.24 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.25 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.24 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.24 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.32 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-13344, -13320) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-19973, -19944) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-15834, -15829) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-15979, -15977) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-16218, -16195) with a span answer. \n",
            " 87%|████████▋ | 87/100 [07:35<02:02,  9.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the critical factors that determine the effect of an epidemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 88/100 [07:36<01:24,  7.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When did the World Health Organization (WHO) officially declare the 2019-nCoV epidemic as a Public Health Emergency of International Concern?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.08it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.34it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.22it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.31it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  2.53it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:07,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  27%|██▋       | 3/11 [00:02<00:06,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  36%|███▋      | 4/11 [00:03<00:05,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:04,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  55%|█████▍    | 6/11 [00:04<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  64%|██████▎   | 7/11 [00:05<00:03,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  73%|███████▎  | 8/11 [00:06<00:02,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  82%|████████▏ | 9/11 [00:06<00:01,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 11/11 [00:07<00:00,  1.40 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-20292, -20282) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-85, -73) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-3869, -3857) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-665, -652) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29304, -29293) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25302, -25290) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31895, -31889) with a span answer. \n",
            " 89%|████████▉ | 89/100 [07:52<01:45,  9.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What influenza virus was identified in China in 2013?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 90/100 [07:54<01:14,  7.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What past research has been done on severe, single-wave pandemics?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 91%|█████████ | 91/100 [07:56<00:51,  5.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a clinical attack rate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.97it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.91it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.66it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.92it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.09it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 1/3 [00:00<00:01,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 2/3 [00:01<00:00,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 3/3 [00:02<00:00,  1.37 Batches/s]\n",
            " 92%|█████████▏| 92/100 [08:04<00:51,  6.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the clinical attack rate in the 2009 H1N1 pandemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 93/100 [08:05<00:34,  4.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the estimated R0 of COVID-19?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  2.96it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.94it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.30it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:02<00:00,  1.33it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:03<00:00,  1.53it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  11%|█         | 1/9 [00:00<00:06,  1.31 Batches/s]\u001b[A\n",
            "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:05,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  33%|███▎      | 3/9 [00:02<00:04,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  44%|████▍     | 4/9 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  56%|█████▌    | 5/9 [00:03<00:03,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  67%|██████▋   | 6/9 [00:04<00:02,  1.30 Batches/s]\u001b[A\n",
            "Inferencing Samples:  78%|███████▊  | 7/9 [00:05<00:01,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples:  89%|████████▉ | 8/9 [00:06<00:00,  1.29 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 9/9 [00:06<00:00,  1.32 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-5639, -5624) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-14949, -14937) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-4027, -4020) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-5348, -5322) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-9747, -9743) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-21080, -21065) with a span answer. \n",
            " 94%|█████████▍| 94/100 [08:21<00:48,  8.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many ventilators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  5.77it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.61it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.42it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.07it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.28it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  10%|█         | 1/10 [00:00<00:07,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 2/10 [00:01<00:06,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  30%|███       | 3/10 [00:02<00:05,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 4/10 [00:03<00:04,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  50%|█████     | 5/10 [00:03<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 6/10 [00:04<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  70%|███████   | 7/10 [00:05<00:02,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 8/10 [00:06<00:01,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples:  90%|█████████ | 9/10 [00:07<00:00,  1.26 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 10/10 [00:07<00:00,  1.31 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-14949, -14937) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29050, -29047) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-29227, -29219) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-25445, -25435) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-23751, -23744) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-17263, -17258) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-7941, -7936) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-6369, -6368) with a span answer. \n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-31195, -31186) with a span answer. \n",
            " 95%|█████████▌| 95/100 [08:35<00:49,  9.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How is exhaled breath condensate used in viral research?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 96%|█████████▌| 96/100 [08:37<00:30,  7.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many patients were i this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 97%|█████████▋| 97/100 [08:38<00:16,  5.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the conclusion of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 98%|█████████▊| 98/100 [08:39<00:08,  4.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long did the patient breath into the RTube?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 99%|█████████▉| 99/100 [08:42<00:03,  3.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What followed the reverse transcription step in the analysis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.00it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.97it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.27it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.38it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.73it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/5 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples:  20%|██        | 1/5 [00:00<00:03,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  40%|████      | 2/5 [00:01<00:02,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  60%|██████    | 3/5 [00:02<00:01,  1.27 Batches/s]\u001b[A\n",
            "Inferencing Samples:  80%|████████  | 4/5 [00:03<00:00,  1.28 Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 5/5 [00:03<00:00,  1.49 Batches/s]\n",
            "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
            "(-22461, -22450) with a span answer. \n",
            "100%|██████████| 100/100 [08:51<00:00,  5.32s/it]\n"
          ]
        }
      ],
      "source": [
        "reader = FARMReader(\n",
        "    model_name_or_path=model, #\"deepset/roberta-base-squad2\"\n",
        "    use_gpu=use_gpu\n",
        ")\n",
        "\n",
        "df_res_wikiAPI_bm25 = qa_dataset.copy()\n",
        "\n",
        "# prepare columns for answers\n",
        "df_res_wikiAPI_bm25['predictions_wikiAPI_context_bm25'] = [list() for x in range(len(df_res_wikiAPI_bm25.index))]\n",
        "\n",
        "\n",
        "for q_i in tqdm(range(len(qa_dataset.question.tolist()))):\n",
        "    \n",
        "    print('question : ', qa_dataset.question[q_i])\n",
        "    \n",
        "    # wikipedia API context prediction \n",
        "    try:\n",
        "        search_results_wikiAPI = wiki.search(qa_dataset.question[q_i])\n",
        "\n",
        "        wiki_pages = [\n",
        "            wiki.page(res)\n",
        "            for res in search_results_wikiAPI[:top_k_reader]\n",
        "        ]\n",
        "        \n",
        "        dicts_wikiAPI = [\n",
        "            {\n",
        "                'content' : wiki_page.content,\n",
        "                'meta' : {\n",
        "                    'name' : wiki_page.title\n",
        "                }\n",
        "            } \n",
        "            for wiki_page in tqdm(wiki_pages)\n",
        "        ]\n",
        "        \n",
        "        \n",
        "        document_store_wikiAPI.delete_documents()\n",
        "        document_store_wikiAPI.write_documents(dicts_wikiAPI)\n",
        "        \n",
        "        retriever_wikiAPI = BM25Retriever(document_store=document_store_wikiAPI)\n",
        "        \n",
        "        pipe_wikiAPI = ExtractiveQAPipeline(reader, retriever_wikiAPI)\n",
        "        \n",
        "        prediction_wikiAPI = pipe_wikiAPI.run(\n",
        "            query=qa_dataset.question[q_i],\n",
        "            params={\n",
        "                \"Retriever\" : {\"top_k\": top_k_retriever},\n",
        "                \"Reader\": {\"top_k\": top_k_reader}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        df_res_wikiAPI_bm25.loc[q_i, 'predictions_wikiAPI_context_bm25'].append(\n",
        "            [prediction_wikiAPI['answers'][k].answer for k in range(len(prediction_wikiAPI['answers']))]\n",
        "        )\n",
        "    except:\n",
        "        df_res_wikiAPI_bm25.loc[q_i, 'predictions_wikiAPI_context_bm25'].append([])\n",
        "\n",
        "\n",
        "df_res_wikiAPI_bm25.to_csv('/content/drive/MyDrive/DeepLearning/df_res_wikiAPI_bm25_org_data.csv', index=False)"
      ],
      "id": "f82f36ec"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLCCxjVI5iQ3"
      },
      "source": [
        "#### RE-INITIALISE ELASTICSEARCH"
      ],
      "id": "jLCCxjVI5iQ3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq2wPgRg8FCp"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "id": "Gq2wPgRg8FCp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12ZmrKWf8FCq"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "id": "12ZmrKWf8FCq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4DoZB9d8FCs"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(30)"
      ],
      "id": "n4DoZB9d8FCs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWaJPgQE8FCs"
      },
      "outputs": [],
      "source": [
        "document_store_wikiAPI = ElasticsearchDocumentStore(\n",
        "    port=9200\n",
        ")\n",
        "document_store_wikiAPI.delete_documents()"
      ],
      "id": "qWaJPgQE8FCs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3feff8f5"
      },
      "source": [
        "#### TF IDF"
      ],
      "id": "3feff8f5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e633bb09",
        "outputId": "8f9c934e-57e3-4a0d-c0a8-36e1fbbe2781"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the main cause of HIV-1 infection in children?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "\r  1%|          | 1/100 [00:00<01:23,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What plays the crucial role in the Mother to Child Transmission of HIV-1 and what increases the risk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 3/100 [00:01<00:39,  2.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many children were infected by HIV-1 in 2008-2009, worldwide?\n",
            "question :  What is the role of C-C Motif Chemokine Ligand 3 Like 1 (CCL3L1) in mother to child transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            "  4%|▍         | 4/100 [00:03<01:41,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is DC-GENR and where is  it expressed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            "  5%|▌         | 5/100 [00:05<02:13,  1.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How does the presence of DC-SIGNR affect the MTCT of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            "  6%|▌         | 6/100 [00:07<02:32,  1.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why do low levels of DC-SIGNR enhance Mother to Child Transmission of HIV-1?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            "  7%|▋         | 7/100 [00:09<02:43,  1.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the percentage of Mother to Child Transmission of HIV-1, when there is no intervention?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.08it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.43it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.63it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  4.19it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.86it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.40 Batches/s]\n",
            "  9%|▉         | 9/100 [00:15<03:06,  2.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Does C-C chemokine receptor type 5 (CCR5) affect the transmission of HIV-1?\n",
            "question :  How does Mannanose Binding Lectin (MBL) affect elimination of HIV-1 pathogen?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 10%|█         | 10/100 [00:17<03:04,  2.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How can CCR5's effect in HIV-1 transmission be reduced?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "\r 11%|█         | 11/100 [00:18<02:38,  1.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is IFITM?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:00<00:00,  3.03it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.40it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.57 Batches/s]\n",
            " 12%|█▏        | 12/100 [00:21<03:23,  2.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cysteine residues are contained in the first transmembrane domain of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 13%|█▎        | 13/100 [00:23<03:14,  2.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What inhibits S-palmitoylation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.14it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.76it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.08it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.65it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.61it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.43 Batches/s]\n",
            " 14%|█▍        | 14/100 [00:28<04:20,  3.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What interaction is inhibited by the presence of 2-bromopalmitic acid (2BP)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 15%|█▌        | 15/100 [00:30<03:52,  2.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a function associated with IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.97 Batches/s]\n",
            " 16%|█▌        | 16/100 [00:34<04:01,  2.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the antiviral activity of IFITM3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 17%|█▋        | 17/100 [00:36<03:37,  2.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is another name for IFITM5?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:00<00:00,  2.97it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.38it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.72 Batches/s]\n",
            " 18%|█▊        | 18/100 [00:39<04:05,  2.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the expression of IFITM5 not promoted by interferons?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 19%|█▉        | 19/100 [00:41<03:39,  2.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM5 and the other IFITM proteins?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 20%|██        | 20/100 [00:44<03:20,  2.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the amino acid similarity between IFITM 1, IFITM 2, and IFITM 3?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 21%|██        | 21/100 [00:46<03:06,  2.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What amino acid might be involved in calcium binding in the C-terminal region of a protein?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "\r 22%|██▏       | 22/100 [00:47<02:31,  1.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 23/100 [00:48<02:13,  1.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the molecular structure of bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.96it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.02it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.48it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.31it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.94it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.16 Batches/s]\n",
            " 24%|██▍       | 24/100 [00:53<03:22,  2.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many nucleotides does bovine coronavirus contain?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:00<00:00,  4.17it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.47it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.62 Batches/s]\n",
            " 25%|██▌       | 25/100 [00:57<03:48,  3.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the size of the orf1ab gene in bovine coronavirus?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.19 Batches/s]\n",
            " 26%|██▌       | 26/100 [01:00<03:46,  3.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is the orf1ab gene at the 3' or 5' end of the bovine coronavirus genome?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.88it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.53 Batches/s]\n",
            " 27%|██▋       | 27/100 [01:03<03:40,  3.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a significant cause of Influenze like illness among healthy adolescents and adults presenting for medical evaluation?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 28%|██▊       | 28/100 [01:05<03:16,  2.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the most common species of Human Coronavirus among adults?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 29%|██▉       | 29/100 [01:06<02:36,  2.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which Human Coronavirus showed species specific clinical characteristics of its infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 30/100 [01:07<02:07,  1.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What causes the outbreak of SARS and MERS.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 31%|███       | 31/100 [01:08<01:50,  1.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the case fatality rate of SARS and MERS?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 32/100 [01:09<01:35,  1.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What were the common HCOV strains in the 5 year USA study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:00<00:00,  3.02it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.02it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.08 Batches/s]\n",
            " 33%|███▎      | 33/100 [01:13<02:29,  2.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which species are more prevalent but less severe?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.04it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.85it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  5.23it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.65it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.50it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.18 Batches/s]\n",
            " 34%|███▍      | 34/100 [01:18<03:19,  3.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is required for a Hepatitis B infection in cells?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 35%|███▌      | 35/100 [01:19<02:38,  2.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What regulates the broad, but less specific, virus-cell interaction in a hepatitis B infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.09it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.39it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.18it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.49it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.35it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.86 Batches/s]\n",
            " 36%|███▌      | 36/100 [01:24<03:24,  3.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Which protein domain of the Hepatitis B envelope is necessary for infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.24it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  6.15it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.97it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.56it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.68it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.16 Batches/s]\n",
            " 37%|███▋      | 37/100 [01:29<03:57,  3.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Where is NTCP located in the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.87it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.97 Batches/s]\n",
            " 38%|███▊      | 38/100 [01:32<03:37,  3.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What does the NTCP protein mediate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1/2 [00:00<00:00,  3.07it/s]\u001b[A\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.39it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.55 Batches/s]\n",
            " 39%|███▉      | 39/100 [01:36<03:49,  3.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Is NTCP sufficient to allow HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 40%|████      | 40/100 [01:38<03:14,  3.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is NTCP thought to not be sufficient for HBV infection?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 41%|████      | 41/100 [01:40<02:49,  2.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kinds of viruses are Japanese encephalitis virus(JEV), tick-borne encephalitis virus(TBEV), eastern equine encephalitis virus (EEEV), sindbis virus(SV), and dengue virus(DV)?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 42%|████▏     | 42/100 [01:42<02:32,  2.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the current clinically-available methods to detect encephalitis viral antigens?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "\r 43%|████▎     | 43/100 [01:43<01:57,  2.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What methods exist for detecting multiple antigens simultaneously in a one-sample, laboratory test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 44/100 [01:43<01:26,  1.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many antigens could be detected by Liew's multiplex ELISA test?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 45%|████▌     | 45/100 [01:45<01:32,  1.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What kind of antibodies were used in the ELISA-array assay?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 46/100 [01:46<01:09,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 47/100 [01:46<00:55,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What capture antibodies were used in the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 48/100 [01:47<00:45,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the spotting concentration range for the capture antibodies?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.03it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.39it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.20it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.77it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.26it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.80 Batches/s]\n",
            " 49%|████▉     | 49/100 [01:52<01:54,  2.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the proper spotting concentration determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.06it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.67it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.30it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.09it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  2.81it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.85 Batches/s]\n",
            " 50%|█████     | 50/100 [01:58<02:48,  3.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was cross reaction detection determined?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.00it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.61it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.32it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.10it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.57it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.59 Batches/s]\n",
            " 51%|█████     | 51/100 [02:03<03:07,  3.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the ELISA-array assay validated?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 52/100 [02:03<02:17,  2.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  In 2010, how many cases of tuberculosis were estimated in China?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.04it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.35it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.17it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.13it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.98it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.45 Batches/s]\n",
            " 53%|█████▎    | 53/100 [02:09<02:51,  3.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the population of Shandong province?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 55/100 [02:09<01:25,  1.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the purpose of this study?\n",
            "question :  What was the age range for the people surveyed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 56/100 [02:10<01:00,  1.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How was the survey designed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.04it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.01it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.69it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.12it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.58it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.77 Batches/s]\n",
            " 57%|█████▋    | 57/100 [02:15<01:52,  2.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Was was the sample size?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 58/100 [02:16<01:26,  2.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How were the clusters selected?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 59%|█████▉    | 59/100 [02:16<01:04,  1.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many people were in a community cluster?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.07it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.53it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.28it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.98it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.53it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.82 Batches/s]\n",
            " 60%|██████    | 60/100 [02:22<01:53,  2.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who was excluded from the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 61%|██████    | 61/100 [02:23<01:23,  2.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When was the study conducted?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.13it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.04it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.98it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.57it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.58it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.46 Batches/s]\n",
            " 62%|██████▏   | 62/100 [02:28<02:01,  3.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Who conducted the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.08it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.30it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.57it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  4.27it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.24it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.55 Batches/s]\n",
            " 63%|██████▎   | 63/100 [02:33<02:18,  3.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medium was used to collect the sputum samples?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25%|██▌       | 1/4 [00:00<00:00,  3.95it/s]\u001b[A\n",
            " 50%|█████     | 2/4 [00:00<00:00,  4.89it/s]\u001b[A\n",
            " 75%|███████▌  | 3/4 [00:00<00:00,  3.75it/s]\u001b[A\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.61it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.64 Batches/s]\n",
            " 64%|██████▍   | 64/100 [02:38<02:28,  4.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the response rate for the study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.40it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  6.29it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  6.17it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  5.14it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.56it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.32 Batches/s]\n",
            " 65%|██████▌   | 65/100 [02:43<02:31,  4.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the average age of a study participant?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.17it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.65it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.30it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.81it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.57it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.07 Batches/s]\n",
            " 66%|██████▌   | 66/100 [02:48<02:34,  4.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the prevalence rate in Shandong in 2010 for sputum positive cases of tuberculosis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.44 Batches/s]\n",
            " 67%|██████▋   | 67/100 [02:52<02:26,  4.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the most striking finding of the study regarding tuberculosis patients?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 68/100 [02:53<01:45,  3.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many cases of sputum positive tuberculosis patients had no persistent cough?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  2.97it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.93it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.95it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.30it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.12it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.86 Batches/s]\n",
            " 69%|██████▉   | 69/100 [02:58<01:59,  3.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many tuberculosis patients in Shandong were over 65 years old?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.05it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.27it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.58it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.72it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.49it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.00 Batches/s]\n",
            " 70%|███████   | 70/100 [03:03<02:07,  4.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What enzymes have been reported to be linked with severity of infection and various pathological conditions caused by microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "\r 71%|███████   | 71/100 [03:04<01:36,  3.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  At what temperatures was the assay completed?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.10it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  4.98it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.85it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.49it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.24it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.05 Batches/s]\n",
            " 72%|███████▏  | 72/100 [03:09<01:45,  3.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What criteria sets the guideline for drug-like properties?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "\r 73%|███████▎  | 73/100 [03:10<01:18,  2.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What could be novel candidates as potent inhibitors of papain like cysteine proteases in resistant microorganisms?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 74%|███████▍  | 74/100 [03:12<01:09,  2.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What method is useful in administering small molecules for systemic delivery to the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.19it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.34it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.62it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.80it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.76it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.90 Batches/s]\n",
            " 75%|███████▌  | 75/100 [03:17<01:23,  3.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  Why is the nasal mucosa useful in the delivery of small molecules into the body?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 76/100 [03:18<01:00,  2.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the most common methods of inhaled delivery of medications?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.16it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.35it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.07it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.61it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.51it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.28 Batches/s]\n",
            " 77%|███████▋  | 77/100 [03:23<01:18,  3.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What medications have shown good promise to in vivo delivery via dry powder inhalers?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 78%|███████▊  | 78/100 [03:25<01:06,  3.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How are siRNAs typically delivered for systemic effect?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.96it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.95it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.99it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.98it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.95it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.44 Batches/s]\n",
            " 79%|███████▉  | 79/100 [03:30<01:15,  3.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What structures form the human airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "\r 80%|████████  | 80/100 [03:31<00:55,  2.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What size of particle has been shown to be most effective in the delivery to the lower airway?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 81%|████████  | 81/100 [03:32<00:42,  2.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the essential conditions in siRNA delivery to effectively produce gene silencing in the lungs?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.01it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.78it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.85it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.43it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.69it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.58 Batches/s]\n",
            " 82%|████████▏ | 82/100 [03:37<00:57,  3.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long is the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 83%|████████▎ | 83/100 [03:40<00:48,  2.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many open reading frames are in the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 84%|████████▍ | 84/100 [03:42<00:41,  2.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What virus has the closest genetic identity with the SAIBK gene?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 85%|████████▌ | 85/100 [03:44<00:36,  2.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many surgical masks or respirators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.99it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:01,  2.81it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.89it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  2.90it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  2.94it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.15 Batches/s]\n",
            " 86%|████████▌ | 86/100 [03:49<00:48,  3.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the acronym MERS-CoV?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.07it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.38it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.54it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.63it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.18it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.93 Batches/s]\n",
            " 87%|████████▋ | 87/100 [03:55<00:54,  4.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What are the critical factors that determine the effect of an epidemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 88/100 [03:56<00:36,  3.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  When did the World Health Organization (WHO) officially declare the 2019-nCoV epidemic as a Public Health Emergency of International Concern?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.10it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.33it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:01<00:00,  2.82it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.20it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.12it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.04 Batches/s]\n",
            " 89%|████████▉ | 89/100 [04:01<00:42,  3.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What influenza virus was identified in China in 2013?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 90/100 [04:02<00:28,  2.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What past research has been done on severe, single-wave pandemics?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "\r 91%|█████████ | 91/100 [04:03<00:20,  2.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is a clinical attack rate?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  6.34it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  6.13it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  6.05it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  4.98it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  4.59it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  4.37 Batches/s]\n",
            " 92%|█████████▏| 92/100 [04:08<00:25,  3.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What was the clinical attack rate in the 2009 H1N1 pandemic?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 93/100 [04:09<00:16,  2.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What is the estimated R0 of COVID-19?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.97it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.25it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.55it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.71it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.44it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.97 Batches/s]\n",
            " 94%|█████████▍| 94/100 [04:15<00:20,  3.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many ventilators have past studies projected will be required for a pandemic in the United States?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:01,  3.81it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  3.81it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  3.36it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:01<00:00,  3.19it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.26it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.51 Batches/s]\n",
            " 95%|█████████▌| 95/100 [04:19<00:19,  3.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How is exhaled breath condensate used in viral research?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 97%|█████████▋| 97/100 [04:22<00:07,  2.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How many patients were i this study?\n",
            "question :  What was the conclusion of this study?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n",
            "\r 98%|█████████▊| 98/100 [04:22<00:03,  1.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  How long did the patient breath into the RTube?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\n",
            " 99%|█████████▉| 99/100 [04:24<00:01,  1.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question :  What followed the reverse transcription step in the analysis?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:00<00:00,  4.09it/s]\u001b[A\n",
            " 40%|████      | 2/5 [00:00<00:00,  5.09it/s]\u001b[A\n",
            " 60%|██████    | 3/5 [00:00<00:00,  4.57it/s]\u001b[A\n",
            " 80%|████████  | 4/5 [00:00<00:00,  3.85it/s]\u001b[A\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.89it/s]\n",
            "\n",
            "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
            "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.81 Batches/s]\n",
            "100%|██████████| 100/100 [04:30<00:00,  2.70s/it]\n"
          ]
        }
      ],
      "source": [
        "reader = FARMReader(\n",
        "    model_name_or_path=model, #\"deepset/roberta-base-squad2\"\n",
        "    use_gpu=use_gpu\n",
        ")\n",
        "\n",
        "df_res_wikiAPI_tfidf = qa_dataset.copy()\n",
        "\n",
        "# prepare columns for answers\n",
        "df_res_wikiAPI_tfidf['predictions_wikiAPI_context_tfidf'] = [list() for x in range(len(df_res_wikiAPI_tfidf.index))]\n",
        "\n",
        "\n",
        "for q_i in tqdm(range(len(qa_dataset.question.tolist()))):\n",
        "    \n",
        "    print('question : ', qa_dataset.question[q_i])\n",
        "    \n",
        "    try:\n",
        "        # wikipedia API context prediction \n",
        "        search_results_wikiAPI = wiki.search(qa_dataset.question[q_i])\n",
        "\n",
        "        wiki_pages = [\n",
        "            wiki.page(res)\n",
        "            for res in search_results_wikiAPI[:top_k_reader]\n",
        "        ]\n",
        "        \n",
        "        dicts_wikiAPI = [\n",
        "            {\n",
        "                'content' : wiki_page.content,\n",
        "                'meta' : {\n",
        "                    'name' : wiki_page.title\n",
        "                }\n",
        "            } \n",
        "            for wiki_page in tqdm(wiki_pages)\n",
        "        ]\n",
        "        \n",
        "        \n",
        "        document_store_wikiAPI.delete_documents()\n",
        "        document_store_wikiAPI.write_documents(dicts_wikiAPI)\n",
        "        \n",
        "        retriever_wikiAPI = TfidfRetriever(document_store=document_store_wikiAPI)\n",
        "        \n",
        "        pipe_wikiAPI = ExtractiveQAPipeline(reader, retriever_wikiAPI)\n",
        "        \n",
        "        prediction_wikiAPI = pipe_wikiAPI.run(\n",
        "            query=qa_dataset.question[q_i],\n",
        "            params={\n",
        "                \"Retriever\" : {\"top_k\": top_k_retriever},\n",
        "                \"Reader\": {\"top_k\": top_k_reader}\n",
        "            }\n",
        "        )\n",
        "\n",
        "        df_res_wikiAPI_tfidf.loc[q_i, 'predictions_wikiAPI_context_tfidf'].append(\n",
        "            [prediction_wikiAPI['answers'][k].answer for k in range(len(prediction_wikiAPI['answers']))]\n",
        "        )\n",
        "    except:\n",
        "        df_res_wikiAPI_tfidf.loc[q_i, 'predictions_wikiAPI_context_tfidf'].append([])\n",
        "\n",
        "df_res_wikiAPI_tfidf.to_csv('/content/drive/MyDrive/DeepLearning/df_res_wikiAPI_tfidf_org_data.csv', index=False)"
      ],
      "id": "e633bb09"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1rat1O82vl7"
      },
      "source": [
        "#### RE-INITIALISE ELASTICSEARCH"
      ],
      "id": "r1rat1O82vl7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKU6ixSG2pOL"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ],
      "id": "YKU6ixSG2pOL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpJVTaBQ2pEe"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "id": "TpJVTaBQ2pEe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gebxe-t2o7j"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(30)"
      ],
      "id": "2gebxe-t2o7j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm10gNzJ2o0z",
        "outputId": "ef6de1d8-dc03-4d4e-d81b-fe242031c5f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 29.7 ms, sys: 3.17 ms, total: 32.9 ms\n",
            "Wall time: 2.04 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "document_store_wikiAPI = ElasticsearchDocumentStore(\n",
        "    port=9200\n",
        ")\n",
        "document_store_wikiAPI.delete_documents()"
      ],
      "id": "Tm10gNzJ2o0z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3721474"
      },
      "source": [
        "# Combining dataframes "
      ],
      "id": "d3721474"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GXyPJcB3fPc"
      },
      "outputs": [],
      "source": [
        "df_res_covid_tfidf = pd.read_csv('/content/drive/MyDrive/DeepLearning/df_res_covid_tfidf_org_data.csv')\n",
        "df_res_covid_bm25 = pd.read_csv('/content/drive/MyDrive/DeepLearning/df_res_covid_bm25_org_data.csv')\n",
        "df_res_wiki_bm25 = pd.read_csv('/content/drive/MyDrive/DeepLearning/df_res_wiki_bm25_org_data.csv')\n",
        "df_res_wiki_tfidf = pd.read_csv('/content/drive/MyDrive/DeepLearning/df_res_wiki_tfidf_org_data.csv')\n",
        "df_res_wikiAPI_bm25 = pd.read_csv('/content/drive/MyDrive/DeepLearning/df_res_wikiAPI_bm25_org_data.csv')\n",
        "df_res_wikiAPI_tfidf = pd.read_csv('/content/drive/MyDrive/DeepLearning/df_res_wikiAPI_tfidf_org_data.csv')"
      ],
      "id": "1GXyPJcB3fPc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg4FiqLa3pMH"
      },
      "outputs": [],
      "source": [
        "df_res = qa_dataset.copy()"
      ],
      "id": "Rg4FiqLa3pMH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46a38bb9"
      },
      "outputs": [],
      "source": [
        "df_res['predictions_covid_context_tfidf'] = df_res_covid_tfidf['predictions_covid_context_tfidf'].copy()\n",
        "df_res['predictions_covid_context_bm25'] = df_res_covid_bm25['predictions_covid_context_bm25'].copy()\n",
        "df_res['predictions_wiki_context_bm25'] = df_res_wiki_bm25['predictions_wiki_context_bm25'].copy()\n",
        "df_res['predictions_wiki_context_tfidf'] = df_res_wiki_tfidf['predictions_wiki_context_tfidf'].copy()\n",
        "df_res['predictions_wikiAPI_context_bm25'] = df_res_wikiAPI_bm25['predictions_wikiAPI_context_bm25'].copy()\n",
        "df_res['predictions_wikiAPI_context_tfidf'] = df_res_wikiAPI_tfidf['predictions_wikiAPI_context_tfidf'].copy()"
      ],
      "id": "46a38bb9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la53JWi_43mn"
      },
      "source": [
        "### Save dataframe as csv to drive\n"
      ],
      "id": "la53JWi_43mn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c6ea308"
      },
      "outputs": [],
      "source": [
        "df_res.to_csv('/content/drive/MyDrive/DeepLearning/df_res_squad_coviddata_top5.csv', index=False)"
      ],
      "id": "5c6ea308"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "358.4px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "022ecb73078c4289a183c74d3e0b50ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b677f50de6eb4e68b2d6b13691e04fd9",
              "IPY_MODEL_098c9f6938394fa982c5f3f6f3cefed1",
              "IPY_MODEL_528ca76f405f41739fdda062a0cb30ee"
            ],
            "layout": "IPY_MODEL_2eb689cc78c640cea5bac92d924646cb"
          }
        },
        "036e122c3d454cb5a751b5625d2cefec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0504834615214693a0f93d6e04bfa435": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07652631fda94743abc34ce411375d58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "093d93fe15f341eab1fc3242edf6f706": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d7e75433aea4a8a921d0e3ac0afc53b",
            "max": 4803,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0989b2985b2446e7b381b32b48abea84",
            "value": 4803
          }
        },
        "0989b2985b2446e7b381b32b48abea84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "098c9f6938394fa982c5f3f6f3cefed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5779fe1603a84378886969626b04a725",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1a4a0db250647df82dd6773c17dde5e",
            "value": 1
          }
        },
        "099f1963d9994587b9287547ee1e2f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_036e122c3d454cb5a751b5625d2cefec",
            "placeholder": "​",
            "style": "IPY_MODEL_605e01e7a6d0470bbf78f9973db610d7",
            "value": "Downloading metadata: 100%"
          }
        },
        "20a7bd472cc34986a60a423bab8d2ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2287d3815d7247a2acfaf87dbdea28c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25050867c1b94216a28c20696f6ecbb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27f602e56b08443b8b2e059146fd0b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e4a46739bd34b90904952defda723bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eb689cc78c640cea5bac92d924646cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31a63be9ffa04fc5880f6da061c738cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80c85d4983ef4998b4397b53d2e6189f",
              "IPY_MODEL_abb99ffe34c345c08054d4d4e3f7a91b",
              "IPY_MODEL_568e2dc6d714462b98a6148fe2d27474"
            ],
            "layout": "IPY_MODEL_7b94b973b7fc473faad1b76c124c49bc"
          }
        },
        "3f55f424e82a431c9b61713359b7914a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece90426b93d45dda2d9346395b5b1e1",
            "placeholder": "​",
            "style": "IPY_MODEL_efec0ea3f4cf4b49b6ea480c0f5704f7",
            "value": " 1759/2019 [00:00&lt;00:00, 9309.04 examples/s]"
          }
        },
        "41c4f9539cf441469f1d5a6fea48d546": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43f96edf4c9c4cb6bc6b4b3e12e5a970": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "47738716ef4c4200876acc8586896d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b182549b8c4c41c28004e940e898f400",
            "placeholder": "​",
            "style": "IPY_MODEL_0504834615214693a0f93d6e04bfa435",
            "value": "Downloading data: "
          }
        },
        "47f9e017e33945b2ac413d5d7c3a6568": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c3b4e2758da491f83b4a13b649ea3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d3d2fab04964a08acaae50e9ef637f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "528ca76f405f41739fdda062a0cb30ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdb0b8a4ba68497d85cc57b0c2e760bc",
            "placeholder": "​",
            "style": "IPY_MODEL_4c3b4e2758da491f83b4a13b649ea3ba",
            "value": " 1/1 [00:00&lt;00:00, 32.44it/s]"
          }
        },
        "568e2dc6d714462b98a6148fe2d27474": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5a481b8908347009a50ce243bd15d3b",
            "placeholder": "​",
            "style": "IPY_MODEL_e7c73b8b539247c5b319ea055f71b809",
            "value": " 1/1 [00:00&lt;00:00, 29.86it/s]"
          }
        },
        "5779fe1603a84378886969626b04a725": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a99c9e555bc41319d503bcd96b410a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95428fc3db1e4c13b8771ffd911add3b",
              "IPY_MODEL_5d81818a3a404c3285f4549446827bb1",
              "IPY_MODEL_3f55f424e82a431c9b61713359b7914a"
            ],
            "layout": "IPY_MODEL_43f96edf4c9c4cb6bc6b4b3e12e5a970"
          }
        },
        "5d7e75433aea4a8a921d0e3ac0afc53b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d81818a3a404c3285f4549446827bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47f9e017e33945b2ac413d5d7c3a6568",
            "max": 2019,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9567bd4b2004af19995c300db38ceeb",
            "value": 2019
          }
        },
        "605e01e7a6d0470bbf78f9973db610d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6097852a87ba4b4f9c056347b5b8909f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641a965f328e486ebd891cffc2651f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e7c17b9517f4234a4295cc834c8bac3",
            "placeholder": "​",
            "style": "IPY_MODEL_b775861d7a164ecca1f3a13b9e62f3dc",
            "value": "Downloading builder script: 100%"
          }
        },
        "66e487d31a00443ba1a3e1d25611f414": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffb215bf4f824804bf61fdc865175652",
            "placeholder": "​",
            "style": "IPY_MODEL_e074cec37e114bc5ada2e5223a07bdce",
            "value": " 1.95k/1.95k [00:00&lt;00:00, 64.9kB/s]"
          }
        },
        "6e7c17b9517f4234a4295cc834c8bac3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7273d9a1701549a5b874b635c4b38213": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73bf105963c0488faa1f5d380c274c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07652631fda94743abc34ce411375d58",
            "max": 1951,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_937aded167874d28a49f244e9ca00138",
            "value": 1951
          }
        },
        "7a7fdf7cadc74d378b140a1bb790c3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b94b973b7fc473faad1b76c124c49bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80c85d4983ef4998b4397b53d2e6189f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab963247b708418a94e35215ff80efcb",
            "placeholder": "​",
            "style": "IPY_MODEL_27f602e56b08443b8b2e059146fd0b08",
            "value": "100%"
          }
        },
        "8c6980069f4b42aca4f4a96db780a87e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_099f1963d9994587b9287547ee1e2f2a",
              "IPY_MODEL_73bf105963c0488faa1f5d380c274c18",
              "IPY_MODEL_66e487d31a00443ba1a3e1d25611f414"
            ],
            "layout": "IPY_MODEL_2287d3815d7247a2acfaf87dbdea28c0"
          }
        },
        "937aded167874d28a49f244e9ca00138": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95428fc3db1e4c13b8771ffd911add3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20a7bd472cc34986a60a423bab8d2ae7",
            "placeholder": "​",
            "style": "IPY_MODEL_7a7fdf7cadc74d378b140a1bb790c3d3",
            "value": "Generating train split:  87%"
          }
        },
        "990e436d54174baf9fdc648a6013d41e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c70a44d728b4e5792f1e5dabb48aa58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47738716ef4c4200876acc8586896d4b",
              "IPY_MODEL_f4b77305816e43c8b71d7a3906ddc664",
              "IPY_MODEL_cadda54e30e049018436f43cb72bea9e"
            ],
            "layout": "IPY_MODEL_d0ce2f77553c416ea03b71a902d066d6"
          }
        },
        "a9567bd4b2004af19995c300db38ceeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab963247b708418a94e35215ff80efcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abb99ffe34c345c08054d4d4e3f7a91b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3f531fbf39d41c4961316843a4fab60",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7273d9a1701549a5b874b635c4b38213",
            "value": 1
          }
        },
        "b182549b8c4c41c28004e940e898f400": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b677f50de6eb4e68b2d6b13691e04fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5a21e4d7fad45b7870578f796197a16",
            "placeholder": "​",
            "style": "IPY_MODEL_4d3d2fab04964a08acaae50e9ef637f1",
            "value": "100%"
          }
        },
        "b775861d7a164ecca1f3a13b9e62f3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdb0b8a4ba68497d85cc57b0c2e760bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5a481b8908347009a50ce243bd15d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cadda54e30e049018436f43cb72bea9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_990e436d54174baf9fdc648a6013d41e",
            "placeholder": "​",
            "style": "IPY_MODEL_41c4f9539cf441469f1d5a6fea48d546",
            "value": " 4.42M/? [00:00&lt;00:00, 45.0MB/s]"
          }
        },
        "d0490958a53649f79c7a722f5ee64fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_641a965f328e486ebd891cffc2651f96",
              "IPY_MODEL_093d93fe15f341eab1fc3242edf6f706",
              "IPY_MODEL_fd105d36dd32429493faa4adbc5cb2db"
            ],
            "layout": "IPY_MODEL_2e4a46739bd34b90904952defda723bd"
          }
        },
        "d0ce2f77553c416ea03b71a902d066d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5a21e4d7fad45b7870578f796197a16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e074cec37e114bc5ada2e5223a07bdce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7c73b8b539247c5b319ea055f71b809": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb05db4f82384aed919a1afef81f92f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ece90426b93d45dda2d9346395b5b1e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efec0ea3f4cf4b49b6ea480c0f5704f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eff345c071df4a33b6c34ce64934de34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1a4a0db250647df82dd6773c17dde5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3f531fbf39d41c4961316843a4fab60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b77305816e43c8b71d7a3906ddc664": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6097852a87ba4b4f9c056347b5b8909f",
            "max": 1346251,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eff345c071df4a33b6c34ce64934de34",
            "value": 1346251
          }
        },
        "fd105d36dd32429493faa4adbc5cb2db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb05db4f82384aed919a1afef81f92f1",
            "placeholder": "​",
            "style": "IPY_MODEL_25050867c1b94216a28c20696f6ecbb9",
            "value": " 4.80k/4.80k [00:00&lt;00:00, 136kB/s]"
          }
        },
        "ffb215bf4f824804bf61fdc865175652": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}